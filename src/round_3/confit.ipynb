{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390367bc-cfaa-495a-a88b-f3a0f224fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa14f81-f913-4512-886d-e3b6b9b82aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0538a7d-54ce-4702-9181-f9e1b33fa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a8e31-b49a-4b0a-a1d5-57d201e51fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.utils.constants import *\n",
    "from DomainPrediction.protein.base import BaseProtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20137c44-e825-488f-8ace-d9bc02ed4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../esm')\n",
    "from DomainPrediction.esm.esmc import ESMCLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416ab40-bf67-4fcc-8bdb-78730a528905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db11f89-0e10-4c3f-a1ab-8b77cd7dce9f",
   "metadata": {},
   "source": [
    "### Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059a81c-518e-427f-9d8f-c44000216cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nethome/kgeorge/workspace/DomainPrediction/Data/round_3_exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346ad30-ee78-4202-9fec-8797ebb91ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_domain = pd.read_csv(os.path.join(data_path, 'dataset_2_tdomain.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2ce64-0c67-4da2-8e50-a219a73573f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3ef60-81e7-4202-bd7a-9cbc467b7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_domain['split_id_1'] = 2\n",
    "df_t_domain['split_id_2'] = 2\n",
    "df_t_domain.loc[df_t_domain['name'].str.contains('WT_EP|DESIGN'), 'split_id_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd7c67-3a01-468d-9c77-ab4f118736fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e186c3a-bb7e-434f-adcc-08c40aaf9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in df_t_domain.columns[df_t_domain.columns.str.contains('split')]:\n",
    "    print(f\"{split}\\t Train: {df_t_domain[split].isin([2]).sum()} Test: {df_t_domain[split].isin([0, 1]).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdd32d-86a4-4aba-b913-327f7a3aa172",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_spearmanr_bootstrap(a, b, n=1000):\n",
    "    assert type(a) == type(b) == np.ndarray\n",
    "    assert len(a) == len(b)\n",
    "    corr = []\n",
    "    p_value = []\n",
    "    np.random.seed(0)\n",
    "    for _ in range(n):\n",
    "        indices = np.random.choice(len(a), size=len(a), replace=True)\n",
    "        res = stats.spearmanr(a[indices], b[indices])\n",
    "        \n",
    "        if not np.isnan(res.statistic):\n",
    "            corr.append(res.statistic)\n",
    "            p_value.append(res.pvalue)\n",
    "\n",
    "    ci_lower, ci_upper = np.percentile(corr, [5, 95]) \n",
    "    # stats.t.interval(confidence=0.95, df=len(corr)-1, loc=np.mean(corr), scale=np.std(corr))\n",
    "    mean_corr = np.mean(corr)\n",
    "\n",
    "    return round(mean_corr, 2), round(ci_lower, 2), round(ci_upper, 2), corr, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323cca0-9f32-40d7-b8f7-b3d32f0908c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c34c7f43-2ded-4bf4-b7e7-ec7870e7d19c",
   "metadata": {},
   "source": [
    "### Setup gen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9f374-5162-40fe-970a-b9422e453648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = pd.read_csv(os.path.join(data_path, 'gen_pred.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fac38-5f89-45cc-8a42-41d028d9beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de62b5-14c1-4cec-9a86-746860f8d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gxps_protein = BaseProtein(file='/nethome/kgeorge/workspace/DomainPrediction/Data/gxps/gxps_ATC_AF.pdb')\n",
    "gxps_T_domain = ''.join([gxps_protein.sequence[i] for i in range(len(gxps_protein.sequence)) if i not in A_gxps_atc+C_gxps_atc])\n",
    "gxps_base_seq = gxps_protein.sequence\n",
    "\n",
    "assert len(gxps_T_domain) == 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a077a-8a42-4623-97fe-1bf0d558328a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1747697-7f17-4ba1-b335-be66dfcc7aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17215b80-9436-4f45-a6a6-b95370582080",
   "metadata": {},
   "source": [
    "### Contrastive Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591fde9-7263-4da3-93cf-60244637a86c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277d117-8fc7-4c42-9170-a5704ebf7cb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ProteinFunDatasetContrast(Dataset):\n",
    "    def __init__(self, df, wt):\n",
    "        self.seq, self.y = df['seq'].to_numpy(), df['fitness_raw'].to_numpy()\n",
    "        self.wt = np.array([wt]*self.seq.shape[0], dtype='object')\n",
    "        self.n_mut = df['n_mut'].to_numpy()\n",
    "\n",
    "        self.positions = []\n",
    "        for _, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            pos = []\n",
    "            for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt)):\n",
    "                if aa_wt != aa_mt:\n",
    "                    ## mutation pos\n",
    "                    pos.append(i)\n",
    "\n",
    "            assert len(pos) == row['n_mut']\n",
    "\n",
    "            self.positions.append(np.array(pos))\n",
    "\n",
    "        assert len(self.positions) == self.seq.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.y[idx], self.wt[idx], self.positions[idx], self.n_mut[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        seq = np.array([x[0] for x in data], dtype='object')\n",
    "        y = torch.tensor([x[1] for x in data])\n",
    "        wt = np.array([x[2] for x in data], dtype='object')\n",
    "        pos = [x[3] for x in data]\n",
    "        n_mut = np.array([x[4] for x in data])\n",
    "        return seq, y, wt, pos, n_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bea198-c7bb-454a-84d0-fe47e7afbc28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ESMCConFit(pl.LightningModule):\n",
    "    def __init__(self, name, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if name == 'esmc_300m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 960\n",
    "        elif name == 'esmc_600m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 1152\n",
    "        else:\n",
    "            raise Exception('Check ESMC name')\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"out_proj\"],\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.basemodel, peft_config)\n",
    "\n",
    "        for name, pm in self.model.named_parameters():\n",
    "            if 'q_ln' in name or 'k_ln' in name or 'norm.weight' in name:\n",
    "                pm.requires_grad = True\n",
    "\n",
    "        if self.config['use_seq_head']:\n",
    "            for name, pm in self.model.named_parameters():\n",
    "                if 'sequence_head' in name:\n",
    "                    pm.requires_grad = True\n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.accumulate_batch_bt_loss_train = []\n",
    "        self.accumulate_batch_bt_loss_val = []\n",
    "        self.accumulate_batch_kl_div_train = []\n",
    "        self.accumulate_batch_kl_div_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch_tokens_masked, batch_tokens, batch_tokens_wt, pos):\n",
    "        \n",
    "        output = self.model(batch_tokens_masked)\n",
    "        logits = output.sequence_logits\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        scores = torch.zeros(log_probs.shape[0])\n",
    "        if self.config['device'] == 'gpu':\n",
    "            scores = scores.cuda()\n",
    "\n",
    "        for i in range(log_probs.shape[0]):\n",
    "            scores[i] = torch.sum(log_probs[i, pos[i]+1, batch_tokens[i][pos[i]+1]] - log_probs[i, pos[i]+1, batch_tokens_wt[i][pos[i]+1]])\n",
    "        \n",
    "        return scores, logits\n",
    "    \n",
    "    def BT_loss(self, scores, y):\n",
    "        loss = torch.tensor(0.)\n",
    "        if self.config['device'] == 'gpu':\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(i, len(scores)):\n",
    "                if y[i] > y[j]:\n",
    "                    if torch.abs(scores[j]-scores[i]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[j]-scores[i]))\n",
    "                else:\n",
    "                    if torch.abs(scores[i]-scores[j]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[i]-scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        _positions = []\n",
    "\n",
    "        if self.config['score'] == 'masked':\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                _positions.append(pos[i])\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "        elif self.config['score'] == 'masked-random':\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    if len(pos[i]) > 40:\n",
    "                        sampled_positions = np.random.choice(pos[i], 40, replace=False)\n",
    "                        _positions.append(sampled_positions)\n",
    "                        batch_tokens_masked[i, sampled_positions+1] = self.model.tokenizer.mask_token_id\n",
    "                    else:\n",
    "                        _positions.append(pos[i])\n",
    "                        batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "                else:\n",
    "                    assert len(pos[i]) == 0\n",
    "                    _positions.append(pos[i])\n",
    "\n",
    "        elif self.config['score'] == 'wildtype':\n",
    "            _positions = pos\n",
    "            batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Anata wa bakadesuka?')\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        assert len(_positions) == len(batch_tokens_masked)\n",
    "\n",
    "        y_hat, logits = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, _positions)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        output = self.model_reg(batch_tokens_masked)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_train.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_train.append(l_reg.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        if 'masked' in self.config['score']:\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "        elif self.config['score'] == 'wildtype':\n",
    "            batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Anata wa bakadesuka?')\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, pos)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_masked)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_val.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_val.append(l_reg.item())\n",
    "\n",
    "    def trainmodel(self, df, wt, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetContrast(df, wt)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetContrast(val, wt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=True)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"bf16-mixed\",\n",
    "                                accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    def sanity_check(self, df, wt):\n",
    "        '''\n",
    "            Needs change to accomodate the new losses\n",
    "        '''\n",
    "\n",
    "        dataset = ProteinFunDatasetContrast(df, wt)\n",
    "        loader = DataLoader(dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        y_pred_1 = []\n",
    "        for batch in loader:\n",
    "            mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "            batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "            batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "            if 'masked' in self.config['score']:\n",
    "                batch_tokens_masked = batch_tokens.clone()\n",
    "                for i in range(batch_tokens.shape[0]):\n",
    "                    if len(pos[i]) > 0:\n",
    "                        batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "            elif self.config['score'] == 'wildtype':\n",
    "                batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "            else:\n",
    "                raise Exception('Anata wa bakadesuka?')\n",
    "            \n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat, _ = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, pos)\n",
    "\n",
    "            y_pred_1.append(y_hat.cpu().numpy())\n",
    "\n",
    "        y_pred_1 = np.concatenate(y_pred_1)\n",
    "\n",
    "        y_pred_2 = []\n",
    "        for i, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "\n",
    "            if 'masked' in self.config['score']:\n",
    "                score, n_muts = self.get_masked_marginal(mt_sequence, wt)\n",
    "            elif self.config['score'] == 'wildtype':\n",
    "                score, n_muts = self.get_masked_marginal(mt_sequence, wt)\n",
    "            else:\n",
    "                raise Exception('Anata wa bakadesuka?')\n",
    "\n",
    "            assert n_muts == row['n_mut']\n",
    "\n",
    "            y_pred_2.append(score)\n",
    "\n",
    "        y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "        np.allclose(y_pred_1, y_pred_2, atol=1e-3)\n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_bt_loss_train.clear()\n",
    "        self.accumulate_batch_bt_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_kl_div_train.clear()\n",
    "        self.accumulate_batch_kl_div_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} bt loss: {np.mean(self.accumulate_batch_bt_loss_train)} kl div {np.mean(self.accumulate_batch_kl_div_train)} val loss: {np.mean(self.accumulate_batch_loss_val)} bt loss: {np.mean(self.accumulate_batch_bt_loss_val)} kl div {np.mean(self.accumulate_batch_kl_div_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        esm_protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model.logits(\n",
    "                esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "            )\n",
    "\n",
    "        logits = results.logits.sequence\n",
    "\n",
    "        log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "        return log_prob.to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '_'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def get_masked_marginal_var(self, mt_sequence, wt_sequence, mask_token = '_', mode='wt'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                masked_query_mt = list(mt_sequence)\n",
    "                masked_query_mt[i] = mask_token\n",
    "                masked_sequence_mt = ''.join(masked_query_mt)\n",
    "                masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "                if mode == 'wt':\n",
    "                    masked_query_wt = list(wt_sequence)\n",
    "                elif mode == 'mt':\n",
    "                    masked_query_wt = list(mt_sequence)\n",
    "                else:\n",
    "                    raise Exception('mode takes values mt and wt')\n",
    "\n",
    "                masked_query_wt[i] = mask_token\n",
    "                masked_sequence_wt = ''.join(masked_query_wt)\n",
    "                masked_log_prob_wt = self.get_log_prob(sequence=masked_sequence_wt)\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob_mt[i, idx_mt] - masked_log_prob_wt[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def pseudolikelihood(self, mt_sequence, mask_token = '_'):\n",
    "        score = 0\n",
    "        for i, aa_mt in enumerate(zip(mt_sequence)):\n",
    "\n",
    "            masked_query_mt = list(mt_sequence)\n",
    "            masked_query_mt[i] = mask_token\n",
    "            masked_sequence_mt = ''.join(masked_query_mt)\n",
    "            masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "            idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "            score += masked_log_prob_mt[i, idx_mt]\n",
    "\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b12071-45ea-4e7a-9c65-64e2faa0dd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c83cc1-7b86-4d4e-8356-5a758dd689d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e052719-ec3d-4352-a455-e04edb19291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = df_t_domain['split_id'].isin([2]), df_t_domain['split_id'].isin([0, 1])\n",
    "df_train = df_t_domain[train_mask]\n",
    "df_test = df_t_domain[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f595e-0868-4427-9d1c-4a70e7007c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 50, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 5,\n",
    "        'early_stopping': True,\n",
    "        'lr': 5e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'use_seq_head': True,\n",
    "        'score': 'masked',\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCConFit(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15ad8c-c9c8-41c9-afee-8f9a14a05051",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_t_domain, gxps_T_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9435a0f-e964-46ba-b7ce-d9565855edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, gxps_T_domain, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c044e-96a3-4c6f-b143-9fa1afac54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a38fe-3bc7-4db2-a376-c6ab113456af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_t_domain.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df_t_domain['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241add4-bf9c-436d-aa85-88818c849d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df_t_domain['fitness_raw'] > 0.01\n",
    "color_mask = df_t_domain['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df1ec4-29e6-4945-baf0-48cf61a909ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_gen.iterrows()):\n",
    "    mt_sequence = row['masked_sequence']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e63ec-38c3-48b5-9b28-5d2f164dfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(30, 70, 5))  # Bins from 30 to 100 in steps of 10\n",
    "bin_labels = [f\"{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "mut_bin = pd.cut(df_gen['n_mut'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "unique_bins = mut_bin.cat.categories\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,3), layout='constrained')\n",
    "for _bin in unique_bins:\n",
    "    _mask = mut_bin == _bin\n",
    "    im = ax.scatter(df_gen['esmc_masked_marginal'][_mask], y_pred[_mask], alpha=0.5, s=1, label=_bin)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117990f-e300-42fc-9dee-3f1b29d40d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].scatter(df_gen['esmc_masked_marginal'], y_pred, alpha=0.5, s=1)\n",
    "ax[1].scatter(df_gen['n_mut'], y_pred, alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd350ba-fe4e-4263-9f8a-f7da1ec63652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen['pred_confit_1'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a7a9d-1dfd-4d8f-9eb3-ab4d864fe616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd865ef4-b6f4-427c-8077-1181b73aa1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.to_csv(os.path.join(data_path, 'gen_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8a623-7a99-4a6d-80b6-a2a8c5fc2695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a4d62c-078a-40e2-b17d-65bc9269f6b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e3700-417a-4895-af3e-63e33e493cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = df_t_domain['split_id_1'].isin([2]), df_t_domain['split_id_1'].isin([0, 1])\n",
    "df_train = df_t_domain[train_mask]\n",
    "df_test = df_t_domain[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eba760-2b83-4664-94b5-ee6c56df87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28510bb8-f81e-4c55-ac4f-36d9c1f0343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 10, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 5,\n",
    "        'early_stopping': False,\n",
    "        'lr': 5e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'use_seq_head': True,\n",
    "        'score': 'masked',\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCConFit(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08265b22-6be2-4685-9dab-2188712f820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_t_domain, gxps_T_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c06c0-177a-44d6-8a79-3a5431bba4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['epoch'] = 10\n",
    "surrogate.config['early_stopping'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27705e2f-de98-430a-8d4c-de3dbacd2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, gxps_T_domain, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25ca2d-4d3f-41bd-beb3-2d28c6913654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f8069-acdb-4490-bd66-6e15b1e16aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_t_domain.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df_t_domain['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a76c0-c4b4-4df5-97b4-592abc66acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df_t_domain['fitness_raw'] > 0.01\n",
    "color_mask = df_t_domain['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2170203-ef9d-42a9-a4db-48341226df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_gen.iterrows()):\n",
    "    mt_sequence = row['masked_sequence']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f7364-91b3-4335-8ce2-3258ae4d25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(30, 70, 5))  # Bins from 30 to 100 in steps of 10\n",
    "bin_labels = [f\"{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "mut_bin = pd.cut(df_gen['n_mut'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "unique_bins = mut_bin.cat.categories\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,3), layout='constrained')\n",
    "for _bin in unique_bins:\n",
    "    _mask = mut_bin == _bin\n",
    "    im = ax.scatter(df_gen['esmc_masked_marginal'][_mask], y_pred[_mask], alpha=0.5, s=1, label=_bin)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc7f4b-4015-4701-89ae-eb1cfa7e096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(10,3), layout='constrained')\n",
    "ax[0].scatter(df_gen['esmc_masked_marginal'], y_pred, alpha=0.5, s=1)\n",
    "ax[1].scatter(df_gen['n_mut'], y_pred, alpha=0.5, s=1)\n",
    "ax[2].scatter(df_gen['pred_confit_1'], y_pred, alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb8927-d142-4679-9a03-cd419638d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen['pred_confit_2'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a627b8e-f8ad-43b0-aaed-811aebf4c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf5632-576a-4e36-a28f-dbce22dacd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.to_csv(os.path.join(data_path, 'gen_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ceb4e-8ffe-443b-b7e1-91dd294ad4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ffdf681-3d65-48af-bbc3-967444dfc858",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f23c9-60de-4a99-9c85-a3663479bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = df_t_domain['split_id_2'].isin([2]), df_t_domain['split_id_2'].isin([0, 1, 2])\n",
    "df_train = df_t_domain[train_mask]\n",
    "df_test = df_t_domain[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097738fd-0f8c-4edb-8d88-cba54dcf843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850729d-1265-415f-9517-06874b91e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 30, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 5,\n",
    "        'early_stopping': True,\n",
    "        'lr': 5e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'use_seq_head': True,\n",
    "        'score': 'masked',\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCConFit(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ada495-7724-4a93-9dbc-1c919af55410",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_t_domain, gxps_T_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62d0d3-d3a7-404f-b34f-bfcd70a86bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, gxps_T_domain, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8be89-c82d-4a9f-8720-9e19ef395a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf9140-4993-41d1-8abf-54f2bcdaf899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_t_domain.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df_t_domain['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1dc0d-3789-45b7-b5c3-ef63306d7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df_t_domain['fitness_raw'] > 0.01\n",
    "color_mask = df_t_domain['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b2fde-3fd8-4e92-9091-cc781ff4fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df_gen.iterrows()):\n",
    "    mt_sequence = row['masked_sequence']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, gxps_T_domain)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd144a-a703-419a-b6e0-3ea75c455510",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(30, 70, 5))  # Bins from 30 to 100 in steps of 10\n",
    "bin_labels = [f\"{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "mut_bin = pd.cut(df_gen['n_mut'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "unique_bins = mut_bin.cat.categories\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,3), layout='constrained')\n",
    "for _bin in unique_bins:\n",
    "    _mask = mut_bin == _bin\n",
    "    im = ax.scatter(df_gen['esmc_masked_marginal'][_mask], y_pred[_mask], alpha=0.5, s=1, label=_bin)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80397f8-2b84-4f88-9fa0-e18bd565d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(10,3), layout='constrained')\n",
    "ax[0].scatter(df_gen['esmc_masked_marginal'], y_pred, alpha=0.5, s=1)\n",
    "ax[1].scatter(df_gen['n_mut'], y_pred, alpha=0.5, s=1)\n",
    "ax[2].scatter(df_gen['pred_confit_1'], y_pred, alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7d1dc-63ca-42f3-a491-31e6f184690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen['pred_confit_3'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c924d6-8885-4b43-ad91-eadd33b2ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d674a42-e752-410f-9c0b-63b015187b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.to_csv(os.path.join(data_path, 'gen_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9ac80-37fd-4932-b052-5db0e056a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0eaeaa-467d-4ba0-b1a1-c950456cae08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
