{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.al import top_model as topmodel\n",
    "from DomainPrediction.al.embeddings import one_hot_encode\n",
    "from DomainPrediction.protein.base import BaseProtein\n",
    "from DomainPrediction.utils.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../esm')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, GenerationConfig, ESMProteinTensor, ESMProteinError, SamplingTrackConfig, SamplingConfig\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "\n",
    "from esm.utils.generation import (\n",
    "    iterative_sampling_raw, \n",
    "    iterative_sampling_tokens, \n",
    "    _get_non_special_tokens, \n",
    "    _get_masked_positions,\n",
    "    _stack_protein_tensors,\n",
    "    _slice_tensor_dataclass,\n",
    "    _trim_sequence_tensor_dataclass,\n",
    "    _get_annealed_temperature,\n",
    "    _sample_per_prompt,\n",
    "    _get_iterative_sampling_mask_for_prompt_and_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial run on Online DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device  = 'gpu'\n",
    "# if device == 'gpu':\n",
    "#     model = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\")\n",
    "# else:\n",
    "#     model = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein = ProteinChain.from_pdb('../../Data/gxps/gxps_ATC_AF.pdb')\n",
    "\n",
    "# sequence_prompt = ''.join([protein[i].sequence if i in A_gxps_atc + C_gxps_atc else '_' for i in range(len(protein))])\n",
    "# structure_prompt = torch.tensor(protein.atom37_positions)\n",
    "\n",
    "# assert sequence_prompt.count('_') == 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sequence_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_prediction_config = GenerationConfig(\n",
    "#         track=\"sequence\", \n",
    "#         num_steps=sequence_prompt.count(\"_\") // 2, \n",
    "#         temperature=0.5\n",
    "#     )\n",
    "\n",
    "# esm_protein = ESMProtein(sequence=sequence_prompt, coordinates=None)\n",
    "# # esm_protein = ESMProtein(sequence=sequence_prompt, coordinates=structure_prompt)\n",
    "\n",
    "# # generated_protein = model.generate(esm_protein, sequence_prediction_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # proteins = model.batch_generate([esm_protein], [sequence_prediction_config])\n",
    "\n",
    "# '''\n",
    "#     we are inside batch generate\n",
    "# '''\n",
    "# inputs = [esm_protein]\n",
    "# configs = [sequence_prediction_config]\n",
    "\n",
    "# assert len(inputs) == len(\n",
    "#             configs\n",
    "#         ), \"Must have the same number of prompts and configs.\"\n",
    "\n",
    "# if inputs == []:\n",
    "#     raise Exception('Empty list')\n",
    "\n",
    "# # Make sure prompts are of the same type.\n",
    "# t = type(inputs[0])\n",
    "# for i in range(1, len(inputs)):\n",
    "#     assert isinstance(inputs[i], t), (\n",
    "#         \"Prompts must have the same type. Got \"\n",
    "#         f\"{t.__name__ and type(inputs[i]).__name__} instead.\"\n",
    "#     )\n",
    "\n",
    "# if isinstance(inputs[0], ESMProtein):\n",
    "#     print('We go with iterative_sampling_raw')\n",
    "#     proteins =  iterative_sampling_raw(model, inputs, configs)  # type: ignore\n",
    "# elif isinstance(inputs[0], ESMProteinTensor):\n",
    "#     print('We go with iterative_sampling_tokens')\n",
    "#     proteins =  iterative_sampling_tokens(\n",
    "#         model,\n",
    "#         inputs,  # type: ignore\n",
    "#         configs,\n",
    "#         model.tokenizers,  # type: ignore\n",
    "#     )\n",
    "# else:\n",
    "#     raise ValueError(\"Input must be an ESMProtein or ESMProteinTensor\")\n",
    "\n",
    "# '''\n",
    "#     proteins is return to self.batch_generate()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#     We are inside iterative_sampling_raw\n",
    "# '''\n",
    "# proteins = [esm_protein]\n",
    "# configs = [sequence_prediction_config]\n",
    "# input_tokens = [model.encode(protein) for protein in proteins]\n",
    "# ## If you have coordinates -> structure tokens are also computed\n",
    "\n",
    "# output_tokens_list = model.batch_generate(input_tokens, configs)\n",
    "\n",
    "# '''\n",
    "#     iterative decoding happens inside batch_generate.\n",
    "#     Initially its sequnce - real decoing happens when its ESM protein Tensor\n",
    "#     then the iterative sample tokens is the real deal\n",
    "# '''\n",
    "\n",
    "# raw_proteins: list[ESMProtein | ESMProteinError] = []\n",
    "# for output_tokens in output_tokens_list:\n",
    "#     if isinstance(output_tokens, ESMProteinTensor):\n",
    "#         raw_proteins.append(model.decode(output_tokens))\n",
    "#     elif isinstance(output_tokens, ESMProteinError):\n",
    "#         raw_proteins.append(output_tokens)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown output type {type(output_tokens)}\")\n",
    "\n",
    "# for input_protein, raw_protein, config in zip(proteins, raw_proteins, configs):\n",
    "#     if isinstance(raw_protein, ESMProteinError):\n",
    "#         # If this generation errored out.\n",
    "#         continue\n",
    "#     if config.track not in [\"function\", \"residue_annotations\"]:\n",
    "#         # Function and residue annotation encoding/decoding is lossy\n",
    "#         # There is no guarantee that decoding encoded tokens will yield the same input\n",
    "#         raw_protein.function_annotations = input_protein.function_annotations\n",
    "\n",
    "# '''\n",
    "#     in raw_proteins we get ESMProteins -> sequences \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#  batch generate with tokens uses iterative_sampling_tokens\n",
    "# '''\n",
    "# proteins = [esm_protein]\n",
    "# configs = [sequence_prediction_config]\n",
    "# input_tokens = [model.encode(protein) for protein in proteins]\n",
    "\n",
    "# inputs = input_tokens\n",
    "\n",
    "# assert len(inputs) == len(\n",
    "#             configs\n",
    "#         ), \"Must have the same number of prompts and configs.\"\n",
    "\n",
    "# if inputs == []:\n",
    "#     raise Exception('Empty list')\n",
    "\n",
    "# # Make sure prompts are of the same type.\n",
    "# t = type(inputs[0])\n",
    "# for i in range(1, len(inputs)):\n",
    "#     assert isinstance(inputs[i], t), (\n",
    "#         \"Prompts must have the same type. Got \"\n",
    "#         f\"{t.__name__ and type(inputs[i]).__name__} instead.\"\n",
    "#     )\n",
    "\n",
    "# if isinstance(inputs[0], ESMProtein):\n",
    "#     print('We go with iterative_sampling_raw')\n",
    "#     proteins =  iterative_sampling_raw(model, inputs, configs)  # type: ignore\n",
    "# elif isinstance(inputs[0], ESMProteinTensor):\n",
    "#     print('We go with iterative_sampling_tokens')\n",
    "#     proteins =  iterative_sampling_tokens(\n",
    "#         model,\n",
    "#         inputs,  # type: ignore\n",
    "#         configs,\n",
    "#         model.tokenizers,  # type: ignore\n",
    "#     )\n",
    "# else:\n",
    "#     raise ValueError(\"Input must be an ESMProtein or ESMProteinTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#  just decode using model.decode\n",
    "# '''\n",
    "# model.decode(proteins[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from esm.utils.sampling import (\n",
    "#     _BatchedESMProteinTensor,\n",
    "#     get_sampling_mask,\n",
    "#     sample_function_logits,\n",
    "#     sample_logits,\n",
    "#     sample_residue_annotation_logits,\n",
    "#     sample_sasa_logits,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _batch_forward(client, protein):\n",
    "#     # Forward pass\n",
    "#     return client.logits(\n",
    "#         protein,\n",
    "#         LogitsConfig(\n",
    "#             sequence=True,\n",
    "#             structure=True,\n",
    "#             secondary_structure=True,\n",
    "#             sasa=True,\n",
    "#             function=True,\n",
    "#             residue_annotations=True,\n",
    "#             return_embeddings=True,\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "# '''\n",
    "#     inside iterative_sample_tokens -> where magic happens\n",
    "#     return ESMProteinTensor\n",
    "# '''\n",
    "# import attr\n",
    "\n",
    "# proteins = [esm_protein]\n",
    "# configs = [sequence_prediction_config]\n",
    "# input_tokens = [model.encode(protein) for protein in proteins]\n",
    "# tokenizers =  model.tokenizers\n",
    "\n",
    "# devices = set([t.device for t in input_tokens])\n",
    "# if len(devices) > 1:\n",
    "#     raise AttributeError(f\"Input tokens on multiple devices {devices}\")\n",
    "\n",
    "# sampled_tokens = [attr.evolve(tokens) for tokens in input_tokens]\n",
    "# ## just ends up making a copy\n",
    "\n",
    "# # Clear structure tokens if user would like to condition only on coordinates.\n",
    "# for tokens, config in zip(sampled_tokens, configs):\n",
    "#     if config.condition_on_coordinates_only and tokens.coordinates is not None:\n",
    "#         print('We enter clear structure tokens')\n",
    "#         tokens.structure = None\n",
    "\n",
    "# # Total sequence lengths.\n",
    "# sequence_lengths = [len(tokens) for tokens in sampled_tokens]\n",
    "# # Figure out the number of tokens to be sampled for each prompt.\n",
    "# total_to_sample = []\n",
    "# for protein, config in zip(sampled_tokens, configs):\n",
    "#     track = config.track\n",
    "\n",
    "#     if getattr(protein, track) is None:\n",
    "#         # We need to sample the entire track.\n",
    "#         num_sampling_steps = _get_non_special_tokens(protein, tokenizers)\n",
    "#         print(f'num_sampling_steps {num_sampling_steps}')\n",
    "#         raise Exception('we wont eneter here')\n",
    "#     else:\n",
    "#         masked = _get_masked_positions(\n",
    "#             track, getattr(protein, track), getattr(tokenizers, track).mask_token_id\n",
    "#         )\n",
    "#         num_sampling_steps = torch.sum(masked).item()\n",
    "\n",
    "#     total_to_sample.append(num_sampling_steps)\n",
    "\n",
    "#     # Users might over-specify the number of sampling steps for a given prompt\n",
    "#     # TODO: Give a warning about mismatched num_steps and number of masks.\n",
    "\n",
    "#     if (num_sampling_steps > 0) and (num_sampling_steps < config.num_steps):\n",
    "#         config.num_steps = int(num_sampling_steps)\n",
    "\n",
    "# max_num_steps = max([config.num_steps for config in configs])\n",
    "\n",
    "# print(total_to_sample, max_num_steps)\n",
    "\n",
    "# batched_tokens = _stack_protein_tensors(\n",
    "#     sampled_tokens, sequence_lengths, tokenizers, devices.pop()\n",
    "# )\n",
    "\n",
    "# # Remember sampled prompts that has somehow errored out.\n",
    "# errors: dict[int, ESMProteinError] = {}\n",
    "\n",
    "# # Decode\n",
    "# disable_tqdm = bool(os.environ.get(\"DISABLE_ITERATIVE_SAMPLING_TQDM\", False))\n",
    "# for t in tqdm(range(max_num_steps), disable=disable_tqdm):\n",
    "#     forward_out = _batch_forward(model, batched_tokens)\n",
    "\n",
    "#     for i, config in enumerate(configs):  # B\n",
    "\n",
    "#         if i in errors:\n",
    "#             # This prompts has errored out in previous steps.\n",
    "#             # Skip.\n",
    "#             continue\n",
    "\n",
    "#         if config.track in [\"coordinates\", \"residue_annotations\"]:\n",
    "#             errors[i] = ESMProteinError(\n",
    "#                 error_code=500,\n",
    "#                 error_msg=f\"Iterative sampling {config.track} is not supported.\",\n",
    "#             )\n",
    "#             continue\n",
    "\n",
    "#         if t >= config.num_steps:\n",
    "#             # Done sampling for this row.\n",
    "#             continue\n",
    "\n",
    "#         per_prompt_cur_sampled = _BatchedESMProteinTensor.from_protein_tensor(\n",
    "#             batched_tokens.slice(i)\n",
    "#         )\n",
    "\n",
    "#         per_prompt_forward_out = _slice_tensor_dataclass(\n",
    "#                 forward_out, i, keep_dim=True\n",
    "#             )\n",
    "        \n",
    "#         per_prompt_forward_out = _trim_sequence_tensor_dataclass(\n",
    "#             per_prompt_forward_out,\n",
    "#             # Note(jungong) : we can not smiply use sequence_lenths[i] here,\n",
    "#             # what we want is for the sequence length of the logits to match\n",
    "#             # that of the prompt, which may or may not be padded, depending on\n",
    "#             # whether the padding was done locally with the open source model\n",
    "#             # (where per_prompt_cur_sampled is already padded) or by\n",
    "#             # BatchedESM3ModelRunner (where per_prompt_cur_sampled is not padded).\n",
    "#             len(per_prompt_cur_sampled),\n",
    "#         )\n",
    "\n",
    "#         if config.temperature_annealing:\n",
    "#             temperature = _get_annealed_temperature(\n",
    "#                 t, config.num_steps, config.temperature\n",
    "#             )\n",
    "#         else:\n",
    "#             temperature = config.temperature\n",
    "\n",
    "#         track_sample_config = SamplingTrackConfig()\n",
    "#         track_sample_config.invalid_ids = config.invalid_ids\n",
    "#         track_sample_config.temperature = temperature\n",
    "#         track_sample_config.top_p = config.top_p\n",
    "#         sampling_config = SamplingConfig(**{config.track: track_sample_config})  # type: ignore\n",
    "\n",
    "#         # Sampling has to be done per-prompt, since sampling configs\n",
    "#         # are likely be different for different prompts.\n",
    "#         per_prompt_forward_and_sample_output = _sample_per_prompt(\n",
    "#             per_prompt_cur_sampled,\n",
    "#             per_prompt_forward_out,\n",
    "#             sampling_config,\n",
    "#             tokenizers,\n",
    "#             decode_sasa_tokens=False,\n",
    "#         )\n",
    "\n",
    "#         # All positions sampled after _sample_per_prompt() above.\n",
    "#         # (B, L) & (B, L, D)\n",
    "#         per_prompt_new_sampled = per_prompt_forward_and_sample_output.protein_tensor\n",
    "\n",
    "#         # Find the positions we should sample this round.\n",
    "#         assert per_prompt_forward_and_sample_output.entropy is not None\n",
    "#         try:\n",
    "#             where_to_sample = _get_iterative_sampling_mask_for_prompt_and_step(\n",
    "#                 per_prompt_cur_sampled,\n",
    "#                 torch.tensor(sequence_lengths[i]),\n",
    "#                 torch.tensor(total_to_sample[i]),\n",
    "#                 t,\n",
    "#                 per_prompt_forward_and_sample_output.entropy,\n",
    "#                 config,\n",
    "#                 tokenizers,\n",
    "#             )\n",
    "#         except ValueError as e:\n",
    "#             errors[i] = ESMProteinError(error_code=500, error_msg=str(e))\n",
    "#             continue\n",
    "\n",
    "#         where_to_sample.to(input_tokens[0].device)\n",
    "\n",
    "#         old_track_samples = getattr(per_prompt_cur_sampled, config.track)\n",
    "#         new_track_samples = getattr(per_prompt_new_sampled, config.track)\n",
    "\n",
    "#         # Iterative sampling by picking the tokens sampled this round\n",
    "#         # from new_track_samples to old_track_samples.\n",
    "#         new_track_samples = torch.where(\n",
    "#             where_to_sample, new_track_samples, old_track_samples\n",
    "#         )\n",
    "\n",
    "#         # Update the corresponding row with new data.\n",
    "#         getattr(batched_tokens, config.track)[i, ...] = new_track_samples[0]\n",
    "\n",
    "# # Un-pack to a list of single ProteinTypes.\n",
    "# output_tokens = [\n",
    "#     batched_tokens.slice(i, sequence_len=sequence_lengths[i])\n",
    "#     if i not in errors\n",
    "#     else errors[i]\n",
    "#     for i in range(len(input_tokens))\n",
    "# ]\n",
    "\n",
    "# # Do not update tracks that were not sampled (e.g. keep None instead of masks)\n",
    "# for inputs, outputs, config in zip(input_tokens, output_tokens, configs):\n",
    "#     if isinstance(outputs, ESMProteinError):\n",
    "#         continue\n",
    "\n",
    "#     # First restore coordinates field.\n",
    "#     # We know coordinates can never be iteratively sampled.\n",
    "#     setattr(outputs, \"coordinates\", getattr(inputs, \"coordinates\"))\n",
    "#     # Maybe restore all the other fields.\n",
    "#     for f in attr.fields(SamplingConfig):\n",
    "#         if \"embedding\" in f.name or f.name == \"return_hidden_states\":\n",
    "#             continue\n",
    "#         if f.name != config.track:\n",
    "#             setattr(outputs, f.name, getattr(inputs, f.name))\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decode(output_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''.join(np.array(list(model.decode(output_tokens[0]).sequence))[T_gxps_atc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DPOTrainer:\n",
    "#     def __init__(self, model, model_ref, device='cpu') -> None:\n",
    "#         self.device = device\n",
    "\n",
    "#         if self.device == 'gpu':\n",
    "#             self.model = model.to(\"cuda\")\n",
    "#             self.model_ref = model_ref.to(\"cuda\")\n",
    "#         else:\n",
    "#             self.model = model.to(\"cpu\")\n",
    "#             self.model_ref = model_ref.to(\"cpu\")\n",
    "\n",
    "#         # self.model_ref.eval()\n",
    "#         # for pm in self.model_ref.parameters():\n",
    "#         #     pm.requires_grad = False\n",
    "\n",
    "#         for name, param in self.model.named_parameters():\n",
    "#             if name in [\n",
    "#                 \"encoder.sequence_embed.weight\", \n",
    "#                 \"output_heads.sequence_head.0.weight\", \n",
    "#                 \"output_heads.sequence_head.0.bias\",\n",
    "#                 \"output_heads.sequence_head.2.weight\",\n",
    "#                 \"output_heads.sequence_head.2.bias\",\n",
    "#                 \"output_heads.sequence_head.3.weight\",\n",
    "#                 \"output_heads.sequence_head.3.bias\"\n",
    "#             ]:\n",
    "#                 param.requires_grad = True\n",
    "#             else:\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         ## need to add peft here ? Do we ?\n",
    "\n",
    "#     def generate_batch(self, sequence_prompt, structure_prompt, batch_size):\n",
    "#         sequence_prediction_config = GenerationConfig(\n",
    "#             track=\"sequence\", \n",
    "#             num_steps=sequence_prompt.count(\"_\") // 2, \n",
    "#             temperature=0.5\n",
    "#         )\n",
    "#         esm_protein = ESMProtein(sequence=sequence_prompt, coordinates=structure_prompt)\n",
    "        \n",
    "#         generated_proteins = []\n",
    "#         for _ in tqdm(range(batch_size)):\n",
    "#             generated_protein = self.model.generate(esm_protein, sequence_prediction_config)\n",
    "#             generated_proteins.append(generated_protein.sequence)\n",
    "\n",
    "#         return generated_proteins\n",
    "    \n",
    "#     def get_mask_positions(self, sequence_prompt, mask_var='_'):\n",
    "#         positions = []\n",
    "#         for i in range(len(sequence_prompt)):\n",
    "#             if sequence_prompt[i] == mask_var:\n",
    "#                 positions.append(i)\n",
    "\n",
    "#         return positions\n",
    "    \n",
    "#     def __get_n_mutations(self, sequence):\n",
    "#         base = 'APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELGGHSLLAVRLLGRLRQHGLGLQMRDLFEAPVLAELATRLRPYQPLEVPANGITPDTTVLTPEMLPLVTLS'\n",
    "#         assert len(base) == len(sequence)\n",
    "\n",
    "#         count = 0\n",
    "#         for aa1, aa2 in zip(base, sequence):\n",
    "#             if aa1 != aa2:\n",
    "#                 count += 1\n",
    "\n",
    "#         return count\n",
    "    \n",
    "#     def get_property_batch(self, batch):\n",
    "#         for item in batch:\n",
    "#             item['property'] = self.__get_n_mutations(item['masked_sequence'])\n",
    "\n",
    "#         return batch\n",
    "    \n",
    "#     def get_log_prob(self, sequence):\n",
    "#         esm_protein = ESMProtein(sequence=sequence)\n",
    "#         esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "#         results = self.model.logits(\n",
    "#             esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "#         )\n",
    "\n",
    "#         logits = results.logits.sequence\n",
    "#         log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "#         return log_prob.cpu().numpy()\n",
    "    \n",
    "#     def get_log_likelihood(self, batch, masked_positions, mask_var='_'):\n",
    "#         '''\n",
    "#             for now mask each pos in mask\n",
    "#         '''\n",
    "#         for item in batch:\n",
    "#             sum_log = 0\n",
    "#             for pos in masked_positions:\n",
    "#                 masked_query = list(item['sequence'])\n",
    "#                 assert mask_var not in masked_query\n",
    "#                 masked_query[pos] = mask_var\n",
    "#                 masked_query = ''.join(masked_query)\n",
    "#                 log_prob = self.get_log_prob(sequence=masked_query)\n",
    "#                 assert log_prob.shape[0] == len(item['sequence'])\n",
    "\n",
    "#                 prob_pos = log_prob[pos, self.model.tokenizers.sequence.convert_tokens_to_ids(item['sequence'][pos])]\n",
    "#                 sum_log += prob_pos                \n",
    "\n",
    "#             item['log_likelihood'] = sum_log/len(masked_positions)\n",
    "\n",
    "#         return batch\n",
    "    \n",
    "#     def create_pairs(self, batch):\n",
    "        \n",
    "#         pair_dataset = []\n",
    "#         for item_1 in batch[:-1]:\n",
    "#             for item_2 in batch[1:]:\n",
    "#                 if item_1['property'] < item_2['property'] and item_1['log_likelihood'] < item_2['log_likelihood']:\n",
    "#                     pair_dataset.append((item_1, item_2))\n",
    "#                 elif item_1['property'] > item_2['property'] and item_1['log_likelihood'] > item_2['log_likelihood']:\n",
    "#                     pair_dataset.append((item_2, item_1))\n",
    "\n",
    "#         _, ax = plt.subplots(1, 2, figsize=(7, 3), layout='constrained')\n",
    "#         ax[0].hist([[item[0]['property'] for item in pair_dataset], \n",
    "#                     [item[1]['property'] for item in pair_dataset]], label=['pos', 'neg'])\n",
    "#         ax[1].hist([[item[0]['log_likelihood'] for item in pair_dataset],\n",
    "#                     [item[1]['log_likelihood'] for item in pair_dataset]], label=['pos', 'neg'])\n",
    "#         ax[0].legend()\n",
    "#         ax[1].legend()\n",
    "#         plt.show()\n",
    "\n",
    "#         return pair_dataset\n",
    "    \n",
    "#     def get_policy(self, batch):\n",
    "\n",
    "#         proteins = [ESMProtein(sequence=item['generated_sequence'], \n",
    "#                                    coordinates=item['structure_prompt']) \n",
    "#                                    for item in batch]\n",
    "        \n",
    "#         input_tokens = [self.model.encode(protein) for protein in proteins]\n",
    "\n",
    "#         print(input_tokens)\n",
    "        \n",
    "    \n",
    "#     def dpo_paired_loss(self, batch):\n",
    "\n",
    "#         self.get_policy(batch)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     def train(self, sequence_prompt, structure_prompt):\n",
    "\n",
    "#         self.model.train()\n",
    "\n",
    "#         mask_positions = self.get_mask_positions(sequence_prompt=sequence_prompt)\n",
    "\n",
    "#         epochs = 3\n",
    "#         batch_size = 8\n",
    "#         batch_dataset = []\n",
    "#         for epoch in range(epochs):\n",
    "#             batch_seq = self.generate_batch(sequence_prompt=sequence_prompt, \n",
    "#                                         structure_prompt=structure_prompt,\n",
    "#                                         batch_size=batch_size)\n",
    "            \n",
    "#             for _seq in batch_seq:\n",
    "#                 batch_dataset.append({\n",
    "#                     'generated_sequence': _seq,\n",
    "#                     'masked_sequence': ''.join([s for _i, s in enumerate(_seq) if _i in mask_positions]),\n",
    "#                     'masked_positions': mask_positions, ## These are the same\n",
    "#                     'sequence_prompt': sequence_prompt, ## These are the same\n",
    "#                     'structure_prompt': structure_prompt ## These are the same        \n",
    "#                 })\n",
    "\n",
    "#             batch_dataset = self.get_property_batch(batch_dataset)\n",
    "\n",
    "#             # batch_dataset = self.get_log_likelihood(batch_dataset, mask_positions)\n",
    "#             # pair_dataset = self.create_pairs(batch_dataset)\n",
    "\n",
    "#             return batch_dataset\n",
    "\n",
    "#             self.dpo_paired_loss(batch_dataset)\n",
    "                \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DPOTrainer:\n",
    "#     def __init__(self, model, model_ref, device='cpu') -> None:\n",
    "#         self.device = device\n",
    "\n",
    "#         # if self.device == 'gpu':\n",
    "#         #     self.model = model.to(\"cuda:0\")\n",
    "#         #     self.model_ref = model_ref.to(\"cuda:1\")\n",
    "#         # else:\n",
    "#         #     self.model = model.to(\"cpu\")\n",
    "#         #     self.model_ref = model_ref.to(\"cpu\")\n",
    "\n",
    "#         self.model = model\n",
    "#         self.model_ref =  model_ref\n",
    "\n",
    "#         self.model_ref.eval()\n",
    "#         for pm in self.model_ref.parameters():\n",
    "#             pm.requires_grad = False\n",
    "\n",
    "#         for name, param in self.model.named_parameters():\n",
    "#             if name in [\n",
    "#                 \"encoder.sequence_embed.weight\", \n",
    "#                 \"output_heads.sequence_head.0.weight\", \n",
    "#                 \"output_heads.sequence_head.0.bias\",\n",
    "#                 \"output_heads.sequence_head.2.weight\",\n",
    "#                 \"output_heads.sequence_head.2.bias\",\n",
    "#                 \"output_heads.sequence_head.3.weight\",\n",
    "#                 \"output_heads.sequence_head.3.bias\"\n",
    "#             ]:\n",
    "#                 param.requires_grad = True\n",
    "#             else:\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         ## need to add peft here ? Do we ?\n",
    "    \n",
    "#     def print_trainable_parameters(self, model):\n",
    "#         trainable_params = 0\n",
    "#         all_param = 0\n",
    "#         for _, param in model.named_parameters():\n",
    "#             all_param += param.numel()\n",
    "#             if param.requires_grad:\n",
    "#                 trainable_params += param.numel()\n",
    "#         print(\n",
    "#             f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "#         )\n",
    "\n",
    "#     def generate_batch(self, sequence_prompt, structure_prompt, batch_size, ref=False):\n",
    "#         sequence_prediction_config = GenerationConfig(\n",
    "#             track=\"sequence\", \n",
    "#             num_steps=sequence_prompt.count(\"_\") // 2, \n",
    "#             temperature=0.5\n",
    "#         )\n",
    "#         esm_protein = ESMProtein(sequence=sequence_prompt, coordinates=structure_prompt)\n",
    "        \n",
    "#         generated_proteins = []\n",
    "#         for _ in tqdm(range(batch_size)):\n",
    "#             generated_protein = self.model.generate(esm_protein, sequence_prediction_config)\n",
    "#             generated_proteins.append(generated_protein.sequence)\n",
    "\n",
    "#         return generated_proteins\n",
    "    \n",
    "#     def get_mask_positions(self, sequence_prompt, mask_var='_'):\n",
    "#         positions = []\n",
    "#         for i in range(len(sequence_prompt)):\n",
    "#             if sequence_prompt[i] == mask_var:\n",
    "#                 positions.append(i)\n",
    "\n",
    "#         return np.array(positions)\n",
    "    \n",
    "#     def __get_n_mutations(self, sequence):\n",
    "#         base = 'APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELGGHSLLAVRLLGRLRQHGLGLQMRDLFEAPVLAELATRLRPYQPLEVPANGITPDTTVLTPEMLPLVTLS'\n",
    "#         assert len(base) == len(sequence)\n",
    "\n",
    "#         count = 0\n",
    "#         for aa1, aa2 in zip(base, sequence):\n",
    "#             if aa1 != aa2:\n",
    "#                 count += 1\n",
    "\n",
    "#         return -1*count\n",
    "    \n",
    "#     def get_property_batch(self, batch):\n",
    "#         for item in batch:\n",
    "#             item['property'] = self.__get_n_mutations(item['masked_sequence'])\n",
    "\n",
    "#         return batch\n",
    "    \n",
    "#     def get_PLL(self, item, ref=False):\n",
    "#         '''\n",
    "#             We always feed in the sequence prompt\n",
    "#             We get Log Prob(A_C) -  entire T region is masked\n",
    "#         '''\n",
    "#         proteins = [ESMProtein(sequence=item['sequence_prompt'], \n",
    "#                         coordinates=item['structure_prompt'])]\n",
    "        \n",
    "#         if ref:\n",
    "#             input_tokens = [self.model_ref.encode(protein) for protein in proteins]\n",
    "#             generated_tokens = self.model_ref.encode(ESMProtein(sequence=item['generated_sequence'], \n",
    "#                                                         coordinates=item['structure_prompt']))\n",
    "#         else:\n",
    "#             input_tokens = [self.model.encode(protein) for protein in proteins]\n",
    "#             generated_tokens = self.model.encode(ESMProtein(sequence=item['generated_sequence'], \n",
    "#                                                         coordinates=item['structure_prompt']))\n",
    "        \n",
    "        \n",
    "#         devices = set([t.device for t in input_tokens])\n",
    "#         if len(devices) > 1:\n",
    "#                 raise AttributeError(f\"Input tokens on multiple devices {devices}\")\n",
    "#         sequence_lengths = [len(tokens) for tokens in input_tokens]\n",
    "#         assert len(set(sequence_lengths)) == 1\n",
    "        \n",
    "#         if ref:\n",
    "#             batched_tokens = _stack_protein_tensors(\n",
    "#                     input_tokens, sequence_lengths, self.model_ref.tokenizers, self.model_ref.device\n",
    "#                 )\n",
    "#         else:\n",
    "#             batched_tokens = _stack_protein_tensors(\n",
    "#                     input_tokens, sequence_lengths, self.model.tokenizers, self.model.device\n",
    "#                 )\n",
    "        \n",
    "#         if batched_tokens.coordinates is None:\n",
    "#             per_res_plddt = None\n",
    "#         else:\n",
    "#             # 1.0 if all coordinates at specific indices have valid non-nan values.\n",
    "#             per_res_plddt = batched_tokens.coordinates.isfinite().all(dim=-1).any(dim=-1).float()\n",
    "\n",
    "#         if ref:\n",
    "#             assert model_ref.device == batched_tokens.device\n",
    "#             with (torch.no_grad(),\n",
    "#                 torch.autocast(enabled=True, device_type=torch.device(batched_tokens.device).type, dtype=torch.bfloat16)):\n",
    "#                 output = self.model_ref.forward(\n",
    "#                         sequence_tokens=batched_tokens.sequence,\n",
    "#                         structure_tokens=batched_tokens.structure,\n",
    "#                         ss8_tokens=batched_tokens.secondary_structure,\n",
    "#                         sasa_tokens=batched_tokens.sasa,\n",
    "#                         function_tokens=batched_tokens.function,\n",
    "#                         residue_annotation_tokens=batched_tokens.residue_annotations,\n",
    "#                         average_plddt=torch.tensor(1.0, device=batched_tokens.device),\n",
    "#                         per_res_plddt=per_res_plddt,\n",
    "#                         structure_coords=batched_tokens.coordinates,\n",
    "#                         chain_id=None,\n",
    "#                         sequence_id=None,\n",
    "#                 )\n",
    "#         else:\n",
    "#             assert model.device == batched_tokens.device\n",
    "#             with (torch.autocast(enabled=True, device_type=torch.device(batched_tokens.device).type, dtype=torch.bfloat16)):\n",
    "#                 output = self.model.forward(\n",
    "#                         sequence_tokens=batched_tokens.sequence,\n",
    "#                         structure_tokens=batched_tokens.structure,\n",
    "#                         ss8_tokens=batched_tokens.secondary_structure,\n",
    "#                         sasa_tokens=batched_tokens.sasa,\n",
    "#                         function_tokens=batched_tokens.function,\n",
    "#                         residue_annotation_tokens=batched_tokens.residue_annotations,\n",
    "#                         average_plddt=torch.tensor(1.0, device=batched_tokens.device),\n",
    "#                         per_res_plddt=per_res_plddt,\n",
    "#                         structure_coords=batched_tokens.coordinates,\n",
    "#                         chain_id=None,\n",
    "#                         sequence_id=None,\n",
    "#                 )\n",
    "        \n",
    "#         log_prob = torch.log_softmax(output.sequence_logits, dim=-1)\n",
    "\n",
    "#         masked_log_prob = log_prob[:, item['masked_positions'] + 1, generated_tokens.sequence[item['masked_positions'] + 1]]\n",
    "#         pll = torch.sum(masked_log_prob)\n",
    "\n",
    "#         # print(item['masked_sequence'])\n",
    "#         print(generated_tokens.sequence[item['masked_positions'] + 1])\n",
    "#         print(masked_log_prob)\n",
    "#         # print(pll)\n",
    "\n",
    "#         return pll\n",
    "         \n",
    "#     def dpo_paired_loss(self, batch):\n",
    "\n",
    "#         pll_policy = []\n",
    "#         pll_ref = []\n",
    "#         for item in batch:\n",
    "#             pll_policy.append(self.get_PLL(item))\n",
    "#             pll_ref.append(self.get_PLL(item, ref=True))\n",
    "#             print(item['property'])\n",
    "\n",
    "#         print([x['property'] for x in batch])\n",
    "#         print(pll_policy)\n",
    "#         print(pll_ref)\n",
    "\n",
    "#         pll_ref = [x.to(self.model.device) for x in pll_ref]\n",
    "\n",
    "#         print(pll_ref)\n",
    "\n",
    "#         batch_size = len(batch)\n",
    "#         assert batch_size > 1\n",
    "#         beta = 0.01\n",
    "\n",
    "#         loss = []\n",
    "\n",
    "#         pairs_debug = []\n",
    "#         for i in range(batch_size-1):\n",
    "#             for j in range(1, batch_size):\n",
    "#                 if batch[i]['property'] > batch[j]['property'] + 0:\n",
    "#                     loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[i] - pll_ref[i]) - (pll_policy[j] - pll_ref[j])) )\n",
    "#                     # loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[i]) - (pll_policy[j])) )\n",
    "#                     loss.append(loss_item)\n",
    "\n",
    "#                     pairs_debug.append((batch[i]['property'], batch[j]['property'],\n",
    "#                                         (pll_policy[i] - pll_ref[i]).item(), (pll_policy[j] - pll_ref[j]).item(),\n",
    "#                                         loss_item.item()))\n",
    "                    \n",
    "#                 elif batch[j]['property'] > batch[i]['property'] + 0:\n",
    "#                     loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[j] - pll_ref[j]) - (pll_policy[i] - pll_ref[i])) )\n",
    "#                     # loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[j]) - (pll_policy[i])) )\n",
    "#                     loss.append(loss_item)\n",
    "\n",
    "#                     pairs_debug.append((batch[j]['property'], batch[i]['property'],\n",
    "#                                         (pll_policy[j] - pll_ref[j]).item(), (pll_policy[i] - pll_ref[i]).item(),\n",
    "#                                         loss_item.item()))\n",
    "                    \n",
    "#         for _pair in pairs_debug:\n",
    "#             print(f'pair :{_pair}')\n",
    "\n",
    "#         if len(loss) > 0:\n",
    "#             loss = torch.mean(torch.stack(loss))\n",
    "#         else:\n",
    "#             loss = torch.tensor(0.).to(self.model.device)\n",
    "                    \n",
    "#         return loss\n",
    " \n",
    "#     def train(self, sequence_prompt, structure_prompt):\n",
    "\n",
    "#         self.model.train()\n",
    "\n",
    "#         self.print_trainable_parameters(self.model)\n",
    "#         self.print_trainable_parameters(self.model_ref)\n",
    "\n",
    "#         optimizer = torch.optim.AdamW(\n",
    "#             self.model.parameters(),\n",
    "#             lr=1e-3,\n",
    "#             betas=(0.9, 0.98),\n",
    "#             eps=1e-8,\n",
    "#             weight_decay=0.1,\n",
    "#         )\n",
    "\n",
    "#         mask_positions = self.get_mask_positions(sequence_prompt=sequence_prompt)\n",
    "\n",
    "#         epochs = 100\n",
    "#         batch_size = 1\n",
    "#         for epoch in range(epochs):\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             batch_dataset = []\n",
    "#             print(f'epoch: {epoch}: Generating batch')\n",
    "#             batch_seq = self.generate_batch(sequence_prompt=sequence_prompt, \n",
    "#                                         structure_prompt=structure_prompt,\n",
    "#                                         batch_size=batch_size)\n",
    "            \n",
    "#             for _seq in batch_seq:\n",
    "#                 batch_dataset.append({\n",
    "#                     'generated_sequence': _seq,\n",
    "#                     'masked_sequence': ''.join([s for _i, s in enumerate(_seq) if _i in mask_positions]),\n",
    "#                     'masked_positions': mask_positions, ## These are the same\n",
    "#                     'sequence_prompt': sequence_prompt, ## These are the same\n",
    "#                     'structure_prompt': structure_prompt ## These are the same        \n",
    "#                 })\n",
    "\n",
    "#             A, C = sequence_prompt.split('_'*115)\n",
    "#             base = 'APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELGGHSLLAVRLLGRLRQHGLGLQMRDLFEAPVLAELATRLRPYQPLEVPANGITPDTTVLTPEMLPLVTLS'\n",
    "#             batch_dataset.append({\n",
    "#                 'generated_sequence': A+base+C,\n",
    "#                 'masked_sequence': base,\n",
    "#                 'masked_positions': mask_positions, ## These are the same\n",
    "#                 'sequence_prompt': sequence_prompt, ## These are the same\n",
    "#                 'structure_prompt': structure_prompt ## These are the same  \n",
    "#             })\n",
    "\n",
    "#             batch_dataset = self.get_property_batch(batch_dataset)\n",
    "            \n",
    "#             print(f'epoch {epoch}: calculating loss')\n",
    "#             loss = self.dpo_paired_loss(batch_dataset)\n",
    "#             print(f'loss {loss.item()}')\n",
    "\n",
    "#             if loss != 0:\n",
    "#                 print(f'epoch {epoch}: optimizer step')\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESM3.from_pretrained(\"esm3_sm_open_v1\").to('cuda')\n",
    "# model_ref = ESM3.from_pretrained(\"esm3_sm_open_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2692.4638671875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11156.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer:\n",
    "    def __init__(self, model, model_ref, device='cpu') -> None:\n",
    "        self.device = device\n",
    "\n",
    "        # if self.device == 'gpu':\n",
    "        #     self.model = model.to(\"cuda:0\")\n",
    "        #     self.model_ref = model_ref.to(\"cuda:1\")\n",
    "        # else:\n",
    "        #     self.model = model.to(\"cpu\")\n",
    "        #     self.model_ref = model_ref.to(\"cpu\")\n",
    "\n",
    "        self.model = model\n",
    "        self.model_ref =  model_ref\n",
    "\n",
    "        self.model_ref.eval()\n",
    "        for pm in self.model_ref.parameters():\n",
    "            pm.requires_grad = False\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in [\n",
    "                \"encoder.sequence_embed.weight\", \n",
    "                \"output_heads.sequence_head.0.weight\", \n",
    "                \"output_heads.sequence_head.0.bias\",\n",
    "                \"output_heads.sequence_head.2.weight\",\n",
    "                \"output_heads.sequence_head.2.bias\",\n",
    "                \"output_heads.sequence_head.3.weight\",\n",
    "                \"output_heads.sequence_head.3.bias\"\n",
    "            ]:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        ## need to add peft here ? Do we ?\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def generate_sequences(self, sequence_prompt, structure_prompt, size, ref=False):\n",
    "        sequence_prediction_config = GenerationConfig(\n",
    "            track=\"sequence\", \n",
    "            num_steps=sequence_prompt.count(\"_\") // 2, \n",
    "            temperature=0.5\n",
    "        )\n",
    "        esm_protein = ESMProtein(sequence=sequence_prompt, coordinates=structure_prompt)\n",
    "        \n",
    "        generated_proteins = []\n",
    "        for _ in tqdm(range(size)):\n",
    "            generated_protein = self.model.generate(esm_protein, sequence_prediction_config)\n",
    "            generated_proteins.append(generated_protein.sequence)\n",
    "\n",
    "        return generated_proteins\n",
    "    \n",
    "    def create_dataset(self, sequence_prompt, structure_prompt, mask_positions, size):\n",
    "\n",
    "        generated_sequences = self.generate_sequences(sequence_prompt, structure_prompt, size)\n",
    "\n",
    "        dataset = []\n",
    "        for _seq in generated_sequences:\n",
    "            dataset.append({\n",
    "                'generated_sequence': _seq,\n",
    "                'masked_sequence': ''.join([s for _i, s in enumerate(_seq) if _i in mask_positions]),\n",
    "                'masked_positions': mask_positions, ## These are the same\n",
    "                'sequence_prompt': sequence_prompt, ## These are the same\n",
    "                'structure_prompt': structure_prompt, ## These are the same   \n",
    "            })\n",
    "\n",
    "        for item in dataset:\n",
    "            item['property'] = self.__get_n_mutations(item['masked_sequence'])\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_mask_positions(self, sequence_prompt, mask_var='_'):\n",
    "        positions = []\n",
    "        for i in range(len(sequence_prompt)):\n",
    "            if sequence_prompt[i] == mask_var:\n",
    "                positions.append(i)\n",
    "\n",
    "        return np.array(positions)\n",
    "    \n",
    "    def __get_n_mutations(self, sequence):\n",
    "        base = 'APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELGGHSLLAVRLLGRLRQHGLGLQMRDLFEAPVLAELATRLRPYQPLEVPANGITPDTTVLTPEMLPLVTLS'\n",
    "        assert len(base) == len(sequence)\n",
    "\n",
    "        count = 0\n",
    "        for aa1, aa2 in zip(base, sequence):\n",
    "            if aa1 != aa2:\n",
    "                count += 1\n",
    "\n",
    "        return -1*count\n",
    "    \n",
    "    # def get_property_batch(self, batch):\n",
    "    #     for item in batch:\n",
    "    #         item['property'] = self.__get_n_mutations(item['masked_sequence'])\n",
    "\n",
    "    #     return batch\n",
    "    \n",
    "    def get_PLL(self, item, ref=False):\n",
    "        '''\n",
    "            We always feed in the sequence prompt\n",
    "            We get Log Prob(A_C) -  entire T region is masked\n",
    "        '''\n",
    "        proteins = [ESMProtein(sequence=item['sequence_prompt'], \n",
    "                        coordinates=item['structure_prompt'])]\n",
    "        \n",
    "        if ref:\n",
    "            input_tokens = [self.model_ref.encode(protein) for protein in proteins]\n",
    "            generated_tokens = self.model_ref.encode(ESMProtein(sequence=item['generated_sequence'], \n",
    "                                                        coordinates=item['structure_prompt']))\n",
    "        else:\n",
    "            input_tokens = [self.model.encode(protein) for protein in proteins]\n",
    "            generated_tokens = self.model.encode(ESMProtein(sequence=item['generated_sequence'], \n",
    "                                                        coordinates=item['structure_prompt']))\n",
    "        \n",
    "        \n",
    "        devices = set([t.device for t in input_tokens])\n",
    "        if len(devices) > 1:\n",
    "                raise AttributeError(f\"Input tokens on multiple devices {devices}\")\n",
    "        sequence_lengths = [len(tokens) for tokens in input_tokens]\n",
    "        assert len(set(sequence_lengths)) == 1\n",
    "        \n",
    "        if ref:\n",
    "            batched_tokens = _stack_protein_tensors(\n",
    "                    input_tokens, sequence_lengths, self.model_ref.tokenizers, self.model_ref.device\n",
    "                )\n",
    "        else:\n",
    "            batched_tokens = _stack_protein_tensors(\n",
    "                    input_tokens, sequence_lengths, self.model.tokenizers, self.model.device\n",
    "                )\n",
    "        \n",
    "        if batched_tokens.coordinates is None:\n",
    "            per_res_plddt = None\n",
    "        else:\n",
    "            # 1.0 if all coordinates at specific indices have valid non-nan values.\n",
    "            per_res_plddt = batched_tokens.coordinates.isfinite().all(dim=-1).any(dim=-1).float()\n",
    "\n",
    "        if ref:\n",
    "            assert model_ref.device == batched_tokens.device\n",
    "            with (torch.no_grad(),\n",
    "                torch.autocast(enabled=True, device_type=torch.device(batched_tokens.device).type, dtype=torch.bfloat16)):\n",
    "                output = self.model_ref.forward(\n",
    "                        sequence_tokens=batched_tokens.sequence,\n",
    "                        structure_tokens=batched_tokens.structure,\n",
    "                        ss8_tokens=batched_tokens.secondary_structure,\n",
    "                        sasa_tokens=batched_tokens.sasa,\n",
    "                        function_tokens=batched_tokens.function,\n",
    "                        residue_annotation_tokens=batched_tokens.residue_annotations,\n",
    "                        average_plddt=torch.tensor(1.0, device=batched_tokens.device),\n",
    "                        per_res_plddt=per_res_plddt,\n",
    "                        structure_coords=batched_tokens.coordinates,\n",
    "                        chain_id=None,\n",
    "                        sequence_id=None,\n",
    "                )\n",
    "        else:\n",
    "            assert model.device == batched_tokens.device\n",
    "            with (torch.autocast(enabled=True, device_type=torch.device(batched_tokens.device).type, dtype=torch.bfloat16)):\n",
    "                output = self.model.forward(\n",
    "                        sequence_tokens=batched_tokens.sequence,\n",
    "                        structure_tokens=batched_tokens.structure,\n",
    "                        ss8_tokens=batched_tokens.secondary_structure,\n",
    "                        sasa_tokens=batched_tokens.sasa,\n",
    "                        function_tokens=batched_tokens.function,\n",
    "                        residue_annotation_tokens=batched_tokens.residue_annotations,\n",
    "                        average_plddt=torch.tensor(1.0, device=batched_tokens.device),\n",
    "                        per_res_plddt=per_res_plddt,\n",
    "                        structure_coords=batched_tokens.coordinates,\n",
    "                        chain_id=None,\n",
    "                        sequence_id=None,\n",
    "                )\n",
    "        \n",
    "        log_prob = torch.log_softmax(output.sequence_logits, dim=-1)\n",
    "\n",
    "        masked_log_prob = log_prob[:, item['masked_positions'] + 1, generated_tokens.sequence[item['masked_positions'] + 1]]\n",
    "        pll = torch.sum(masked_log_prob)\n",
    "\n",
    "        # print(item['masked_sequence'])\n",
    "        print(generated_tokens.sequence[item['masked_positions'] + 1])\n",
    "        print(masked_log_prob)\n",
    "        # print(pll)\n",
    "\n",
    "        return pll\n",
    "         \n",
    "    def dpo_paired_loss(self, batch):\n",
    "\n",
    "        pll_policy = []\n",
    "        pll_ref = []\n",
    "        for item in batch:\n",
    "            pll_policy.append(self.get_PLL(item))\n",
    "            pll_ref.append(self.get_PLL(item, ref=True))\n",
    "            print(item['property'])\n",
    "\n",
    "        print([x['property'] for x in batch])\n",
    "        print(pll_policy)\n",
    "        print(pll_ref)\n",
    "\n",
    "        pll_ref = [x.to(self.model.device) for x in pll_ref]\n",
    "\n",
    "        print(pll_ref)\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        assert batch_size > 1\n",
    "        beta = 0.01\n",
    "\n",
    "        loss = []\n",
    "\n",
    "        pairs_debug = []\n",
    "        for i in range(batch_size-1):\n",
    "            for j in range(1, batch_size):\n",
    "                if batch[i]['property'] > batch[j]['property'] + 0:\n",
    "                    loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[i] - pll_ref[i]) - (pll_policy[j] - pll_ref[j])) )\n",
    "                    # loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[i]) - (pll_policy[j])) )\n",
    "                    loss.append(loss_item)\n",
    "\n",
    "                    pairs_debug.append((batch[i]['property'], batch[j]['property'],\n",
    "                                        (pll_policy[i] - pll_ref[i]).item(), (pll_policy[j] - pll_ref[j]).item(),\n",
    "                                        loss_item.item()))\n",
    "                    \n",
    "                elif batch[j]['property'] > batch[i]['property'] + 0:\n",
    "                    loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[j] - pll_ref[j]) - (pll_policy[i] - pll_ref[i])) )\n",
    "                    # loss_item = -torch.nn.functional.logsigmoid( beta * ((pll_policy[j]) - (pll_policy[i])) )\n",
    "                    loss.append(loss_item)\n",
    "\n",
    "                    pairs_debug.append((batch[j]['property'], batch[i]['property'],\n",
    "                                        (pll_policy[j] - pll_ref[j]).item(), (pll_policy[i] - pll_ref[i]).item(),\n",
    "                                        loss_item.item()))\n",
    "                    \n",
    "        for _pair in pairs_debug:\n",
    "            print(f'pair :{_pair}')\n",
    "\n",
    "        if len(loss) > 0:\n",
    "            loss = torch.mean(torch.stack(loss))\n",
    "        else:\n",
    "            loss = torch.tensor(0.).to(self.model.device)\n",
    "                    \n",
    "        return loss\n",
    " \n",
    "    def train(self, sequence_prompt, structure_prompt):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        self.print_trainable_parameters(self.model)\n",
    "        self.print_trainable_parameters(self.model_ref)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-8,\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "\n",
    "        mask_positions = self.get_mask_positions(sequence_prompt=sequence_prompt)\n",
    "\n",
    "        dataset = self.create_dataset(sequence_prompt, structure_prompt,\n",
    "                                      mask_positions, 10)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "        # epochs = 100\n",
    "        # batch_size = 1\n",
    "        # for epoch in range(epochs):\n",
    "        #     optimizer.zero_grad()\n",
    "\n",
    "        #     # A, C = sequence_prompt.split('_'*115)\n",
    "        #     # base = 'APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELGGHSLLAVRLLGRLRQHGLGLQMRDLFEAPVLAELATRLRPYQPLEVPANGITPDTTVLTPEMLPLVTLS'\n",
    "        #     # batch_dataset.append({\n",
    "        #     #     'generated_sequence': A+base+C,\n",
    "        #     #     'masked_sequence': base,\n",
    "        #     #     'masked_positions': mask_positions, ## These are the same\n",
    "        #     #     'sequence_prompt': sequence_prompt, ## These are the same\n",
    "        #     #     'structure_prompt': structure_prompt ## These are the same  \n",
    "        #     # })\n",
    "\n",
    "        #     batch_dataset = self.get_property_batch(batch_dataset)\n",
    "            \n",
    "        #     print(f'epoch {epoch}: calculating loss')\n",
    "        #     loss = self.dpo_paired_loss(batch_dataset)\n",
    "        #     print(f'loss {loss.item()}')\n",
    "\n",
    "        #     if loss != 0:\n",
    "        #         print(f'epoch {epoch}: optimizer step')\n",
    "        #         loss.backward()\n",
    "        #         optimizer.step()\n",
    "\n",
    "        #     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = ProteinChain.from_pdb('../../Data/gxps/gxps_ATC_AF.pdb')\n",
    "\n",
    "sequence_prompt = ''.join([protein[i].sequence if i in A_gxps_atc + C_gxps_atc else '_' for i in range(len(protein))])\n",
    "# structure_prompt = torch.full((len(sequence_prompt), 37, 3), np.nan)\n",
    "# structure_prompt[T_gxps_atc] = torch.tensor(protein.atom37_positions)[T_gxps_atc]\n",
    "# structure_prompt = torch.tensor(protein.atom37_positions)[T_gxps_atc[0]-50:T_gxps_atc[-1]+50]\n",
    "\n",
    "# sequence_prompt = ''.join([s for i, s in enumerate(sequence_prompt) if i in list(range(T_gxps_atc[0]-50,T_gxps_atc[-1]+50))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034,\n",
       " 'VCVHQLFEQQIEKTPDAIAVIYENQTLSYAELNARANRLAHQLIALGVAPDQRVAICVTRSLARIIGLLAVLKAGGAYVPLDPAYPGERLAYMLTDATPVILMADNVGRAALSEDILATLTVLDPNTLLEQPDHNPQVSGLTPQHLAYVIYTSGSTGRPKGVMIEHRSVVNLTLTQITQFDVCATSRMLQFASFGFDASVWEIMMALSCGAMLVIPTETVRQDPQRLWRYLEEQAITHACLTPAMFHDGTDLPAIAIKPTLIFAGEAPSPALFQALCSRADLFNAYGPTEITVCATTWDCPADYTGGVIPIGSPVANKRLYLLDEHRQPVPLGTVGELYIGGVGVARGYLNRPELTAERFLNDPFSDETNARMYRAGDLARYLPDGNLVFVGRNDQQVKIRGFRIEPGEIEARLVEHSEVSEALVLALGDGQDKRLVAYVVALADDGLATKLREHLSDILPDYMIPAAFVRLDAFPLTPNGKLDRRSLP___________________________________________________________________________________________________________________QAEIDRIVEQVPGGIANIQDIYALSPLQDGILFHHLLANEGDPYLLITQQAFADRPLLNRYLAAVQQVVDRHDILRTAFIWEGLSVPAQVICRQAPLSVTELTLNPADGAISNQLAQRFDPRRHRIDLNQAPLLRFVVAQESDGRWILLQLLHHLIGDHTTLEVMNSEVQACLLGQMDSLPAPVPFRHLVAQARQGVSQAEHTRFFTDMLAEVDEPTLLFGLAEAHHDGSQVTESHRMLTAGLNERLRGQARRLGVSVAALCHLAWAQVLSRTSGQTQVVFGTVLFGRMQAGEGSDSGMGLFINTLPLRLDIDNTPVRDSVRAAHSRLAGLLEHEHASLALAQRCSGVESGTPLFNALLNYRHNTQPVTPDEIVSGIEFLGAQERTNYPFVLSVEDSGSDLGLTAQVVQPFDPERICGYMQQALASLVQA')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_prompt), sequence_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'structure_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(sequence_prompt) \u001b[38;5;241m==\u001b[39m \u001b[43mstructure_prompt\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'structure_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "len(sequence_prompt) == structure_prompt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = model\n",
    "dpo_trainer = DPOTrainer(model, model_ref, device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DISABLE_ITERATIVE_SAMPLING_TQDM\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2560576 || all params: 1401735748 || trainable%: 0.18\n",
      "trainable params: 2560576 || all params: 1401735748 || trainable%: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:59<00:00,  5.97s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = dpo_trainer.train(sequence_prompt=sequence_prompt, structure_prompt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(dataset):\n",
    "    \n",
    "    pair_dataset = []\n",
    "    for item_1 in batch[:-1]:\n",
    "        for item_2 in batch[1:]:\n",
    "            if item_1['property'] < item_2['property'] and item_1['log_likelihood'] < item_2['log_likelihood']:\n",
    "                pair_dataset.append((item_1, item_2))\n",
    "            elif item_1['property'] > item_2['property'] and item_1['log_likelihood'] > item_2['log_likelihood']:\n",
    "                pair_dataset.append((item_2, item_1))\n",
    "\n",
    "    # _, ax = plt.subplots(1, 2, figsize=(7, 3), layout='constrained')\n",
    "    # ax[0].hist([[item[0]['property'] for item in pair_dataset], \n",
    "    #             [item[1]['property'] for item in pair_dataset]], label=['pos', 'neg'])\n",
    "    # ax[1].hist([[item[0]['log_likelihood'] for item in pair_dataset],\n",
    "    #             [item[1]['log_likelihood'] for item in pair_dataset]], label=['pos', 'neg'])\n",
    "    # ax[0].legend()\n",
    "    # ax[1].legend()\n",
    "    # plt.show()\n",
    "\n",
    "    return pair_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
