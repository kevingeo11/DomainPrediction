{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 09:38:36.511716: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 09:38:37.317420: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-09 09:38:37.886998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 09:38:38.478082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 09:38:38.483439: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 09:38:39.680193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 09:38:42.438261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, inject_adapter_in_model, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2Regression(nn.Module):\n",
    "    def __init__(self, model_path) -> None:\n",
    "        super().__init__()\n",
    "        self.esm2, self.alphabet = esm.pretrained.load_model_and_alphabet(model_path)\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.tok_to_idx = self.alphabet.tok_to_idx\n",
    "        self.idx_to_tok = {v:k for k,v in self.tok_to_idx.items()}\n",
    "\n",
    "    def forward(self, batch_tokens):\n",
    "        rep = self.esm2(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        embedding = rep['representations'][33].mean(1)\n",
    "\n",
    "        pred = self.mlp(embedding)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    # def get_res_batch(self, sequences):\n",
    "    #     data = [\n",
    "    #         (f\"P{i+1}\", seq) for i, seq in enumerate(sequences)\n",
    "    #     ]\n",
    "    #     batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
    "    #     batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "\n",
    "    #     if self.device == 'gpu':\n",
    "    #         batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         results = self.model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "\n",
    "    #     return results, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../..'\n",
    "data_path = os.path.join(root, 'Data/al_test_experiments/Evolvepro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path, 'brenan.csv')\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>0.265834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>0.397712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>0.324874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MFAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>0.151913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MGAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>0.261203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  function\n",
       "0  MCAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...  0.265834\n",
       "1  MDAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...  0.397712\n",
       "2  MEAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...  0.324874\n",
       "3  MFAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...  0.151913\n",
       "4  MGAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...  0.261203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESM2Regression(model_path='/data/users/kgeorge/workspace/esm2/checkpoints/esm2_t33_650M_UR50D.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2.embed_tokens.weight 42240 torch.Size([33, 1280]) True\n",
      "esm2.layers.0.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.0.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.0.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.0.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.0.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.0.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.0.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.0.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.0.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.1.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.1.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.1.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.1.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.1.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.1.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.1.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.1.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.2.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.2.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.2.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.2.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.2.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.2.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.2.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.2.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.3.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.3.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.3.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.3.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.3.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.3.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.3.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.3.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.4.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.4.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.4.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.4.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.4.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.4.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.4.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.4.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.5.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.5.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.5.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.5.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.5.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.5.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.5.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.5.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.6.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.6.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.6.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.6.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.6.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.6.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.6.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.6.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.7.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.7.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.7.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.7.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.7.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.7.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.7.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.7.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.8.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.8.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.8.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.8.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.8.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.8.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.8.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.8.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.9.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.9.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.9.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.9.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.9.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.9.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.9.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.9.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.10.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.10.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.10.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.10.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.10.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.10.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.10.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.10.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.11.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.11.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.11.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.11.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.11.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.11.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.11.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.11.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.12.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.12.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.12.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.12.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.12.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.12.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.12.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.12.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.13.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.13.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.13.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.13.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.13.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.13.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.13.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.13.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.14.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.14.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.14.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.14.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.14.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.14.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.14.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.14.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.15.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.15.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.15.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.15.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.15.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.15.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.15.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.15.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.16.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.16.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.16.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.16.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.16.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.16.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.16.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.16.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.17.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.17.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.17.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.17.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.17.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.17.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.17.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.17.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.18.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.18.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.18.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.18.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.18.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.18.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.18.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.18.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.19.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.19.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.19.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.19.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.19.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.19.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.19.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.19.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.20.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.20.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.20.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.20.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.20.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.20.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.20.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.20.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.21.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.21.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.21.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.21.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.21.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.21.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.21.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.21.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.22.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.22.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.22.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.22.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.22.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.22.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.22.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.22.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.23.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.23.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.23.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.23.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.23.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.23.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.23.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.23.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.24.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.24.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.24.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.24.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.24.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.24.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.24.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.24.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.25.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.25.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.25.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.25.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.25.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.25.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.25.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.25.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.26.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.26.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.26.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.26.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.26.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.26.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.26.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.26.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.27.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.27.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.27.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.27.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.27.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.27.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.27.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.27.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.28.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.28.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.28.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.28.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.28.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.28.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.28.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.28.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.29.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.29.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.29.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.29.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.29.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.29.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.29.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.29.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.30.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.30.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.30.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.30.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.30.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.30.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.30.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.30.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.31.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.31.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.31.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.31.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.31.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.31.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.31.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.31.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn.k_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.32.self_attn.k_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn.v_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.32.self_attn.v_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn.q_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.32.self_attn.q_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn.out_proj.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.layers.32.self_attn.out_proj.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.self_attn_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.fc1.weight 6553600 torch.Size([5120, 1280]) True\n",
      "esm2.layers.32.fc1.bias 5120 torch.Size([5120]) True\n",
      "esm2.layers.32.fc2.weight 6553600 torch.Size([1280, 5120]) True\n",
      "esm2.layers.32.fc2.bias 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.final_layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.layers.32.final_layer_norm.bias 1280 torch.Size([1280]) True\n",
      "esm2.contact_head.regression.weight 660 torch.Size([1, 660]) True\n",
      "esm2.contact_head.regression.bias 1 torch.Size([1]) True\n",
      "esm2.emb_layer_norm_after.weight 1280 torch.Size([1280]) True\n",
      "esm2.emb_layer_norm_after.bias 1280 torch.Size([1280]) True\n",
      "esm2.lm_head.bias 33 torch.Size([33]) True\n",
      "esm2.lm_head.dense.weight 1638400 torch.Size([1280, 1280]) True\n",
      "esm2.lm_head.dense.bias 1280 torch.Size([1280]) True\n",
      "esm2.lm_head.layer_norm.weight 1280 torch.Size([1280]) True\n",
      "esm2.lm_head.layer_norm.bias 1280 torch.Size([1280]) True\n",
      "mlp.0.weight 655360 torch.Size([512, 1280]) True\n",
      "mlp.0.bias 512 torch.Size([512]) True\n",
      "mlp.2.weight 512 torch.Size([1, 512]) True\n",
      "mlp.2.bias 1 torch.Size([1]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.numel(), param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "            (fun, seq) for i, (seq, fun) in enumerate(zip(df['seq'][:3], df['function'][:3]))\n",
    "        ]\n",
    "batch_labels, batch_strs, batch_tokens = model.batch_converter(data)\n",
    "batch_lens = (batch_tokens != model.alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=1,\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,510,307 || all params: 654,740,919 || trainable%: 0.5361\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'mlp' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,166,179 || all params: 654,740,919 || trainable%: 0.6363\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm2.layers.0.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.0.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.0.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.0.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.0.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.0.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.0.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.1.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.1.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.1.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.1.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.1.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.1.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.1.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.2.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.2.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.2.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.2.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.2.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.2.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.2.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.3.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.3.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.3.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.3.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.3.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.3.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.3.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.4.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.4.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.4.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.4.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.4.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.4.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.4.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.5.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.5.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.5.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.5.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.5.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.5.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.5.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.6.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.6.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.6.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.6.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.6.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.6.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.6.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.7.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.7.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.7.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.7.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.7.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.7.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.7.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.8.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.8.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.8.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.8.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.8.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.8.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.8.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.9.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.9.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.9.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.9.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.9.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.9.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.9.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.10.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.10.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.10.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.10.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.10.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.10.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.10.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.11.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.11.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.11.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.11.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.11.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.11.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.11.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.12.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.12.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.12.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.12.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.12.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.12.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.12.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.13.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.13.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.13.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.13.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.13.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.13.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.13.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.14.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.14.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.14.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.14.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.14.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.14.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.14.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.15.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.15.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.15.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.15.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.15.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.15.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.15.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.16.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.16.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.16.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.16.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.16.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.16.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.16.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.17.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.17.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.17.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.17.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.17.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.17.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.17.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.18.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.18.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.18.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.18.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.18.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.18.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.18.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.19.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.19.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.19.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.19.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.19.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.19.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.19.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.20.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.20.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.20.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.20.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.20.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.20.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.20.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.21.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.21.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.21.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.21.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.21.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.21.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.21.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.22.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.22.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.22.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.22.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.22.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.22.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.22.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.23.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.23.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.23.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.23.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.23.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.23.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.23.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.24.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.24.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.24.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.24.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.24.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.24.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.24.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.25.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.25.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.25.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.25.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.25.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.25.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.25.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.26.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.26.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.26.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.26.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.26.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.26.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.26.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.27.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.27.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.27.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.27.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.27.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.27.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.27.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.28.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.28.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.28.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.28.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.28.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.28.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.28.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.29.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.29.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.29.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.29.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.29.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.29.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.29.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.30.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.30.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.30.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.30.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.30.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.30.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.30.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.31.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.31.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.31.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.31.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.31.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.31.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.31.final_layer_norm.bias\n",
      "base_model.model.esm2.layers.32.self_attn.k_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.32.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.32.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.32.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.out_proj.base_layer.bias\n",
      "base_model.model.esm2.layers.32.self_attn.out_proj.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn.out_proj.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.self_attn_layer_norm.bias\n",
      "base_model.model.esm2.layers.32.fc1.base_layer.bias\n",
      "base_model.model.esm2.layers.32.fc1.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.fc1.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.fc2.base_layer.bias\n",
      "base_model.model.esm2.layers.32.fc2.lora_A.default.weight\n",
      "base_model.model.esm2.layers.32.fc2.lora_B.default.weight\n",
      "base_model.model.esm2.layers.32.final_layer_norm.bias\n",
      "base_model.model.esm2.contact_head.regression.bias\n",
      "base_model.model.esm2.emb_layer_norm_after.bias\n",
      "base_model.model.esm2.lm_head.bias\n",
      "base_model.model.esm2.lm_head.dense.bias\n",
      "base_model.model.esm2.lm_head.layer_norm.bias\n",
      "base_model.model.mlp.0.weight\n",
      "base_model.model.mlp.0.bias\n",
      "base_model.model.mlp.2.weight\n",
      "base_model.model.mlp.2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0308],\n",
       "        [0.0306],\n",
       "        [0.0314]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2658, 0.3977, 0.3249])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class ESM2LoRALit(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        data = [\n",
    "            (fun, seq) for (seq, fun) in zip(x, y)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.model.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat = self.model(batch_tokens)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.accumulate_batch_loss_train.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        data = [\n",
    "            (fun, seq) for (seq, fun) in zip(x, y)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.model.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat = self.model(batch_tokens)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.accumulate_batch_loss_val.append(loss.item())\n",
    "    \n",
    "    @staticmethod\n",
    "    def trainmodel(model, X, y, val=True, debug=True):\n",
    "        '''\n",
    "            X - embeddings from esm2\n",
    "            X - shape (n, features)\n",
    "            y - shape (n, )\n",
    "        '''\n",
    "        if val:\n",
    "            batch_size = 1\n",
    "            idx = np.arange(X.shape[0])\n",
    "            train_idx, val_idx = train_test_split(idx, test_size=0.2)\n",
    "            train_dataset = ProteinFunDataset(X[train_idx], y[train_idx])\n",
    "            val_dataset = ProteinFunDataset(X[val_idx], y[val_idx])\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=5, verbose=False, mode=\"min\")\n",
    "\n",
    "            trainer = pl.Trainer(max_epochs=5, callbacks=[earlystopping_callback],\n",
    "                                 accelerator=\"auto\",\n",
    "                                 enable_progress_bar=True,\n",
    "                                 enable_model_summary=True,\n",
    "                                 precision=\"16-mixed\",\n",
    "                                 accumulate_grad_batches=32\n",
    "                                 )\n",
    "            trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "            ## Needs to change - we need to load the least val loss model\n",
    "            # y_pred = validation_step(X[val_idx])\n",
    "            # val_mse = mean_squared_error(y_pred, y[val_idx])\n",
    "            # print(f'Train end val mse: {val_mse}')\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Needs Fix\")\n",
    "\n",
    "    # def on_train_epoch_start(self):\n",
    "    #     self.accumulate_batch_loss_train.clear()\n",
    "    #     self.accumulate_batch_loss_val.clear()\n",
    "    \n",
    "    # def on_train_epoch_end(self):\n",
    "    #     if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "    #         print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    # def on_train_end(self):\n",
    "    #     print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    # def predict(self, X):\n",
    "    #     '''\n",
    "    #         X is numpy array\n",
    "    #     '''\n",
    "    #     with torch.no_grad():\n",
    "    #         y = self(torch.tensor(X))\n",
    "    #     return y.numpy().flatten()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "litmodel = ESM2LoRALit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(litmodel.model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A2000 12GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 654 M  | train\n",
      "--------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "650 M     Non-trainable params\n",
      "654 M     Total params\n",
      "2,618.964 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652493cc85264eaf933d4082e72134c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/kgeorge/miniconda3/envs/workspace-esm/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/nethome/kgeorge/miniconda3/envs/workspace-esm/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52a0eaab82a4328ac32ea385d61c766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eac58e054e4d31992f5d085e95f4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b83a39d6cc14dfabd206a55cb46dbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db61de2eec8493da493d5522454f21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43e9cac30a743ddb7572080b20638af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1150c8e66a44bb7bac353634435618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "ESM2LoRALit.trainmodel(litmodel, X=df['seq'].to_numpy()[:200], y=df['function'].to_numpy().astype(np.float16)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "            (fun, seq) for i, (seq, fun) in enumerate(zip(df['seq'][:10], df['function'][:10]))\n",
    "        ]\n",
    "batch_labels, batch_strs, batch_tokens = model.batch_converter(data)\n",
    "batch_lens = (batch_tokens != model.alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have identical weights\n"
     ]
    }
   ],
   "source": [
    "for param1, param2 in zip(model.parameters(), litmodel.model.parameters()):\n",
    "    if not torch.equal(param1.data, param2.data):\n",
    "        print(\"Models have different weights\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Models have identical weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 362])\n",
      "10 torch.Size([10, 362])\n",
      "20 torch.Size([10, 362])\n",
      "30 torch.Size([10, 362])\n",
      "40 torch.Size([10, 362])\n",
      "50 torch.Size([10, 362])\n",
      "60 torch.Size([10, 362])\n",
      "70 torch.Size([10, 362])\n",
      "80 torch.Size([10, 362])\n",
      "90 torch.Size([10, 362])\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "            (fun, seq) for i, (seq, fun) in enumerate(zip(df['seq'][:100], df['function'][:100]))\n",
    "        ]\n",
    "batch_labels, batch_strs, batch_tokens = model.batch_converter(data)\n",
    "batch_lens = (batch_tokens != model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "pred = []\n",
    "for i in range(0, 100, 10):\n",
    "    with torch.no_grad():\n",
    "        print(i, batch_tokens[i:i+10].shape)\n",
    "        y_pred = litmodel.model(batch_tokens[i:i+10])\n",
    "        pred.append(y_pred.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3da5131b70>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGiCAYAAADnfswJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2WklEQVR4nO3dfXBU133/8c9KRA8w3m1BQojByBpHGAzURjKwQgaiBsmlmAR7OiilAdzSOExwWoVOGmH8gOz5WTgTkiIbHOOm0ZAMiuypbGjDhKiDjaCaeKgseew0jakHW5SsSuWEXSELYaT7+4NorUVPe6V9uA/v18yOR9dXe88eVnu/+z3nfI/HMAxDAAAALpCS7AYAAAAkCoEPAABwDQIfAADgGgQ+AADANQh8AACAaxD4AAAA1yDwAQAArkHgAwAAXIPABwAAuAaBDwAAcI0JBT4HDx5Ufn6+MjIyVFRUpNOnT496bmNjo8rKypSdnS2v16vi4mKdOHEi4py6ujp5PJ5hj6tXr4bPaW5u1vr16zV79mx5PB699tprw65lGIb27Nmj2bNnKzMzU5/73Of0y1/+ciIvEQAAOJDpwKehoUGVlZXavXu32tratHLlSq1du1YdHR0jnt/c3KyysjIdP35cra2tKi0t1fr169XW1hZxntfrVSAQiHhkZGSE/39PT4/uuusuPf/886O27dvf/ra++93v6vnnn9fZs2c1a9YslZWVqbu72+zLBAAADuQxu0np8uXLVVhYqBdeeCF8bMGCBdqwYYNqamqieo6FCxeqoqJCTzzxhKQbGZ/Kykpdvnw5ukZ7PHr11Ve1YcOG8DHDMDR79mxVVlbqW9/6liSpr69POTk5evbZZ/XVr341uhcIAAAca4qZk69du6bW1lZVVVVFHC8vL1dLS0tUzzEwMKDu7m5Nnz494viVK1eUl5en/v5+3X333Xr66ae1ZMmSqNt2/vx5dXZ2qry8PHwsPT1dq1evVktLy4iBT19fn/r6+iLa9tvf/lYzZsyQx+OJ+toAACB5DMNQd3e3Zs+erZSUsQezTAU+XV1d6u/vV05OTsTxnJwcdXZ2RvUc+/btU09PjzZu3Bg+Nn/+fNXV1Wnx4sUKhULav3+/SkpK9Pbbb6ugoCCq5x28/kht+/DDD0f8nZqaGlVXV0f1/AAAwNouXLigOXPmjHmOqcBn0M3ZEMMwosqQ1NfXa8+ePTp69KhmzpwZPu73++X3+8M/l5SUqLCwUM8995xqa2vj1rZdu3Zp586d4Z+DwaDmzp2rCxcuyOv1mrouAABIjlAopFtvvVW33HLLuOeaCnyysrKUmpo6LLtz6dKlYZmWmzU0NGjbtm165ZVXtGbNmjHPTUlJ0dKlS3Xu3Lmo2zZr1ixJNzI/ubm5UbUtPT1d6enpw457vV4CHwAAbCaaJIypVV1paWkqKipSU1NTxPGmpiatWLFi1N+rr6/XQw89pCNHjmjdunXjXscwDLW3t0cEMOPJz8/XrFmzItp27do1nTp1asy2AQAA9zA91LVz505t3rxZ99xzj4qLi3Xo0CF1dHRo+/btkm4MH128eFGHDx+WdCPo2bJli/bv3y+/3x/OFmVmZsrn80mSqqur5ff7VVBQoFAopNraWrW3t+vAgQPh6165ckX//d//Hf75/Pnzam9v1/Tp0zV37lx5PB5VVlbqmWeeUUFBgQoKCvTMM89o6tSp2rRp08R7CAAAOIcxAQcOHDDy8vKMtLQ0o7Cw0Dh16lT4/23dutVYvXp1+OfVq1cbkoY9tm7dGj6nsrLSmDt3rpGWlmZkZ2cb5eXlRktLS8Q1X3/99XGfZ2BgwHjyySeNWbNmGenp6caqVauMd955J+rXFQwGDUlGMBg03ScAACA5zNy/TdfxcbJQKCSfz6dgMMgcHwAAbMLM/Zu9ugAAgGsQ+AAAANcg8AEAAK5B4AMAAFyDwAcAALgGgQ8AAHANAh/AIgLBXrW836VAsDfZTQEAx5rQJqUAYqvhbId2Nb6jAUNK8Ug1Dy5WxdK5yW4WADgOGR8gyQLB3nDQI0kDhvRo47tkfgAgDgh8gCQ739UTDnoG9RuGPuj6ODkNAgAHI/ABkiw/a5pSPJHHUj0e3ZY1NTkNcgjmTAEYCYEPkGS5vkzVPLhYqZ4b0U+qx6NnHlykXF9mkltmXw1nO1Sy96Q2vfSmSvaeVMPZjmQ3CYBFsEnpEGxSimQKBHv1QdfHui1rKkHPJASCvSrZezJi+DDV49GZqlL6FXAoM/dvVnUBFpHry+TGHANjzZmifwEw1AXAUZgzBWAsBD4AHCUWc6aYGA04F0NdABynYulcrZqXPaE5UxSTBJyNjA8AR8r1Zar49hmmMz0UkwScjcAHAH6PYpKA8xH4AMDvMTEacD4CHwD4PYpJAs7H5GYAGGIyE6MBWB+BD2AhgWCvznf1KD9rGjfcJKKYJOBcBD6ARbCMGgDijzk+gAWwjBoAEoPAB7AAllEDQGIQ+AAWwDJqAEgMAh/AAlhGDQCJweRmwCJYRg0A8UfgA1gIy6gBIL4Y6gIAAK5B4AMAAFyDwAcAALgGgQ8AAHANAh8AAOAaBD4AAMA1CHwAAIBrEPgAAADXIPABAACuQeADAABcg8AHlhMI9qrl/S4Fgr3JbgoAwGHYqwuW0nC2Q7sa39GAIaV4pJoHF6ti6dxkNwsA4BBkfGAZgWBvOOiRpAFDerTxXTI/AICYIfCBZZzv6gkHPYP6DUMfdH2cnAYBAByHwAeWkZ81TSmeyGOpHo9uy5qanAYBAByHwAeWkevLVM2Di5XquRH9pHo8eubBRcr1ZSa5ZQAAp2ByMyylYulcrZqXrQ+6PtZtWVMJegAAMUXgA8vJ9WUS8CRBINir8109ys+aFlX/mz0fAKyAwAeA6TIClB0AYFfM8QFczmwZAcoOALAzAh/A5cyWEaDsAAA7I/ABXM5sGQHKDgCwMwIfwOXMlhGg7AAAO5tQ4HPw4EHl5+crIyNDRUVFOn369KjnNjY2qqysTNnZ2fJ6vSouLtaJEycizqmrq5PH4xn2uHr1qqnrXrlyRY888ojmzJmjzMxMLViwQC+88MJEXiLgKhVL5+pMVanqv+LXmarScScqmz0fAKzCdODT0NCgyspK7d69W21tbVq5cqXWrl2rjo6OEc9vbm5WWVmZjh8/rtbWVpWWlmr9+vVqa2uLOM/r9SoQCEQ8MjIyTF33G9/4hn72s5/pxz/+sX71q1/pG9/4hr7+9a/r6NGjZl8m4Dq5vkwV3z4j6syN2fMBwAo8hmEY45/2qeXLl6uwsDAik7JgwQJt2LBBNTU1UT3HwoULVVFRoSeeeELSjYxPZWWlLl++PKnrLlq0SBUVFXr88cfD5xQVFelP//RP9fTTT4/brlAoJJ/Pp2AwKK/XG9VrAQAAyWXm/m0q43Pt2jW1traqvLw84nh5eblaWlqieo6BgQF1d3dr+vTpEcevXLmivLw8zZkzR/fff39ERija69577706duyYLl68KMMw9Prrr+u9997TfffdZ+ZlAgAAhzJVwLCrq0v9/f3KycmJOJ6Tk6POzs6onmPfvn3q6enRxo0bw8fmz5+vuro6LV68WKFQSPv371dJSYnefvttFRQURH3d2tpafeUrX9GcOXM0ZcoUpaSk6B//8R917733jtiWvr4+9fX1hX8OhUJRvQYAAGBPE6rc7PFErmU1DGPYsZHU19drz549Onr0qGbOnBk+7vf75ff7wz+XlJSosLBQzz33nGpra6O+bm1trX7xi1/o2LFjysvLU3Nzs772ta8pNzdXa9asGdaempoaVVdXj/+CAQCAI5gKfLKyspSamjosu3Pp0qVh2ZibNTQ0aNu2bXrllVdGDEKGSklJ0dKlS3Xu3Lmor9vb26tHH31Ur776qtatWydJ+qM/+iO1t7frO9/5zojX3LVrl3bu3Bn+ORQK6dZbbx2zbQAAwL5MzfFJS0tTUVGRmpqaIo43NTVpxYoVo/5efX29HnroIR05ciQclIzFMAy1t7crNzc36ut+8skn+uSTT5SSEvmSUlNTNTAwMOJ10tPT5fV6Ix4AAMC5TA917dy5U5s3b9Y999yj4uJiHTp0SB0dHdq+fbukG1mUixcv6vDhw5JuBD1btmzR/v375ff7w1mbzMxM+Xw+SVJ1dbX8fr8KCgoUCoVUW1ur9vZ2HThwIOrrer1erV69Wt/85jeVmZmpvLw8nTp1SocPH9Z3v/vdyfUSAAzBzvSAfZkOfCoqKvTRRx/pqaeeUiAQ0KJFi3T8+HHl5eVJkgKBQERtnRdffFHXr1/Xjh07tGPHjvDxrVu3qq6uTpJ0+fJlPfzww+rs7JTP59OSJUvU3NysZcuWRX1dSfrJT36iXbt26S/+4i/029/+Vnl5efp//+//hYMjAJgsdqYH7M10HR8no44PgLEEgr0q2XsyYpPWVI9HZ6pKyfwASRS3Oj4A4GbsTA/YH4EPAESJnekB+yPwAYAosTM9YH8TKmAIAG5VsXSuVs3L1gddH+u2rKkEPYDNEPgAgEm5vkwCHsCmGOoCAACuQeADAABcg8AHAAC4BoEPAABwDQIfwOICwV61vN+lQLA32U2BTfEeAj7Fqi7Awuy6LxSbeFqHXd9DQLyQ8QEsKhDsDd+wJGnAkB5tfNfy39obznaoZO9JbXrpTZXsPamGsx3j/xLiwq7vISCeCHwAi7LjvlDcaK3Fju8hIN4IfACLsuO+UNxorcWO7yEg3gh8AIuy475Q3GitxY7vISDePIZhGOOf5g6hUEg+n0/BYFBerzfZzQEk3Rg+stO+UA1nO/Ro47vqN4zwjZbJtMllt/cQYJaZ+zeBzxAEPkBscKMFkEhm7t8sZwcQc2ziiXigTAJigcAHAGB51CNCrDC5GQBgaZRJQCwR+AAALI0yCYglAh8AgKVRJgGxROADALA06hEhlpjcDACwvIqlc7VqXjZlEjBpBD6AA7HsF05EmQTEAoEP4DAs+wWA0THHB3AQlv0CwNgIfAAHsfKy30CwVy3vdxGEAUgqhroABxlc9js0+LHCsl+G3wBYBRkfwEGsuOyX4TcAVkLGB3AYqy37HWv4LdltczNW/sGtCHwAB7LSsl+rDr+5GUOPcDOGugDElRWH39yMoUe4HRkfAHFnteE3N2PoEW5H4AMgIaw0/OZmDD3C7RjqAgAXYegRbkfGBwBchqFHuBmBDwC4EEOPcCuGugAAgGsQ+AAAANcg8AFcgk1CAYA5PoArUKkXAG4g4wM4HJV6AeBTBD6Aw41VqRcA3IbAB3C4wUq9Q1GpF4BbEfgADkelXgD4FJObARegUi8A3EDgA7gElXoBgKEuABgRdY8AZyLjAwA3oe4R4FxkfABgCOoeAc5G4AMAQ1D3CHA2Ah8AGIK6R4CzEfgAwBDUPQKcbUKBz8GDB5Wfn6+MjAwVFRXp9OnTo57b2NiosrIyZWdny+v1qri4WCdOnIg4p66uTh6PZ9jj6tWrpq/7q1/9Sl/4whfk8/l0yy23yO/3q6OjYyIvE4BLVSydqzNVpar/il9nqkqZ2Aw4iOnAp6GhQZWVldq9e7fa2tq0cuVKrV27dtTgorm5WWVlZTp+/LhaW1tVWlqq9evXq62tLeI8r9erQCAQ8cjIyDB13ffff1/33nuv5s+frzfeeENvv/22Hn/88YjnAYBo5PoyVXz7DDI9gMN4DMMwxj/tU8uXL1dhYaFeeOGF8LEFCxZow4YNqqmpieo5Fi5cqIqKCj3xxBOSbmR8Kisrdfny5Uld90tf+pI+85nP6Ec/+pGZlxQWCoXk8/kUDAbl9Xon9BwAACCxzNy/TWV8rl27ptbWVpWXl0ccLy8vV0tLS1TPMTAwoO7ubk2fPj3i+JUrV5SXl6c5c+bo/vvvj8gIRXPdgYEB/fSnP9W8efN03333aebMmVq+fLlee+21UdvS19enUCgU8QAAAM5lKvDp6upSf3+/cnJyIo7n5OSos7MzqufYt2+fenp6tHHjxvCx+fPnq66uTseOHVN9fb0yMjJUUlKic+fORX3dS5cu6cqVK9q7d6/+5E/+RD//+c/1wAMP6MEHH9SpU6dGbEtNTY18Pl/4ceutt0bdFwAAwH4mVLnZ44lc62kYxrBjI6mvr9eePXt09OhRzZw5M3zc7/fL7/eHfy4pKVFhYaGee+451dbWRnXdgYEBSdIXv/hFfeMb35Ak3X333WppadH3v/99rV69elh7du3apZ07d4Z/DoVCBD8AADiYqcAnKytLqampw7I7ly5dGpaNuVlDQ4O2bdumV155RWvWrBnz3JSUFC1dujSc8YnmullZWZoyZYruvPPOiHMWLFigM2fOjHid9PR0paenj9kWAADgHKaGutLS0lRUVKSmpqaI401NTVqxYsWov1dfX6+HHnpIR44c0bp168a9jmEYam9vV25ubtTXTUtL09KlS/XrX/864pz33ntPeXl5Ub0+AADgbKaHunbu3KnNmzfrnnvuUXFxsQ4dOqSOjg5t375d0o3ho4sXL+rw4cOSbgQ9W7Zs0f79++X3+8NZm8zMTPl8PklSdXW1/H6/CgoKFAqFVFtbq/b2dh04cCDq60rSN7/5TVVUVGjVqlUqLS3Vz372M/3Lv/yL3njjjQl3EAAAcBBjAg4cOGDk5eUZaWlpRmFhoXHq1Knw/9u6dauxevXq8M+rV682JA17bN26NXxOZWWlMXfuXCMtLc3Izs42ysvLjZaWFlPXHfSDH/zA+OxnP2tkZGQYd911l/Haa69F/bqCwaAhyQgGg1H/DgAASC4z92/TdXycjDo+AADYT9zq+AAAANgZgQ8AAHANAh8AAOAaBD6ATQWCvWp5v0uBYG+ymwIAtjGhys0AkqvhbId2Nb6jAUNK8Ug1Dy5WxdK5yW7WpASCvTrf1aP8rGnsiA4gbgh8AJsJBHvDQY8kDRjSo43vatW8bNsGDE4M5ABYE0NdgM2c7+oJBz2D+g1DH3R9nJwGTdJogRxDeADigcAHsJjx5u7kZ01Tyk17Aqd6PLota2oCWhd7TgvkAFgbgQ8Sgom40Wk426GSvSe16aU3VbL3pBrOdgw7J9eXqZoHFyvVcyP6SfV49MyDi2w7zOW0QA6AtVG5eQgqN8cH8zeiEwj2qmTvyYjsR6rHozNVpSMGNYFgrz7o+li3ZU21bdAzqOFshx5tfFf9hhEO5HiPAIiWmfs3k5sRV06ciBsvYw35jNRXub5Mx/RhxdK5WjUv2zGBHADrIvBBXJm9mbvZ4JDPzRkftwz5OCmQA2BdzPFBXDF/I3pOm7sDAFZExgdxNXgzv3n+BjfzkTHkAwDxReCDuJvszdxtFX0Z8gGA+CHwQUJM9GbOijAAQCwxxweWRUVfAECsEfjAsqjoCwCINQIfWBYrwgAAsUbgA8tieTcAINaY3AxLY3k3BrltdR+A+CDwgeWxvBus7gMQKwx1AbA0VvcBiCUCHwCWxuo+ALFE4APA0iayui8Q7FXL+11khQAMQ+ADwNLMru5rONuhkr0ntemlN1Wy96QaznYksrkALM5jGIYx/mnuEAqF5PP5FAwG5fV6k90cAEMEgr3jru4LBHtVsvdkxNBYqsejM1WlTJAHHMzM/ZtVXQBsIZrVfWPNByLwASAx1AXAQaj2DWA8BD4AHINq3wDGw1AXAEeh2jeAsRD4AHAcqn0DGA1DXbA9arYgGrxPAEhkfGBz7OGEaPA+ATCIjA9sa6Q9nHY1vqN/efsi3+oRxl5fAIYi8IFtjVSzZcCQvl7fTsVehLHXF4ChCHxgWyPVbBnEt3oMorYPgKEIfGBbN9dsuRnf6iFR2wdAJCY3w9YGa7a89eHv9MiRNg0d0eBbPQZR2wfAIAIf2F6uL1Pr/ihTV/qu69HGd9VvGErxSH91723JbhoshNo+8REI9up8V4/ys6bRv7AFdmcfgt3Z7S8Q7NUP//28Xmo+L0MsXQbiiTIBsAoz92/m+MBx/vH0+fCQF5OcnYlihMlHmQDYFUNdsIVo0+ljLV0mDe8MZBmswQ5/awzDYSQEPrA8Mze6waXLQz+QmeTsHKNlGVbNy+bGlmBW/1sjQMZoGOqCpZlNp7N02dkoRmgdVv5bYxgOYyHjA0ubSDqdpcvOZfUsg9tY9W/NDsNwSB4yPrC0iVbdzfVlqvj2Gcr1ZTIR1kGsnGVwq6F/a1ZBtW6MhYwPLG3wRjdYn8fsjY5xfuexapYB1jHZzw04G3V8hqCOj3UFgr2mb3SBYK9K9p4cNixypqqUD0DABSbyuQF7MnP/JuMDW5hI1V07j/OzDNc8+gw3o1o3RkLgA8ey60RYhufMo88ARIvJzXAsO06EZRmuefQZ4onFEc5DxgeOZreJsMkanrPzMJGdhzRhbWQSnWlCGZ+DBw8qPz9fGRkZKioq0unTp0c9t7GxUWVlZcrOzpbX61VxcbFOnDgRcU5dXZ08Hs+wx9WrVyd83a9+9avyeDz6h3/4h4m8RDiIFZfbjiYZy3AbznaoZO9JbXrpTZXsPamGsx1xu1Y8sHQZ8UAm0blMBz4NDQ2qrKzU7t271dbWppUrV2rt2rXq6Bj5w7K5uVllZWU6fvy4WltbVVpaqvXr16utrS3iPK/Xq0AgEPHIyMiY0HVfe+01vfnmm5o9e7bZlwckNbWd6OE5J3y4x7PPGOZwL6qEO5fp5ezLly9XYWGhXnjhhfCxBQsWaMOGDaqpqYnqORYuXKiKigo98cQTkm5kfCorK3X58uVJX/fixYtavny5Tpw4oXXr1qmyslKVlZVRtYvl7LBKajtRy3Bb3u/SppfeHHa8/it+Fd8+I27XjYdY95lV3gtIDsph2IuZ+7epjM+1a9fU2tqq8vLyiOPl5eVqaWmJ6jkGBgbU3d2t6dOnRxy/cuWK8vLyNGfOHN1///0RGaForzswMKDNmzfrm9/8phYuXGjmpQGWyn4kanjOScNEsewzK70XkBx2XByB6Jia3NzV1aX+/n7l5OREHM/JyVFnZ2dUz7Fv3z719PRo48aN4WPz589XXV2dFi9erFAopP3796ukpERvv/22CgoKor7us88+qylTpuhv/uZvompLX1+f+vr6wj+HQqGofg/O5MZJslS4HZkb3wsYzm6LIxCdCa3q8ngivyIahjHs2Ejq6+u1Z88eHT16VDNnzgwf9/v98vv94Z9LSkpUWFio5557TrW1tVFdt7W1Vfv379dbb70VVVskqaamRtXV1VGdC+eza92fyeLDfTi3vhcwHEUQncfUUFdWVpZSU1OHZXcuXbo0LBtzs4aGBm3btk0vv/yy1qxZM3ajUlK0dOlSnTt3Lurrnj59WpcuXdLcuXM1ZcoUTZkyRR9++KH+7u/+TrfddtuI19m1a5eCwWD4ceHChTHbBWdzc2rbTivfEsHN7wXA6UxlfNLS0lRUVKSmpiY98MAD4eNNTU364he/OOrv1dfX66/+6q9UX1+vdevWjXsdwzDU3t6uxYsXR33dzZs3Dwuo7rvvPm3evFl/+Zd/OeJ10tPTlZ6ePm574B5kPzCI9wJGYueaV7jB9FDXzp07tXnzZt1zzz0qLi7WoUOH1NHRoe3bt0u6kUW5ePGiDh8+LOlG0LNlyxbt379ffr8/nLXJzMyUz+eTJFVXV8vv96ugoEChUEi1tbVqb2/XgQMHor7ujBkzNGNG5CqUz3zmM5o1a5buuOOOCXQN3IrUNgbxXsBQrPRzBtOBT0VFhT766CM99dRTCgQCWrRokY4fP668vDxJUiAQiKit8+KLL+r69evasWOHduzYET6+detW1dXVSZIuX76shx9+WJ2dnfL5fFqyZImam5u1bNmyqK8LAEC8jLbSb9W8bIJjmzFdx8fJqOMDRI+UP9zESTWvnMjM/Zu9ugCYRsofbsNKP+dgd3YAplDcD27ESj/nIOMDW2KYJXko7ge3YqWfMxD4wHYYZkkuUv5wM1b62R9DXbAVhlmSz0zKn93NAVgNGR/YCsMs1hBNyp/MHAArIuMDW3HSbuJ2N9Y2F2TmAFgVgQ9shZUV9jBWZg4AkomhLtgOKyusjwnQAKyKjA9sid3ErS2ZmTkmVAMYCxkfAHGRjMwcE6oBjIeMD4C4SWRmjgnVAKJB4APAEZhQDSAaBD4AHIFSBwCiQeADwBEodQAgGkxuBmBbN29WS6kDAOMh8AFgS6Ot4BptE8mbgyQA7kTgg5jjBoN4G20F16p52ewbBmBMBD6IKW4wSAQzm9WaDZIAOBuTmxPEDdVkqaOCRDGzgotl7jDDDZ/VbkfGJwHckgUx8y0cmIzBFVyPNr6rfsMYcwUX+4YhWm75rHY7Ap84c1OanRsMEinaFVxmgiS4l5s+q92OwCfO3JQF4QaDRBttBdfNWOaO8bjps9rtCHzizG1ZEG4wsKpogyS4k9s+q92Myc1x5sZqsoncmBIAYsGNn9Vu5TEMwxj/NHcIhULy+XwKBoPyer0xfe5AsJcsCABYXKw+q6lnllhm7t8MdSUIaXYA0eCGmVyx+KxmdZi1EfgAgEVww7Q/VodZH3N8AMACKADqDBTMtD4CHwCwAG6YzmCmqjiSg8AHACyAG6YzsDrM+pjjAwAWQAFQ56CembUR+ACARXDDdA5W8loXgQ8ShmW6wPi4YQLxReCDhGCZLgDACpjcjLhjmS4AwCoIfBB3LNMFAFgFgQ/ijmW6QOIEgr1qeb/LdhlVu7Yb9sMcH8Qdy3SBxLDrXLp4tZsFFRgJu7MPEc/d2WGtHer5QITTBIK9Ktl7MmJYOdXj0ZmqUku/x+PVbrsGgZgYdmeHJVllma5dPxAJ1jCWsebSWfn9Eo92s1EoxkLgA1ex6weiXYM1JM7gXLqbMydWn0sXj3bbNQhEYjC5Ga5ixxVmlANANOy6R1Q82s2CCoyFjA9cxY7fivn2imjZdcuLWLebBRUYC4EPXMWOH4jxDNaYN+Q8VplLZ1as223XIBDxR+AD17HbB2K8gjXmDcHp7BoEIr5Yzj4Ey9lhZbEsBzDZJcRkimB1vEfdheXsgAPF8tvrZOYNkSmC1fEexVhY1QW40ERXvbDCDFbHexTjIfABbCYWexpNdAmxHcsBwF14j2I8DHXBdew89h/LFP5EJnnnZ02TR9LQ+4rHI0uXA5Ds/W/uBInsfzuWrEBiEfjAVew89j9aCn/+rFvUc61/QjeVmMwbsvjyCDv/mztBovvfjiUrkFgEPnANu25XMWi0FP6GAy0ylJibyvmunmFxjiFZtpii3f/N7S5Z/W+3khVILOb4wDXsPvY/0oRk6dOESyImcdptKwC7/5vbXTL7P9eXqeLbZxD0YJgJBT4HDx5Ufn6+MjIyVFRUpNOnT496bmNjo8rKypSdnS2v16vi4mKdOHEi4py6ujp5PJ5hj6tXr0Z93U8++UTf+ta3tHjxYk2bNk2zZ8/Wli1b9Jvf/GYiLxEOZLeb9s1unpA8UhAU75tKIvaDisXk7UF2/ze3O/ofVmQ68GloaFBlZaV2796ttrY2rVy5UmvXrlVHR8eI5zc3N6usrEzHjx9Xa2urSktLtX79erW1tUWc5/V6FQgEIh4ZGRlRX/fjjz/WW2+9pccff1xvvfWWGhsb9d577+kLX/iC2ZcIh7LrJo5DVSydqzNVpar/il+vfm1FUm4qQ9twpqo0pkNrDWc7VLL3pDa99KZK9p5Uw9mRP1ei5YR/czuj/2FFpis3L1++XIWFhXrhhRfCxxYsWKANGzaopqYmqudYuHChKioq9MQTT0i6kfGprKzU5cuXY3rds2fPatmyZfrwww81d+74H85UbnaHWFZATraGsx3DJnHadeLuZKtJj/fcTvk3tyP6H/EWt8rN165dU2trq6qqqiKOl5eXq6WlJarnGBgYUHd3t6ZPnx5x/MqVK8rLy1N/f7/uvvtuPf3001qyZMmkrhsMBuXxePQHf/AHI/7/vr4+9fX1hX8OhUJRvQbYm5P273HSJM547kLvpH9zO6L/YSWmhrq6urrU39+vnJyciOM5OTnq7OyM6jn27dunnp4ebdy4MXxs/vz5qqur07Fjx1RfX6+MjAyVlJTo3LlzE77u1atXVVVVpU2bNo0a/dXU1Mjn84Uft956a1SvAbCSZE/ijNWcHOaDAEiECU1u9ngiP50Mwxh2bCT19fXas2ePGhoaNHPmzPBxv9+vL3/5y7rrrru0cuVKvfzyy5o3b56ee+65CV33k08+0Ze+9CUNDAzo4MGDo7Zn165dCgaD4ceFCxfGfQ0APjXenBwzQRHzQYDYiOUCAScyNdSVlZWl1NTUYVmWS5cuDcvG3KyhoUHbtm3TK6+8ojVr1ox5bkpKipYuXRrO+Ji57ieffKKNGzfq/PnzOnny5Jhjfenp6UpPTx+zLQBGNl6NlokUrnPS0B2QDBTsHJ+pjE9aWpqKiorU1NQUcbypqUkrVqwY9ffq6+v10EMP6ciRI1q3bt241zEMQ+3t7crNzTV13cGg59y5c/q3f/s3zZgxw8zLA2DCWHNyJrNRZLKH7gC7YoPW6Jiu3Lxz505t3rxZ99xzj4qLi3Xo0CF1dHRo+/btkm4MH128eFGHDx+WdCPo2bJli/bv3y+/3x/O2mRmZsrn80mSqqur5ff7VVBQoFAopNraWrW3t+vAgQNRX/f69ev6sz/7M7311lv613/9V/X394evNX36dKWlpU2imwDcbKw9keI5URnAyPi7i47pwKeiokIfffSRnnrqKQUCAS1atEjHjx9XXl6eJCkQCETU9HnxxRd1/fp17dixQzt27Agf37p1q+rq6iRJly9f1sMPP6zOzk75fD4tWbJEzc3NWrZsWdTX/Z//+R8dO3ZMknT33XdHtPn111/X5z73ObMvFcAYxtsTiY0iY4uNVjEeNmiNjuk6Pk5GHR/AvNFqtDipxlCyMW9jOALBkbn1787M/ZvAZwgCHyC2KFw3efEs7BhriQpGCATH5sa/u7gVMAQAMyhcN3l2mbeRqGAkWTu+2wl/d2Njd3YAsDA7FHZM5GqiZO74Dmcg8AEAC7NDYcdEBiN2CARhbQx1AWKiJKzN6oUd47GaaLS/yfFWEwLjIfCB6zFREnZg5XkbsQ5GxvubtHogCGtjVdcQrOpyHzutmAGsLharifibxESwqguIkl1WzAB2EIusFH+TiDcmN8PVmCgJxN5kdgfnbxLxRuADV7PDihnAThrOdqhk70lteulNlew9qYazHeP/0hD8TSLemOMzBHN83MuNlU6BWIvl/Bz+JmEGc3wAk6y8Ygawi1jOz+FvEvHCUBcAICampaWOeHxqGrcaWAfvRiDJJjMRFLCSnmv9Ix7/+NpAglsCjI6hLiCJKJ4IJ4lHBWcg1sj4AEmSyI0dgURgRRbsgIwPkCQUaoMTsZ0ErI7AB0gShgXgVKzIgpUx1AUkCcMCwHBM9ke8kfEBkohhAeBTTPZHIpDxAZIs15ep4ttnEPTA1Zjsj0Qh8AEgiSEGJNdYk/2BWGKoC0iCQLBX57t6lJ81bdKZnlg8F0MMSDYm+yNRCHyABItlkBGL5xptiGHVvGyG35Awg5P9H218V/2GEffJ/rH88gF7IfCBrdntwyuWQUasnsup9YTs9t5A4ib7k+F0NwIf2JYdP7xiGWTE4rkCwV59dKXPcUMMdnxv4IZ41wAiwwkmN8OW7LoCZHAew1ATDTIm+1wNZztUsvekvl7fLsOQfl9OyPb1hOz63kBiMIkaBD6wpWg+vKy4SimWRQsn81w3BweGJI8hHdi0RGeqSm2dHeHGhrHE8ssH7ImhLtjSeCtArDzUEct5DBN9rpGCgwFJ06el2zbTM4jVQRhLoidRw3oIfGBLY3142WEMP5bzGCbyXCMFBykeaWqa/ZPA3NgwHiqmu5vHMAxj/NPcIRQKyefzKRgMyuv1Jrs5iEIg2Dvsw6vl/S5teunNYefWf8Wv4ttnJLqJltVwtiMcHAyyWnZsMkZ6b5j5XVaEAfZh5v5Nxge2NlK2g6GO6FQsnav5s27RhgMtGuwqK2bHJmqiWTUrD5MCmDz757WBm7DrefR6rvXr5pSvmycCsyIMcD4yPnAkxvCjQ3YsklOLOQL4FBkfOBa7no+P7FgkljoDzkfGB3C5aLJjbpnsy4owwPkIfACXGSmIGWsisNsm+zJMCjgbgQ/gImaDGDvURIqHeO8XBSB5mOMDuMREViyx/QMApyHwAVxiIkEMk30BOA2BD2AR8d5UdSJBDKu+ADgNc3wAC0jEBOKJrlhisi8AJ2GvriHYqwvJEAj2qmTvyWFFBM9UlcYlyJjMHlYAYEXs1QXYSKKrBbNiCYCbMccHSDImEANA4hD4AEnGBGIASByGugALsPoEYrdsWQHA+Qh8AIuw6twbt21ZAcDZGOoCMKqJVHsGACsj8AEwKrasAOA0BD4ARsWKMwBOQ+ADYFSsOAPgNExuBjAmq684AwAzJpTxOXjwoPLz85WRkaGioiKdPn161HMbGxtVVlam7Oxseb1eFRcX68SJExHn1NXVyePxDHtcvXrV1HUNw9CePXs0e/ZsZWZm6nOf+5x++ctfTuQlAhgi15ep4ttnEPQAsD3TgU9DQ4MqKyu1e/dutbW1aeXKlVq7dq06OjpGPL+5uVllZWU6fvy4WltbVVpaqvXr16utrS3iPK/Xq0AgEPHIyMgwdd1vf/vb+u53v6vnn39eZ8+e1axZs1RWVqbu7m6zLxMAADiRYdKyZcuM7du3RxybP3++UVVVFfVz3HnnnUZ1dXX45x/+8IeGz+eb1HUHBgaMWbNmGXv37g3//6tXrxo+n8/4/ve/H1W7gsGgIckIBoNRvhIAAJBsZu7fpjI+165dU2trq8rLyyOOl5eXq6WlJarnGBgYUHd3t6ZPnx5x/MqVK8rLy9OcOXN0//33R2SEornu+fPn1dnZGXFOenq6Vq9eHXXbAACAs5kKfLq6utTf36+cnJyI4zk5Oers7IzqOfbt26eenh5t3LgxfGz+/Pmqq6vTsWPHVF9fr4yMDJWUlOjcuXNRX3fwv2ba1tfXp1AoFPEAAADONaFVXR5PZGEPwzCGHRtJfX299uzZo6NHj2rmzJnh436/X36/P/xzSUmJCgsL9dxzz6m2ttbUdc20raamRtXV1eO2GwAAOIOpjE9WVpZSU1OHZVAuXbo0LNNys4aGBm3btk0vv/yy1qxZM3ajUlK0dOnScMYnmuvOmjVLkky1bdeuXQoGg+HHhQsXxmwXAACwN1OBT1pamoqKitTU1BRxvKmpSStWrBj19+rr6/XQQw/pyJEjWrdu3bjXMQxD7e3tys3Njfq6+fn5mjVrVsQ5165d06lTp0ZtW3p6urxeb8QDAAA4l+mhrp07d2rz5s265557VFxcrEOHDqmjo0Pbt2+XdCOLcvHiRR0+fFjSjaBny5Yt2r9/v/x+fzgjk5mZKZ/PJ0mqrq6W3+9XQUGBQqGQamtr1d7ergMHDkR9XY/Ho8rKSj3zzDMqKChQQUGBnnnmGU2dOlWbNm2aXC8BAABHMB34VFRU6KOPPtJTTz2lQCCgRYsW6fjx48rLy5MkBQKBiNo6L774oq5fv64dO3Zox44d4eNbt25VXV2dJOny5ct6+OGH1dnZKZ/PpyVLlqi5uVnLli2L+rqS9Pd///fq7e3V1772Nf3ud7/T8uXL9fOf/1y33HKL6Y4BAADO4zEMwxj/NHcIhULy+XwKBoMMewEAYBNm7t9sUgoAAFyDwAcAACREINirlve7FAj2Jq0N7M4OAADiruFsh3Y1vqMBQ0rxSDUPLlbF0rkJbwcZHwAAEFeBYG846JGkAUN6tPHdpGR+CHwAAEBcne/qCQc9g/oNQx90fZzwthD4AACAuMrPmqaUm3aPSvV4dFvW1IS3hcAHgGtYYWIl4Ea5vkzVPLhYqb/fOzPV49EzDy5Sri8z4W1hcjMAV7DKxErArSqWztWqedn6oOtj3ZY1NSlBj0TGB4ALWGliJeBmub5MFd8+I2lBj0TgA8AFrDSxEkByEfjAVZjj4U5WmlgJILkIfOAaDWc7VLL3pDa99KZK9p5Uw9mO8X8JjmCliZUAkotNSodgk1LnCgR7VbL3ZMRwR6rHozNVpdz8XCQQ7E36xEoAsWfm/s2qLrjCWHM8uAG6R64vk39vwOUY6oIrMMcDACAR+MAlmOMBAJAY6oKLJLp4ViDYq/NdPcrPmkaABQAWQeADV0nUHA+qBAOANTHUBUexQp0eqgQD9mCFzwskHhkfOIZVsiysIAOszyqfF0g8Mj5whPGyLIn8ZscKMsDayMq6G4EPHGGsLEuiKzazggywNvZuczeGuuAIg1mWmyszT01LGfGb3ap52XENRBK9ggxA9Eb7vCAr6w5kfOAIo2VZeq71J+2bXa4vU8W3zyDoASyGrKy7kfGBY4yUZQkEe/lmB2AYsrLuRcYHjnJzloVvdgBGQ1bWncj4wPH4ZgcAGETgA1dgV24AgMRQFwAAcBECHwAA4BoEPgAAwDUIfAAAgGsQ+AAAANcg8AEAAK5B4AMAAFyDwAcAALgGgQ8AAHANAh8AAOAaBD4AAMA12KtrCMMwJEmhUCjJLQEAANEavG8P3sfHQuAzRHd3tyTp1ltvTXJLAACAWd3d3fL5fGOe4zGiCY9cYmBgQL/5zW90yy23yOPxJKUNoVBIt956qy5cuCCv15uUNrgVfZ889H3y0PfJQ9/HjmEY6u7u1uzZs5WSMvYsHjI+Q6SkpGjOnDnJboYkyev18oeQJPR98tD3yUPfJw99HxvjZXoGMbkZAAC4BoEPAABwDQIfi0lPT9eTTz6p9PT0ZDfFdej75KHvk4e+Tx76PjmY3AwAAFyDjA8AAHANAh8AAOAaBD4AAMA1CHwAAIBrEPgkwcGDB5Wfn6+MjAwVFRXp9OnTo54bCAS0adMm3XHHHUpJSVFlZWXiGupAZvq+sbFRZWVlys7OltfrVXFxsU6cOJHA1jqLmb4/c+aMSkpKNGPGDGVmZmr+/Pn63ve+l8DWOouZvh/q3//93zVlyhTdfffd8W2gg5np+zfeeEMej2fY47/+678S2GLnI/BJsIaGBlVWVmr37t1qa2vTypUrtXbtWnV0dIx4fl9fn7Kzs7V7927dddddCW6ts5jt++bmZpWVlen48eNqbW1VaWmp1q9fr7a2tgS33P7M9v20adP0yCOPqLm5Wb/61a/02GOP6bHHHtOhQ4cS3HL7M9v3g4LBoLZs2aLPf/7zCWqp80y073/9618rEAiEHwUFBQlqsUsYSKhly5YZ27dvjzg2f/58o6qqatzfXb16tfG3f/u3cWqZ802m7wfdeeedRnV1dayb5nix6PsHHnjA+PKXvxzrpjneRPu+oqLCeOyxx4wnn3zSuOuuu+LYQucy2/evv/66Icn43e9+l4DWuRcZnwS6du2aWltbVV5eHnG8vLxcLS0tSWqVO8Si7wcGBtTd3a3p06fHo4mOFYu+b2trU0tLi1avXh2PJjrWRPv+hz/8od5//309+eST8W6iY03mfb9kyRLl5ubq85//vF5//fV4NtOV2KQ0gbq6utTf36+cnJyI4zk5Oers7ExSq9whFn2/b98+9fT0aOPGjfFoomNNpu/nzJmj//u//9P169e1Z88e/fVf/3U8m+o4E+n7c+fOqaqqSqdPn9aUKdwiJmoifZ+bm6tDhw6pqKhIfX19+tGPfqTPf/7zeuONN7Rq1apENNsVeFcngcfjifjZMIxhxxAfE+37+vp67dmzR0ePHtXMmTPj1TxHm0jfnz59WleuXNEvfvELVVVV6bOf/az+/M//PJ7NdKRo+76/v1+bNm1SdXW15s2bl6jmOZqZ9/0dd9yhO+64I/xzcXGxLly4oO985zsEPjFE4JNAWVlZSk1NHRbtX7p0adi3AsTWZPq+oaFB27Zt0yuvvKI1a9bEs5mONJm+z8/PlyQtXrxY//u//6s9e/YQ+Jhgtu+7u7v1H//xH2pra9Mjjzwi6cYQr2EYmjJlin7+85/rj//4jxPSdruL1ee93+/Xj3/841g3z9WY45NAaWlpKioqUlNTU8TxpqYmrVixIkmtcoeJ9n19fb0eeughHTlyROvWrYt3Mx0pVu97wzDU19cX6+Y5mtm+93q9euedd9Te3h5+bN++XXfccYfa29u1fPnyRDXd9mL1vm9ra1Nubm6sm+duSZxY7Uo/+clPjM985jPGD37wA+M///M/jcrKSmPatGnGBx98YBiGYVRVVRmbN2+O+J22tjajra3NKCoqMjZt2mS0tbUZv/zlL5PRfFsz2/dHjhwxpkyZYhw4cMAIBALhx+XLl5P1EmzLbN8///zzxrFjx4z33nvPeO+994x/+qd/Mrxer7F79+5kvQTbmshnzlCs6po4s33/ve99z3j11VeN9957z3j33XeNqqoqQ5Lxz//8z8l6CY5E4JMEBw4cMPLy8oy0tDSjsLDQOHXqVPj/bd261Vi9enXE+ZKGPfLy8hLbaIcw0/erV68ese+3bt2a+IY7gJm+r62tNRYuXGhMnTrV8Hq9xpIlS4yDBw8a/f39SWi5/Zn9zBmKwGdyzPT9s88+a9x+++1GRkaG8Yd/+IfGvffea/z0pz9NQqudzWMYhpGkZBMAAEBCMccHAAC4BoEPAABwDQIfAADgGgQ+AADANQh8AACAaxD4AAAA1yDwAQAArkHgAwAAXIPABwAAuAaBDwAAcA0CHwAA4BoEPgAAwDX+P7hotEAqeYwFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_labels, np.concatenate(pred),  '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
