{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.al import top_model as topmodel\n",
    "from DomainPrediction.al.embeddings import one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../esm')\n",
    "from DomainPrediction.esm.esm3 import ESM3LM\n",
    "from DomainPrediction.esm.esmc import ESMCLM\n",
    "from DomainPrediction.al.confit import ESMCConFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nethome/kgeorge/workspace/DomainPrediction/Data/al_test_experiments/Tdomain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(data_path, 'dataset_2_tdomain.csv')\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>seq</th>\n",
       "      <th>fitness_raw</th>\n",
       "      <th>split_id</th>\n",
       "      <th>n_mut</th>\n",
       "      <th>fitness_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT</td>\n",
       "      <td>APGEDAFARQAYQAPQGEIEIALATIWRELLNVEQVGRHDSFFALG...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESM1</td>\n",
       "      <td>APEDSSFPRPPYAAPEGEIEQTLAGIWMELLGVERVGRHDSFFALG...</td>\n",
       "      <td>0.982485</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.017670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESM2</td>\n",
       "      <td>APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELG...</td>\n",
       "      <td>0.958725</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>-0.042151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESM3</td>\n",
       "      <td>APSEDSYPRPAYVAPEGPTEQLLAGIWQELLNVSKVGRDDSFFDLG...</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>-3.343159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESM4</td>\n",
       "      <td>APEEASYPREPYVAPQGETEQLLASIWQELLGVERVGAGDNFFELG...</td>\n",
       "      <td>0.457921</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.781059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name                                                seq  fitness_raw  \\\n",
       "0    WT  APGEDAFARQAYQAPQGEIEIALATIWRELLNVEQVGRHDSFFALG...     1.000000   \n",
       "1  ESM1  APEDSSFPRPPYAAPEGEIEQTLAGIWMELLGVERVGRHDSFFALG...     0.982485   \n",
       "2  ESM2  APSEDAYPRATYEAPEGETEQLLAGIWMDLLQVDRVGRHDSFFELG...     0.958725   \n",
       "3  ESM3  APSEDSYPRPAYVAPEGPTEQLLAGIWQELLNVSKVGRDDSFFDLG...     0.035325   \n",
       "4  ESM4  APEEASYPREPYVAPQGETEQLLASIWQELLGVERVGAGDNFFELG...     0.457921   \n",
       "\n",
       "   split_id  n_mut  fitness_log  \n",
       "0         2      0     0.000000  \n",
       "1         2     44    -0.017670  \n",
       "2         0     45    -0.042151  \n",
       "3         2     46    -3.343159  \n",
       "4         2     43    -0.781059  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_mask(df, omit_zero=False):\n",
    "    if omit_zero:\n",
    "        train_mask = (df['split_id'] == 2) & (df['fitness_raw'] != 0)\n",
    "    else:\n",
    "        train_mask = (df['split_id'] == 2)\n",
    "\n",
    "    val_mask = df['split_id'] == 1\n",
    "    test_mask = df['split_id'].isin([0, 1])\n",
    "    # test_mask = df['split_id'] == 0\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearmanr_bootstrap(a, b, n=1000):\n",
    "    assert type(a) == type(b) == np.ndarray\n",
    "    assert len(a) == len(b)\n",
    "    corr = []\n",
    "    p_value = []\n",
    "    np.random.seed(0)\n",
    "    for _ in range(n):\n",
    "        indices = np.random.choice(len(a), size=len(a), replace=True)\n",
    "        res = stats.spearmanr(a[indices], b[indices])\n",
    "        \n",
    "        if not np.isnan(res.statistic):\n",
    "            corr.append(res.statistic)\n",
    "            p_value.append(res.pvalue)\n",
    "\n",
    "    ci_lower, ci_upper = np.percentile(corr, [5, 95]) \n",
    "    # stats.t.interval(confidence=0.95, df=len(corr)-1, loc=np.mean(corr), scale=np.std(corr))\n",
    "    mean_corr = np.mean(corr)\n",
    "\n",
    "    return round(mean_corr, 2), round(ci_lower, 2), round(ci_upper, 2), corr, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask, test_mask = get_split_mask(df, omit_zero=False)\n",
    "df_train = df[train_mask]\n",
    "df_val = df[val_mask]\n",
    "df_test = df[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmc = ESMCLM(name='esmc_600m', device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wt marginals\n",
    "y_pred = []\n",
    "n_muts_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = esmc.get_wildtype_marginal(mt_sequence, wt_sequence)\n",
    "    # score, n_muts = esmc.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "    # score, n_muts = esmc.get_masked_marginal_var(mt_sequence, wt_sequence, mode='wt')\n",
    "    # score, n_muts = esmc.get_masked_marginal_var(mt_sequence, wt_sequence, mode='mt')\n",
    "    # score = esmc.pseudolikelihood(mt_sequence)\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "    n_muts_list.append(n_muts)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df['fitness_log'].to_numpy().astype(np.float32)\n",
    "\n",
    "n_muts_list = np.array(n_muts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > -1\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_val_pred, y_val = y_pred[val_mask & omit_mask], y[val_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "color_mask = df['n_mut'] > 15\n",
    "# color_mask = n_muts_list > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7,3), layout='constrained')\n",
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "ax[0].hist(df['n_mut'][train_mask & omit_mask])\n",
    "ax[1].hist(df['n_mut'][test_mask & omit_mask])\n",
    "for i in range(2):\n",
    "    ax[i].set_title('hamming dist', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7,3), layout='constrained')\n",
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "ax[0].hist(n_muts_list[train_mask & omit_mask])\n",
    "ax[1].hist(n_muts_list[test_mask & omit_mask])\n",
    "for i in range(2):\n",
    "    ax[i].set_title('hamming dist', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDatasetContrast(Dataset):\n",
    "    def __init__(self, df, wt):\n",
    "        self.seq, self.y = df['seq'].to_numpy(), df['fitness_raw'].to_numpy()\n",
    "        self.wt = np.array([wt]*self.seq.shape[0], dtype='object')\n",
    "        self.n_mut = df['n_mut'].to_numpy()\n",
    "\n",
    "        self.positions = []\n",
    "        for _, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            pos = []\n",
    "            for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt)):\n",
    "                if aa_wt != aa_mt:\n",
    "                    ## mutation pos\n",
    "                    pos.append(i)\n",
    "\n",
    "            assert len(pos) == row['n_mut']\n",
    "\n",
    "            self.positions.append(np.array(pos))\n",
    "\n",
    "        assert len(self.positions) == self.seq.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.y[idx], self.wt[idx], self.positions[idx], self.n_mut[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        seq = np.array([x[0] for x in data], dtype='object')\n",
    "        y = torch.tensor([x[1] for x in data])\n",
    "        wt = np.array([x[2] for x in data], dtype='object')\n",
    "        pos = [x[3] for x in data]\n",
    "        n_mut = np.array([x[4] for x in data])\n",
    "        return seq, y, wt, pos, n_mut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence is not correctly recorded -- I forgot why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMCConFit(pl.LightningModule):\n",
    "    def __init__(self, name, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if name == 'esmc_300m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 960\n",
    "        elif name == 'esmc_600m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 1152\n",
    "        else:\n",
    "            raise Exception('Check ESMC name')\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"out_proj\"],\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.basemodel, peft_config)\n",
    "\n",
    "        for name, pm in self.model.named_parameters():\n",
    "            if 'q_ln' in name or 'k_ln' in name:\n",
    "                pm.requires_grad = True\n",
    "\n",
    "        if self.config['use_seq_head']:\n",
    "            for name, pm in self.model.named_parameters():\n",
    "                if 'sequence_head' in name:\n",
    "                    pm.requires_grad = True\n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.accumulate_batch_bt_loss_train = []\n",
    "        self.accumulate_batch_bt_loss_val = []\n",
    "        self.accumulate_batch_kl_div_train = []\n",
    "        self.accumulate_batch_kl_div_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch_tokens_masked, batch_tokens, batch_tokens_wt, pos):\n",
    "        \n",
    "        output = self.model(batch_tokens_masked)\n",
    "        logits = output.sequence_logits\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        scores = torch.zeros(log_probs.shape[0])\n",
    "        if self.config['device'] == 'gpu':\n",
    "            scores = scores.cuda()\n",
    "\n",
    "        for i in range(log_probs.shape[0]):\n",
    "            scores[i] = torch.sum(log_probs[i, pos[i]+1, batch_tokens[i][pos[i]+1]] - log_probs[i, pos[i]+1, batch_tokens_wt[i][pos[i]+1]])\n",
    "        \n",
    "        return scores, logits\n",
    "    \n",
    "    def BT_loss(self, scores, y):\n",
    "        loss = torch.tensor(0.)\n",
    "        if self.config['device'] == 'gpu':\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(i, len(scores)):\n",
    "                if y[i] > y[j]:\n",
    "                    if torch.abs(scores[j]-scores[i]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[j]-scores[i]))\n",
    "                else:\n",
    "                    if torch.abs(scores[i]-scores[j]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[i]-scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        _positions = []\n",
    "\n",
    "        if self.config['score'] == 'masked':\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                _positions.append(pos[i])\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "        elif self.config['score'] == 'masked-random':\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    if len(pos[i]) > 40:\n",
    "                        sampled_positions = np.random.choice(pos[i], 40, replace=False)\n",
    "                        _positions.append(sampled_positions)\n",
    "                        batch_tokens_masked[i, sampled_positions+1] = self.model.tokenizer.mask_token_id\n",
    "                    else:\n",
    "                        _positions.append(pos[i])\n",
    "                        batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "                else:\n",
    "                    assert len(pos[i]) == 0\n",
    "                    _positions.append(pos[i])\n",
    "\n",
    "        elif self.config['score'] == 'wildtype':\n",
    "            _positions = pos\n",
    "            batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Anata wa bakadesuka?')\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        assert len(_positions) == len(batch_tokens_masked)\n",
    "\n",
    "        y_hat, logits = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, _positions)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_train.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_train.append(l_reg.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        # batch_tokens_masked = batch_tokens.clone()\n",
    "        # for i in range(batch_tokens.shape[0]):\n",
    "        #     if len(pos[i]) > 0:\n",
    "        #         batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "        \n",
    "        # if self.config['device'] == 'gpu':\n",
    "        #     batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        # y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        # print(y_hat)\n",
    "\n",
    "        '''\n",
    "            During validation random stuff is not needed\n",
    "        '''\n",
    "\n",
    "        if 'masked' in self.config['score']:\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "        elif self.config['score'] == 'wildtype':\n",
    "            batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Anata wa bakadesuka?')\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, pos)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_val.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_val.append(l_reg.item())\n",
    "\n",
    "    def trainmodel(self, df, wt, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetContrast(df, wt)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetContrast(val, wt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=True)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"bf16-mixed\",\n",
    "                                accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    def sanity_check(self, df, wt):\n",
    "        '''\n",
    "            Needs change to accomodate the new losses\n",
    "        '''\n",
    "\n",
    "        # print('Not usable at the moment')\n",
    "\n",
    "        # return None\n",
    "\n",
    "        dataset = ProteinFunDatasetContrast(df, wt)\n",
    "        loader = DataLoader(dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        y_pred_1 = []\n",
    "        for batch in loader:\n",
    "            mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "            batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "            batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "            if 'masked' in self.config['score']:\n",
    "                batch_tokens_masked = batch_tokens.clone()\n",
    "                for i in range(batch_tokens.shape[0]):\n",
    "                    if len(pos[i]) > 0:\n",
    "                        batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "\n",
    "            elif self.config['score'] == 'wildtype':\n",
    "                batch_tokens_masked = batch_tokens_wt.clone()\n",
    "\n",
    "            else:\n",
    "                raise Exception('Anata wa bakadesuka?')\n",
    "            \n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat, _ = self(batch_tokens_masked, batch_tokens, batch_tokens_wt, pos)\n",
    "\n",
    "            y_pred_1.append(y_hat.cpu().numpy())\n",
    "\n",
    "        y_pred_1 = np.concatenate(y_pred_1)\n",
    "\n",
    "        y_pred_2 = []\n",
    "        for i, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "\n",
    "            if 'masked' in self.config['score']:\n",
    "                score, n_muts = self.get_masked_marginal(mt_sequence, wt)\n",
    "            elif self.config['score'] == 'wildtype':\n",
    "                score, n_muts = self.get_masked_marginal(mt_sequence, wt)\n",
    "            else:\n",
    "                raise Exception('Anata wa bakadesuka?')\n",
    "\n",
    "            assert n_muts == row['n_mut']\n",
    "\n",
    "            y_pred_2.append(score)\n",
    "\n",
    "        y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "        np.allclose(y_pred_1, y_pred_2, atol=1e-3)\n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_bt_loss_train.clear()\n",
    "        self.accumulate_batch_bt_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_kl_div_train.clear()\n",
    "        self.accumulate_batch_kl_div_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} bt loss: {np.mean(self.accumulate_batch_bt_loss_train)} kl div {np.mean(self.accumulate_batch_kl_div_train)} val loss: {np.mean(self.accumulate_batch_loss_val)} bt loss: {np.mean(self.accumulate_batch_bt_loss_val)} kl div {np.mean(self.accumulate_batch_kl_div_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        esm_protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model.logits(\n",
    "                esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "            )\n",
    "\n",
    "        logits = results.logits.sequence\n",
    "\n",
    "        log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "        return log_prob.to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '_'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def get_masked_marginal_var(self, mt_sequence, wt_sequence, mask_token = '_', mode='wt'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                masked_query_mt = list(mt_sequence)\n",
    "                masked_query_mt[i] = mask_token\n",
    "                masked_sequence_mt = ''.join(masked_query_mt)\n",
    "                masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "                if mode == 'wt':\n",
    "                    masked_query_wt = list(wt_sequence)\n",
    "                elif mode == 'mt':\n",
    "                    masked_query_wt = list(mt_sequence)\n",
    "                else:\n",
    "                    raise Exception('mode takes values mt and wt')\n",
    "\n",
    "                masked_query_wt[i] = mask_token\n",
    "                masked_sequence_wt = ''.join(masked_query_wt)\n",
    "                masked_log_prob_wt = self.get_log_prob(sequence=masked_sequence_wt)\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob_mt[i, idx_mt] - masked_log_prob_wt[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def pseudolikelihood(self, mt_sequence, mask_token = '_'):\n",
    "        score = 0\n",
    "        for i, aa_mt in enumerate(zip(mt_sequence)):\n",
    "\n",
    "            masked_query_mt = list(mt_sequence)\n",
    "            masked_query_mt[i] = mask_token\n",
    "            masked_sequence_mt = ''.join(masked_query_mt)\n",
    "            masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "            idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "            score += masked_log_prob_mt[i, idx_mt]\n",
    "\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask, test_mask = get_split_mask(df, omit_zero=False)\n",
    "df_train = df[train_mask]\n",
    "df_val = df[val_mask]\n",
    "df_test = df[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 50, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 20,\n",
    "        'early_stopping': False,\n",
    "        'lr': 5e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'use_seq_head': True,\n",
    "        'score': 'masked-random',\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCConFit(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pm in surrogate.model.named_parameters():\n",
    "    if pm.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_train, wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surrogate.config['epoch'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, wt_sequence, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "    # score, n_muts = surrogate.get_wildtype_marginal(mt_sequence, wt_sequence)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > -1\n",
    "color_mask = df['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_val_pred, y_val = y_pred[val_mask & omit_mask], y[val_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "color_mask = df['n_mut'] > 15\n",
    "# color_mask = n_muts_list > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM3 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDatasetLora(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.seq, self.y = df['seq'].to_numpy(), df['fitness_log'].to_numpy().astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMCLoRA(pl.LightningModule):\n",
    "    def __init__(self, name, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if name == 'esmc_300m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 960\n",
    "        elif name == 'esmc_600m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 1152\n",
    "        else:\n",
    "            raise Exception('Check ESMC name')\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"out_proj\", \"ffn.1\", \"ffn.3\", \"layernorm_qkv.1\"],\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.basemodel, peft_config)\n",
    "        # self.model = self.basemodel\n",
    "\n",
    "        for name, pm in self.model.named_parameters():\n",
    "            if 'q_ln' in name or 'k_ln' in name or 'transformer.norm.weight' in name or 'layernorm_qkv.0' in name or 'ffn.0' in name:\n",
    "                pm.requires_grad = True\n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        layers = []\n",
    "        for i in range(1, len(config['layers'])-1):\n",
    "            layers.append(nn.Linear(config['layers'][i-1], config['layers'][i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(config['layers'][-2], config['layers'][-1]))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        if config['freezePLM']:\n",
    "            for name, pm in self.model.named_parameters():\n",
    "                pm.requires_grad = False\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.accumulate_batch_mse_loss_train = []\n",
    "        self.accumulate_batch_mse_loss_val = []\n",
    "        self.accumulate_batch_kl_div_train = []\n",
    "        self.accumulate_batch_kl_div_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch_tokens):\n",
    "        \n",
    "        output = self.model(batch_tokens)\n",
    "\n",
    "        logits = output.sequence_logits\n",
    "        embeddings = output.embeddings\n",
    "\n",
    "        if self.config['embeddings'] == 'cls':\n",
    "            cls_embedding = embeddings[:, 0, :]\n",
    "        elif self.config['embeddings'] == 'mean':\n",
    "            assert self.config['freezePLM']\n",
    "            cls_embedding = embeddings[:,1:-1,:].mean(1)\n",
    "        elif self.config['freezePLM'] == 'concat':\n",
    "            assert self.config['freeze']\n",
    "            cls_embedding = embeddings[:,1:-1,:].flatten()\n",
    "        else:\n",
    "            raise Exception('Check your config man')\n",
    "\n",
    "        y_hat = self.mlp(cls_embedding)\n",
    "        \n",
    "        return y_hat, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        batch_tokens = self.model._tokenize(seq)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch_tokens)\n",
    "\n",
    "        mse_loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "\n",
    "        output = self.model_reg(batch_tokens)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = mse_loss + self.lambda_reg*l_reg\n",
    "        # loss = mse_loss \n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        self.accumulate_batch_mse_loss_train.append(mse_loss.item())\n",
    "        self.accumulate_batch_kl_div_train.append(l_reg.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        batch_tokens = self.model._tokenize(seq)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch_tokens)\n",
    "\n",
    "        mse_loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "\n",
    "        output = self.model_reg(batch_tokens)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = mse_loss + self.lambda_reg*l_reg\n",
    "\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "        self.accumulate_batch_mse_loss_val.append(mse_loss.item())\n",
    "        self.accumulate_batch_kl_div_val.append(l_reg.item())\n",
    "\n",
    "    def trainmodel(self, df, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetLora(df)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetLora(val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=False)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"bf16-mixed\",\n",
    "                                accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "  \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_mse_loss_train.clear()\n",
    "        self.accumulate_batch_mse_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_kl_div_train.clear()\n",
    "        self.accumulate_batch_kl_div_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} mse loss: {np.mean(self.accumulate_batch_mse_loss_train)} kl div {np.mean(self.accumulate_batch_kl_div_train)} val loss: {np.mean(self.accumulate_batch_loss_val)} mse loss: {np.mean(self.accumulate_batch_mse_loss_val)} kl div {np.mean(self.accumulate_batch_kl_div_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        esm_protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model.logits(\n",
    "                esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "            )\n",
    "\n",
    "        logits = results.logits.sequence\n",
    "\n",
    "        log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "        return log_prob.to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '_'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def get_masked_marginal_var(self, mt_sequence, wt_sequence, mask_token = '_', mode='wt'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                masked_query_mt = list(mt_sequence)\n",
    "                masked_query_mt[i] = mask_token\n",
    "                masked_sequence_mt = ''.join(masked_query_mt)\n",
    "                masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "                if mode == 'wt':\n",
    "                    masked_query_wt = list(wt_sequence)\n",
    "                elif mode == 'mt':\n",
    "                    masked_query_wt = list(mt_sequence)\n",
    "                else:\n",
    "                    raise Exception('mode takes values mt and wt')\n",
    "\n",
    "                masked_query_wt[i] = mask_token\n",
    "                masked_sequence_wt = ''.join(masked_query_wt)\n",
    "                masked_log_prob_wt = self.get_log_prob(sequence=masked_sequence_wt)\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob_mt[i, idx_mt] - masked_log_prob_wt[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def pseudolikelihood(self, mt_sequence, mask_token = '_'):\n",
    "        score = 0\n",
    "        for i, aa_mt in enumerate(zip(mt_sequence)):\n",
    "\n",
    "            masked_query_mt = list(mt_sequence)\n",
    "            masked_query_mt[i] = mask_token\n",
    "            masked_sequence_mt = ''.join(masked_query_mt)\n",
    "            masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "            idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "            score += masked_log_prob_mt[i, idx_mt]\n",
    "\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def predict(self, sequences):\n",
    "        pred = []\n",
    "        for seq in tqdm(sequences):\n",
    "            batch_tokens = self.model._tokenize([seq])\n",
    "\n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens = batch_tokens.cuda()\n",
    "                self.cuda()\n",
    "\n",
    "                assert next(self.parameters()).is_cuda == True\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                output = self.model(batch_tokens)\n",
    "                embeddings = output.embeddings\n",
    "\n",
    "                if self.config['embeddings'] == 'cls':\n",
    "                    cls_embedding = embeddings[:, 0, :]\n",
    "                elif self.config['embeddings'] == 'mean':\n",
    "                    assert self.config['freezePLM']\n",
    "                    cls_embedding = embeddings[:,1:-1,:].mean(1)\n",
    "                elif self.config['freezePLM'] == 'concat':\n",
    "                    assert self.config['freeze']\n",
    "                    cls_embedding = embeddings[:,1:-1,:].flatten()\n",
    "                else:\n",
    "                    raise Exception('Check your config man')\n",
    "    \n",
    "                y_hat = self.mlp(cls_embedding.to(torch.float32))\n",
    "\n",
    "                assert y_hat.shape[0] == 1\n",
    "                assert y_hat.shape[1] == 1\n",
    "                pred.append(y_hat[0][0].cpu().item())\n",
    "\n",
    "        return np.array(pred)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask, test_mask = get_split_mask(df, omit_zero=False)\n",
    "df_train = df[train_mask]\n",
    "df_val = df[val_mask]\n",
    "df_test = df[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5558400 || all params: 580345408 || trainable%: 0.96\n"
     ]
    }
   ],
   "source": [
    "config={'layers': [1152, 512, 1],\n",
    "        'epoch': 100, \n",
    "        'batch_size': 16,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 20,\n",
    "        'early_stopping': False,\n",
    "        'lr': 1e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'freezePLM': False,\n",
    "        'embeddings': 'cls',\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCLoRA(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel.transformer.blocks.0.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.0.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.0.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.0.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.0.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.0.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.0.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.0.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.0.ffn.0.weight\n",
      "basemodel.transformer.blocks.0.ffn.0.bias\n",
      "basemodel.transformer.blocks.0.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.0.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.0.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.0.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.1.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.1.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.1.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.1.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.1.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.1.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.1.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.1.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.1.ffn.0.weight\n",
      "basemodel.transformer.blocks.1.ffn.0.bias\n",
      "basemodel.transformer.blocks.1.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.1.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.1.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.1.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.2.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.2.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.2.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.2.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.2.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.2.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.2.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.2.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.2.ffn.0.weight\n",
      "basemodel.transformer.blocks.2.ffn.0.bias\n",
      "basemodel.transformer.blocks.2.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.2.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.2.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.2.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.3.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.3.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.3.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.3.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.3.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.3.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.3.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.3.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.3.ffn.0.weight\n",
      "basemodel.transformer.blocks.3.ffn.0.bias\n",
      "basemodel.transformer.blocks.3.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.3.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.3.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.3.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.4.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.4.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.4.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.4.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.4.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.4.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.4.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.4.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.4.ffn.0.weight\n",
      "basemodel.transformer.blocks.4.ffn.0.bias\n",
      "basemodel.transformer.blocks.4.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.4.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.4.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.4.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.5.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.5.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.5.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.5.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.5.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.5.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.5.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.5.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.5.ffn.0.weight\n",
      "basemodel.transformer.blocks.5.ffn.0.bias\n",
      "basemodel.transformer.blocks.5.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.5.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.5.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.5.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.6.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.6.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.6.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.6.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.6.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.6.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.6.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.6.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.6.ffn.0.weight\n",
      "basemodel.transformer.blocks.6.ffn.0.bias\n",
      "basemodel.transformer.blocks.6.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.6.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.6.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.6.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.7.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.7.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.7.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.7.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.7.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.7.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.7.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.7.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.7.ffn.0.weight\n",
      "basemodel.transformer.blocks.7.ffn.0.bias\n",
      "basemodel.transformer.blocks.7.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.7.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.7.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.7.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.8.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.8.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.8.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.8.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.8.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.8.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.8.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.8.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.8.ffn.0.weight\n",
      "basemodel.transformer.blocks.8.ffn.0.bias\n",
      "basemodel.transformer.blocks.8.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.8.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.8.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.8.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.9.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.9.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.9.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.9.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.9.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.9.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.9.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.9.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.9.ffn.0.weight\n",
      "basemodel.transformer.blocks.9.ffn.0.bias\n",
      "basemodel.transformer.blocks.9.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.9.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.9.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.9.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.10.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.10.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.10.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.10.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.10.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.10.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.10.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.10.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.10.ffn.0.weight\n",
      "basemodel.transformer.blocks.10.ffn.0.bias\n",
      "basemodel.transformer.blocks.10.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.10.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.10.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.10.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.11.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.11.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.11.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.11.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.11.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.11.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.11.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.11.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.11.ffn.0.weight\n",
      "basemodel.transformer.blocks.11.ffn.0.bias\n",
      "basemodel.transformer.blocks.11.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.11.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.11.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.11.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.12.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.12.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.12.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.12.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.12.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.12.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.12.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.12.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.12.ffn.0.weight\n",
      "basemodel.transformer.blocks.12.ffn.0.bias\n",
      "basemodel.transformer.blocks.12.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.12.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.12.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.12.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.13.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.13.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.13.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.13.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.13.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.13.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.13.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.13.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.13.ffn.0.weight\n",
      "basemodel.transformer.blocks.13.ffn.0.bias\n",
      "basemodel.transformer.blocks.13.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.13.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.13.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.13.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.14.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.14.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.14.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.14.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.14.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.14.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.14.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.14.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.14.ffn.0.weight\n",
      "basemodel.transformer.blocks.14.ffn.0.bias\n",
      "basemodel.transformer.blocks.14.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.14.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.14.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.14.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.15.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.15.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.15.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.15.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.15.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.15.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.15.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.15.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.15.ffn.0.weight\n",
      "basemodel.transformer.blocks.15.ffn.0.bias\n",
      "basemodel.transformer.blocks.15.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.15.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.15.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.15.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.16.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.16.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.16.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.16.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.16.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.16.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.16.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.16.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.16.ffn.0.weight\n",
      "basemodel.transformer.blocks.16.ffn.0.bias\n",
      "basemodel.transformer.blocks.16.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.16.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.16.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.16.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.17.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.17.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.17.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.17.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.17.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.17.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.17.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.17.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.17.ffn.0.weight\n",
      "basemodel.transformer.blocks.17.ffn.0.bias\n",
      "basemodel.transformer.blocks.17.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.17.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.17.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.17.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.18.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.18.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.18.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.18.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.18.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.18.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.18.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.18.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.18.ffn.0.weight\n",
      "basemodel.transformer.blocks.18.ffn.0.bias\n",
      "basemodel.transformer.blocks.18.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.18.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.18.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.18.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.19.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.19.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.19.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.19.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.19.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.19.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.19.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.19.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.19.ffn.0.weight\n",
      "basemodel.transformer.blocks.19.ffn.0.bias\n",
      "basemodel.transformer.blocks.19.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.19.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.19.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.19.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.20.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.20.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.20.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.20.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.20.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.20.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.20.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.20.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.20.ffn.0.weight\n",
      "basemodel.transformer.blocks.20.ffn.0.bias\n",
      "basemodel.transformer.blocks.20.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.20.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.20.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.20.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.21.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.21.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.21.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.21.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.21.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.21.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.21.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.21.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.21.ffn.0.weight\n",
      "basemodel.transformer.blocks.21.ffn.0.bias\n",
      "basemodel.transformer.blocks.21.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.21.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.21.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.21.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.22.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.22.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.22.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.22.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.22.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.22.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.22.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.22.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.22.ffn.0.weight\n",
      "basemodel.transformer.blocks.22.ffn.0.bias\n",
      "basemodel.transformer.blocks.22.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.22.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.22.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.22.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.23.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.23.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.23.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.23.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.23.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.23.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.23.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.23.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.23.ffn.0.weight\n",
      "basemodel.transformer.blocks.23.ffn.0.bias\n",
      "basemodel.transformer.blocks.23.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.23.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.23.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.23.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.24.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.24.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.24.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.24.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.24.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.24.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.24.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.24.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.24.ffn.0.weight\n",
      "basemodel.transformer.blocks.24.ffn.0.bias\n",
      "basemodel.transformer.blocks.24.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.24.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.24.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.24.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.25.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.25.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.25.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.25.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.25.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.25.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.25.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.25.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.25.ffn.0.weight\n",
      "basemodel.transformer.blocks.25.ffn.0.bias\n",
      "basemodel.transformer.blocks.25.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.25.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.25.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.25.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.26.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.26.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.26.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.26.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.26.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.26.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.26.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.26.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.26.ffn.0.weight\n",
      "basemodel.transformer.blocks.26.ffn.0.bias\n",
      "basemodel.transformer.blocks.26.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.26.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.26.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.26.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.27.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.27.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.27.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.27.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.27.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.27.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.27.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.27.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.27.ffn.0.weight\n",
      "basemodel.transformer.blocks.27.ffn.0.bias\n",
      "basemodel.transformer.blocks.27.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.27.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.27.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.27.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.28.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.28.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.28.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.28.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.28.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.28.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.28.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.28.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.28.ffn.0.weight\n",
      "basemodel.transformer.blocks.28.ffn.0.bias\n",
      "basemodel.transformer.blocks.28.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.28.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.28.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.28.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.29.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.29.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.29.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.29.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.29.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.29.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.29.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.29.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.29.ffn.0.weight\n",
      "basemodel.transformer.blocks.29.ffn.0.bias\n",
      "basemodel.transformer.blocks.29.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.29.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.29.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.29.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.30.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.30.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.30.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.30.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.30.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.30.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.30.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.30.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.30.ffn.0.weight\n",
      "basemodel.transformer.blocks.30.ffn.0.bias\n",
      "basemodel.transformer.blocks.30.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.30.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.30.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.30.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.31.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.31.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.31.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.31.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.31.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.31.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.31.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.31.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.31.ffn.0.weight\n",
      "basemodel.transformer.blocks.31.ffn.0.bias\n",
      "basemodel.transformer.blocks.31.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.31.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.31.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.31.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.32.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.32.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.32.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.32.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.32.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.32.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.32.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.32.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.32.ffn.0.weight\n",
      "basemodel.transformer.blocks.32.ffn.0.bias\n",
      "basemodel.transformer.blocks.32.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.32.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.32.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.32.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.33.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.33.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.33.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.33.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.33.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.33.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.33.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.33.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.33.ffn.0.weight\n",
      "basemodel.transformer.blocks.33.ffn.0.bias\n",
      "basemodel.transformer.blocks.33.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.33.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.33.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.33.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.34.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.34.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.34.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.34.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.34.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.34.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.34.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.34.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.34.ffn.0.weight\n",
      "basemodel.transformer.blocks.34.ffn.0.bias\n",
      "basemodel.transformer.blocks.34.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.34.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.34.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.34.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.blocks.35.attn.layernorm_qkv.0.weight\n",
      "basemodel.transformer.blocks.35.attn.layernorm_qkv.0.bias\n",
      "basemodel.transformer.blocks.35.attn.layernorm_qkv.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.35.attn.layernorm_qkv.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.35.attn.out_proj.lora_A.default.weight\n",
      "basemodel.transformer.blocks.35.attn.out_proj.lora_B.default.weight\n",
      "basemodel.transformer.blocks.35.attn.q_ln.weight\n",
      "basemodel.transformer.blocks.35.attn.k_ln.weight\n",
      "basemodel.transformer.blocks.35.ffn.0.weight\n",
      "basemodel.transformer.blocks.35.ffn.0.bias\n",
      "basemodel.transformer.blocks.35.ffn.1.lora_A.default.weight\n",
      "basemodel.transformer.blocks.35.ffn.1.lora_B.default.weight\n",
      "basemodel.transformer.blocks.35.ffn.3.lora_A.default.weight\n",
      "basemodel.transformer.blocks.35.ffn.3.lora_B.default.weight\n",
      "basemodel.transformer.norm.weight\n",
      "mlp.0.weight\n",
      "mlp.0.bias\n",
      "mlp.2.weight\n",
      "mlp.2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, pm in surrogate.named_parameters():\n",
    "    if pm .requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCLoRA(\n",
       "  (basemodel): ESMC(\n",
       "    (embed): Embedding(64, 1152)\n",
       "    (transformer): TransformerStack(\n",
       "      (blocks): ModuleList(\n",
       "        (0-35): 36 x UnifiedTransformerBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm_qkv): Sequential(\n",
       "              (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=3456, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3456, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (out_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (k_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (rotary): RotaryEmbedding()\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1152, out_features=6144, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (2): SwiGLU()\n",
       "            (3): lora.Linear(\n",
       "              (base_layer): Linear(in_features=3072, out_features=1152, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sequence_head): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1152, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model_reg): ESMC(\n",
       "    (embed): Embedding(64, 1152)\n",
       "    (transformer): TransformerStack(\n",
       "      (blocks): ModuleList(\n",
       "        (0-35): 36 x UnifiedTransformerBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm_qkv): Sequential(\n",
       "              (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1152, out_features=3456, bias=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "            (q_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (k_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (rotary): RotaryEmbedding()\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1152, out_features=6144, bias=False)\n",
       "            (2): SwiGLU()\n",
       "            (3): Linear(in_features=3072, out_features=1152, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sequence_head): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1152, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): ESMC(\n",
       "        (embed): Embedding(64, 1152)\n",
       "        (transformer): TransformerStack(\n",
       "          (blocks): ModuleList(\n",
       "            (0-35): 36 x UnifiedTransformerBlock(\n",
       "              (attn): MultiHeadAttention(\n",
       "                (layernorm_qkv): Sequential(\n",
       "                  (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                  (1): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=3456, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=3456, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                (k_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                (rotary): RotaryEmbedding()\n",
       "              )\n",
       "              (ffn): Sequential(\n",
       "                (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1152, out_features=6144, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (2): SwiGLU()\n",
       "                (3): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=3072, out_features=1152, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (sequence_head): Sequential(\n",
       "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "          (3): Linear(in_features=1152, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['epoch'] = 150\n",
    "surrogate.config['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A2000 12GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-01-24 13:39:02.952203: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 13:39:02.954538: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-24 13:39:02.959271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-24 13:39:02.968272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-24 13:39:02.968290: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 13:39:02.974445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-24 13:39:03.902477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | basemodel | ESMC       | 580 M  | train\n",
      "1 | model_reg | ESMC       | 575 M  | eval \n",
      "2 | model     | PeftModel  | 580 M  | train\n",
      "3 | mlp       | Sequential | 590 K  | train\n",
      "-------------------------------------------------\n",
      "6.1 M     Trainable params\n",
      "1.1 B     Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,623.893 Total estimated model params size (MB)\n",
      "1960      Modules in train mode\n",
      "514       Modules in eval mode\n",
      "/nethome/kgeorge/miniconda3/envs/workspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/nethome/kgeorge/miniconda3/envs/workspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/nethome/kgeorge/miniconda3/envs/workspace/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: train loss: 38.292799949645996 mse loss: 38.292799949645996 kl div 0.0 val loss: 19.21043086051941 mse loss: 19.200048446655273 kl div 0.10382414609193802\n",
      "Epoch: 1: train loss: 44.4222993850708 mse loss: 44.41083526611328 kl div 0.11464129947125912 val loss: 19.173666179180145 mse loss: 19.162968039512634 kl div 0.10698577016592026\n",
      "Epoch: 2: train loss: 30.975264318287373 mse loss: 30.964109655469656 kl div 0.11154678650200367 val loss: 19.13584315776825 mse loss: 19.125374019145966 kl div 0.10468899086117744\n",
      "Epoch: 3: train loss: 31.115004897117615 mse loss: 31.103587716817856 kl div 0.11416732147336006 val loss: 19.100135445594788 mse loss: 19.089695811271667 kl div 0.10439766943454742\n",
      "Epoch: 4: train loss: 45.874873638153076 mse loss: 45.863664627075195 kl div 0.11209895461797714 val loss: 19.06350290775299 mse loss: 19.05288165807724 kl div 0.10620872676372528\n",
      "Epoch: 5: train loss: 38.02980136871338 mse loss: 38.01845455169678 kl div 0.11347308941185474 val loss: 19.028770923614502 mse loss: 19.017883360385895 kl div 0.10887925326824188\n",
      "Epoch: 6: train loss: 32.866514921188354 mse loss: 32.85493588447571 kl div 0.11579907312989235 val loss: 18.99398422241211 mse loss: 18.98281490802765 kl div 0.11169157922267914\n",
      "Epoch: 7: train loss: 38.57405471801758 mse loss: 38.561994552612305 kl div 0.1205988023430109 val loss: 18.95789211988449 mse loss: 18.946601033210754 kl div 0.11291110888123512\n",
      "Epoch: 8: train loss: 30.88718220591545 mse loss: 30.875438302755356 kl div 0.11743701621890068 val loss: 18.923308670520782 mse loss: 18.9116268157959 kl div 0.11682239174842834\n",
      "Epoch: 9: train loss: 43.897541999816895 mse loss: 43.8857216835022 kl div 0.11819637008011341 val loss: 18.887121438980103 mse loss: 18.875564634799957 kl div 0.11557549238204956\n",
      "Epoch: 10: train loss: 45.690885066986084 mse loss: 45.67767095565796 kl div 0.13213864900171757 val loss: 18.852258682250977 mse loss: 18.84026837348938 kl div 0.1199098601937294\n",
      "Epoch: 11: train loss: 32.23498010635376 mse loss: 32.22270750999451 kl div 0.12271725945174694 val loss: 18.818089485168457 mse loss: 18.80530571937561 kl div 0.12784530222415924\n",
      "Epoch: 12: train loss: 32.221370458602905 mse loss: 32.208457708358765 kl div 0.12913187965750694 val loss: 18.782842874526978 mse loss: 18.768726587295532 kl div 0.14116571843624115\n",
      "Epoch: 13: train loss: 33.29607963562012 mse loss: 33.28273916244507 kl div 0.13340547680854797 val loss: 18.747792899608612 mse loss: 18.73317289352417 kl div 0.14620617777109146\n",
      "Epoch: 14: train loss: 43.85056781768799 mse loss: 43.83628273010254 kl div 0.14285467565059662 val loss: 18.712217092514038 mse loss: 18.695974707603455 kl div 0.16242830455303192\n",
      "Epoch: 15: train loss: 31.03545343875885 mse loss: 31.020906567573547 kl div 0.1454719565808773 val loss: 18.67699146270752 mse loss: 18.659624338150024 kl div 0.1736709177494049\n",
      "Epoch: 16: train loss: 43.3553524017334 mse loss: 43.33924436569214 kl div 0.16108490899205208 val loss: 18.641762256622314 mse loss: 18.622607231140137 kl div 0.19154094904661179\n",
      "Epoch: 17: train loss: 44.985997676849365 mse loss: 44.968249797821045 kl div 0.17747950926423073 val loss: 18.60324317216873 mse loss: 18.582069277763367 kl div 0.2117452546954155\n",
      "Epoch: 18: train loss: 32.13627529144287 mse loss: 32.11658954620361 kl div 0.1968565359711647 val loss: 18.563880503177643 mse loss: 18.539762377738953 kl div 0.24117740243673325\n",
      "Epoch: 19: train loss: 36.98487186431885 mse loss: 36.96301078796387 kl div 0.21860963106155396 val loss: 18.524358987808228 mse loss: 18.495553076267242 kl div 0.2880547046661377\n",
      "Epoch: 20: train loss: 44.80987191200256 mse loss: 44.78296113014221 kl div 0.26910490542650223 val loss: 18.48424643278122 mse loss: 18.44949722290039 kl div 0.3474889397621155\n",
      "Epoch: 21: train loss: 36.708927631378174 mse loss: 36.677064418792725 kl div 0.31862976402044296 val loss: 18.441282868385315 mse loss: 18.398962378501892 kl div 0.42320460081100464\n",
      "Epoch: 22: train loss: 32.66988229751587 mse loss: 32.62923836708069 kl div 0.4064401760697365 val loss: 18.39586192369461 mse loss: 18.343326568603516 kl div 0.5253618061542511\n",
      "Epoch: 23: train loss: 36.67894744873047 mse loss: 36.625269412994385 kl div 0.5367774143815041 val loss: 18.35152006149292 mse loss: 18.286299467086792 kl div 0.6522048711776733\n",
      "Epoch: 24: train loss: 36.39865970611572 mse loss: 36.321882486343384 kl div 0.76777583360672 val loss: 18.310041904449463 mse loss: 18.23594617843628 kl div 0.7409655153751373\n",
      "Epoch: 25: train loss: 36.2906928062439 mse loss: 36.211955070495605 kl div 0.7873785048723221 val loss: 18.267133474349976 mse loss: 18.18848890066147 kl div 0.7864540815353394\n",
      "Epoch: 26: train loss: 48.958718061447144 mse loss: 48.87663531303406 kl div 0.8208234906196594 val loss: 18.221952438354492 mse loss: 18.137124359607697 kl div 0.8482764065265656\n",
      "Epoch: 27: train loss: 37.32105875015259 mse loss: 37.235790491104126 kl div 0.8526796847581863 val loss: 18.169843912124634 mse loss: 18.080509543418884 kl div 0.8933499157428741\n",
      "Epoch: 28: train loss: 31.10826086997986 mse loss: 31.021807193756104 kl div 0.8645374029874802 val loss: 18.11594820022583 mse loss: 18.021345257759094 kl div 0.9460331797599792\n",
      "Epoch: 29: train loss: 31.530316591262817 mse loss: 31.439705848693848 kl div 0.9061046540737152 val loss: 18.054603338241577 mse loss: 17.952714771032333 kl div 1.018883228302002\n",
      "Epoch: 30: train loss: 37.05968761444092 mse loss: 36.96086406707764 kl div 0.9882330298423767 val loss: 17.981382846832275 mse loss: 17.871238231658936 kl div 1.1014376878738403\n",
      "Epoch: 31: train loss: 37.6503701210022 mse loss: 37.53232288360596 kl div 1.1804668307304382 val loss: 17.907533556222916 mse loss: 17.789924293756485 kl div 1.1760973930358887\n",
      "Epoch: 32: train loss: 41.86903715133667 mse loss: 41.74639439582825 kl div 1.226426213979721 val loss: 17.837010979652405 mse loss: 17.709771782159805 kl div 1.272391676902771\n",
      "Epoch: 33: train loss: 31.07599425315857 mse loss: 30.949292421340942 kl div 1.2670193016529083 val loss: 17.760673850774765 mse loss: 17.621347874403 kl div 1.393269121646881\n",
      "Epoch: 34: train loss: 37.84796380996704 mse loss: 37.70515155792236 kl div 1.4281296133995056 val loss: 17.68215650320053 mse loss: 17.533228158950806 kl div 1.489290177822113\n",
      "Epoch: 35: train loss: 36.869646072387695 mse loss: 36.72987413406372 kl div 1.3977168202400208 val loss: 17.584616124629974 mse loss: 17.425357043743134 kl div 1.592593789100647\n",
      "Epoch: 36: train loss: 41.663132190704346 mse loss: 41.51536226272583 kl div 1.4776900708675385 val loss: 17.47397541999817 mse loss: 17.301383942365646 kl div 1.7259119749069214\n",
      "Epoch: 37: train loss: 28.919140338897705 mse loss: 28.765818655490875 kl div 1.5332156121730804 val loss: 17.375376492738724 mse loss: 17.195151805877686 kl div 1.8022487163543701\n",
      "Epoch: 38: train loss: 36.68981742858887 mse loss: 36.523250102996826 kl div 1.6656726002693176 val loss: 17.294736713171005 mse loss: 17.10731753706932 kl div 1.874187707901001\n",
      "Epoch: 39: train loss: 35.134074211120605 mse loss: 34.97427797317505 kl div 1.5979531109333038 val loss: 17.199965864419937 mse loss: 17.002741366624832 kl div 1.9722425937652588\n",
      "Epoch: 40: train loss: 48.860008239746094 mse loss: 48.68605661392212 kl div 1.739518165588379 val loss: 17.093296110630035 mse loss: 16.883090555667877 kl div 2.1020589470863342\n",
      "Epoch: 41: train loss: 35.10149574279785 mse loss: 34.929611682891846 kl div 1.7188391387462616 val loss: 16.99483072757721 mse loss: 16.785161316394806 kl div 2.0966873168945312\n",
      "Epoch: 42: train loss: 33.959853649139404 mse loss: 33.79465579986572 kl div 1.651978462934494 val loss: 16.89921149611473 mse loss: 16.691504180431366 kl div 2.0770658254623413\n",
      "Epoch: 43: train loss: 33.947388648986816 mse loss: 33.789127826690674 kl div 1.5826042890548706 val loss: 16.771081686019897 mse loss: 16.561476409435272 kl div 2.096061587333679\n",
      "Epoch: 44: train loss: 34.24026346206665 mse loss: 34.08276605606079 kl div 1.5749826729297638 val loss: 16.618560910224915 mse loss: 16.408233880996704 kl div 2.1032795906066895\n",
      "Epoch: 45: train loss: 33.34670805931091 mse loss: 33.18810558319092 kl div 1.5860159397125244 val loss: 16.50116854906082 mse loss: 16.294697701931 kl div 2.0647151470184326\n",
      "Epoch: 46: train loss: 34.46734857559204 mse loss: 34.30737113952637 kl div 1.5997729897499084 val loss: 16.393146842718124 mse loss: 16.190361708402634 kl div 2.0278493762016296\n",
      "Epoch: 47: train loss: 33.32547330856323 mse loss: 33.17725086212158 kl div 1.4822306334972382 val loss: 16.278524518013 mse loss: 16.07758730649948 kl div 2.0093683004379272\n",
      "Epoch: 48: train loss: 33.894448041915894 mse loss: 33.74068880081177 kl div 1.5375883877277374 val loss: 16.146441519260406 mse loss: 15.944440245628357 kl div 2.0200140476226807\n",
      "Epoch: 49: train loss: 27.455357789993286 mse loss: 27.30188751220703 kl div 1.534709095954895 val loss: 15.995646864175797 mse loss: 15.790298044681549 kl div 2.05348539352417\n",
      "Epoch: 50: train loss: 26.571794524788857 mse loss: 26.425243020057678 kl div 1.465518057346344 val loss: 15.879928857088089 mse loss: 15.672856718301773 kl div 2.0707221031188965\n",
      "Epoch: 51: train loss: 32.17196440696716 mse loss: 32.02180790901184 kl div 1.5015675127506256 val loss: 15.747244030237198 mse loss: 15.535197466611862 kl div 2.1204694509506226\n",
      "Epoch: 52: train loss: 37.967061281204224 mse loss: 37.805102586746216 kl div 1.6195808351039886 val loss: 15.618672490119934 mse loss: 15.399880141019821 kl div 2.1879249811172485\n",
      "Epoch: 53: train loss: 37.56337356567383 mse loss: 37.40742826461792 kl div 1.559455692768097 val loss: 15.47436261177063 mse loss: 15.249232053756714 kl div 2.2513041496276855\n",
      "Epoch: 54: train loss: 25.85843774676323 mse loss: 25.70315971970558 kl div 1.5527812540531158 val loss: 15.352045476436615 mse loss: 15.123213440179825 kl div 2.2883206605911255\n",
      "Epoch: 55: train loss: 32.59747862815857 mse loss: 32.43587255477905 kl div 1.6160591542720795 val loss: 15.214594155550003 mse loss: 14.983569622039795 kl div 2.310246229171753\n",
      "Epoch: 56: train loss: 30.992210626602173 mse loss: 30.82983422279358 kl div 1.6237731873989105 val loss: 15.083363085985184 mse loss: 14.850973725318909 kl div 2.323890805244446\n",
      "Epoch: 57: train loss: 30.826436281204224 mse loss: 30.667534351348877 kl div 1.5890186727046967 val loss: 14.938940823078156 mse loss: 14.705809831619263 kl div 2.3313082456588745\n",
      "Epoch: 58: train loss: 25.13586401939392 mse loss: 24.9812763184309 kl div 1.545876830816269 val loss: 14.810178279876709 mse loss: 14.57936817407608 kl div 2.3081023693084717\n",
      "Epoch: 59: train loss: 36.000433921813965 mse loss: 35.831209659576416 kl div 1.692243069410324 val loss: 14.679555833339691 mse loss: 14.450484275817871 kl div 2.2907114028930664\n",
      "Epoch: 60: train loss: 30.213592052459717 mse loss: 30.05657434463501 kl div 1.5701722502708435 val loss: 14.559681594371796 mse loss: 14.33081066608429 kl div 2.2887121438980103\n",
      "Epoch: 61: train loss: 24.521255880594254 mse loss: 24.363284781575203 kl div 1.5797100961208344 val loss: 14.418799042701721 mse loss: 14.188612192869186 kl div 2.3018656969070435\n",
      "Epoch: 62: train loss: 24.52536243200302 mse loss: 24.376821100711823 kl div 1.485414445400238 val loss: 14.302051305770874 mse loss: 14.068513870239258 kl div 2.3353729248046875\n",
      "Epoch: 63: train loss: 35.35752725601196 mse loss: 35.19350814819336 kl div 1.6401880383491516 val loss: 14.214200913906097 mse loss: 13.980631589889526 kl div 2.335694670677185\n",
      "Epoch: 64: train loss: 34.575119495391846 mse loss: 34.4226713180542 kl div 1.5244832932949066 val loss: 14.08731484413147 mse loss: 13.851261734962463 kl div 2.3605282306671143\n",
      "Epoch: 65: train loss: 29.846837520599365 mse loss: 29.69465970993042 kl div 1.521774411201477 val loss: 13.975046455860138 mse loss: 13.737503707408905 kl div 2.3754303455352783\n",
      "Epoch: 66: train loss: 33.83833694458008 mse loss: 33.68108892440796 kl div 1.572482407093048 val loss: 13.88497108221054 mse loss: 13.647674322128296 kl div 2.3729687929153442\n",
      "Epoch: 67: train loss: 23.480635851621628 mse loss: 23.33350047469139 kl div 1.4713491201400757 val loss: 13.811037480831146 mse loss: 13.57741403579712 kl div 2.3362315893173218\n",
      "Epoch: 68: train loss: 24.146650314331055 mse loss: 23.993610382080078 kl div 1.5304012894630432 val loss: 13.710949540138245 mse loss: 13.478220462799072 kl div 2.3272894620895386\n",
      "Epoch: 69: train loss: 28.52986478805542 mse loss: 28.378053665161133 kl div 1.5181131064891815 val loss: 13.585807263851166 mse loss: 13.353264033794403 kl div 2.3254337310791016\n",
      "Epoch: 70: train loss: 28.04924249649048 mse loss: 27.900089263916016 kl div 1.49153333902359 val loss: 13.488347589969635 mse loss: 13.259296536445618 kl div 2.2905101776123047\n",
      "Epoch: 71: train loss: 23.782522201538086 mse loss: 23.63887655735016 kl div 1.4364612400531769 val loss: 13.483470797538757 mse loss: 13.260390400886536 kl div 2.2308040261268616\n",
      "Epoch: 72: train loss: 27.44227123260498 mse loss: 27.30029582977295 kl div 1.4197534918785095 val loss: 13.419746398925781 mse loss: 13.200705111026764 kl div 2.190412163734436\n",
      "Epoch: 73: train loss: 27.119099617004395 mse loss: 26.977718830108643 kl div 1.4138075411319733 val loss: 13.380967855453491 mse loss: 13.16300743818283 kl div 2.1796090602874756\n",
      "Epoch: 74: train loss: 22.2093288898468 mse loss: 22.075864017009735 kl div 1.3346485793590546 val loss: 13.457712531089783 mse loss: 13.244724035263062 kl div 2.1298835277557373\n",
      "Epoch: 75: train loss: 22.30938357114792 mse loss: 22.169102609157562 kl div 1.402815043926239 val loss: 13.276150107383728 mse loss: 13.059599757194519 kl div 2.1654993295669556\n",
      "Epoch: 76: train loss: 26.17887854576111 mse loss: 26.047186136245728 kl div 1.3169254958629608 val loss: 13.1614431142807 mse loss: 12.940427780151367 kl div 2.210151195526123\n",
      "Epoch: 77: train loss: 26.160434424877167 mse loss: 26.02488422393799 kl div 1.3555015921592712 val loss: 13.203317880630493 mse loss: 12.986663341522217 kl div 2.1665477752685547\n",
      "Epoch: 78: train loss: 26.424693822860718 mse loss: 26.2862491607666 kl div 1.3844518959522247 val loss: 13.3754723072052 mse loss: 13.164039134979248 kl div 2.1143299341201782\n",
      "Epoch: 79: train loss: 30.01485800743103 mse loss: 29.879812717437744 kl div 1.3504569828510284 val loss: 13.14226245880127 mse loss: 12.919841885566711 kl div 2.2242071628570557\n",
      "Epoch: 80: train loss: 29.642558574676514 mse loss: 29.49650812149048 kl div 1.4605052769184113 val loss: 13.068578124046326 mse loss: 12.8410964012146 kl div 2.274818480014801\n",
      "Epoch: 81: train loss: 20.736542642116547 mse loss: 20.594832122325897 kl div 1.4171070456504822 val loss: 13.270198106765747 mse loss: 13.04868745803833 kl div 2.215107798576355\n",
      "Epoch: 82: train loss: 24.913567543029785 mse loss: 24.77486562728882 kl div 1.3870208263397217 val loss: 13.393845081329346 mse loss: 13.176316857337952 kl div 2.1752817630767822\n",
      "Epoch: 83: train loss: 20.05918562412262 mse loss: 19.930662870407104 kl div 1.2852284610271454 val loss: 13.335450053215027 mse loss: 13.114234447479248 kl div 2.212160110473633\n",
      "Epoch: 84: train loss: 24.093578577041626 mse loss: 23.952245235443115 kl div 1.4133317470550537 val loss: 13.25740933418274 mse loss: 13.030454754829407 kl div 2.2695401906967163\n",
      "Epoch: 85: train loss: 27.716201305389404 mse loss: 27.5757999420166 kl div 1.4040138125419617 val loss: 13.296751976013184 mse loss: 13.07779335975647 kl div 2.1895867586135864\n",
      "Epoch: 86: train loss: 23.499881505966187 mse loss: 23.36837911605835 kl div 1.3150227069854736 val loss: 13.432365417480469 mse loss: 13.217745661735535 kl div 2.1461939811706543\n",
      "Epoch: 87: train loss: 22.83958411216736 mse loss: 22.70301866531372 kl div 1.3656571209430695 val loss: 13.507873177528381 mse loss: 13.295920610427856 kl div 2.1195297241210938\n",
      "Epoch: 88: train loss: 26.26163125038147 mse loss: 26.128097534179688 kl div 1.3353392779827118 val loss: 13.592474222183228 mse loss: 13.380555152893066 kl div 2.1191914081573486\n",
      "Epoch: 89: train loss: 22.34511399269104 mse loss: 22.203975439071655 kl div 1.4113832712173462 val loss: 13.661647319793701 mse loss: 13.44818639755249 kl div 2.1346088647842407\n",
      "Epoch: 90: train loss: 25.468937158584595 mse loss: 25.331462383270264 kl div 1.3747429251670837 val loss: 13.720386743545532 mse loss: 13.51036822795868 kl div 2.100187063217163\n",
      "Epoch: 91: train loss: 21.161625862121582 mse loss: 21.028254508972168 kl div 1.3337124288082123 val loss: 13.76774537563324 mse loss: 13.56165599822998 kl div 2.0608971118927\n",
      "Epoch: 92: train loss: 20.846641778945923 mse loss: 20.721675395965576 kl div 1.2496639639139175 val loss: 13.824777841567993 mse loss: 13.610567569732666 kl div 2.1421019434928894\n",
      "Epoch: 93: train loss: 16.729373589158058 mse loss: 16.598731473088264 kl div 1.3064202070236206 val loss: 13.810224771499634 mse loss: 13.595736503601074 kl div 2.144877314567566\n",
      "Epoch: 94: train loss: 23.785162806510925 mse loss: 23.64372444152832 kl div 1.4143818318843842 val loss: 13.832116842269897 mse loss: 13.624349236488342 kl div 2.077679932117462\n",
      "Epoch: 95: train loss: 19.715370416641235 mse loss: 19.58375382423401 kl div 1.316164880990982 val loss: 13.803520441055298 mse loss: 13.595897912979126 kl div 2.076226592063904\n",
      "Epoch: 96: train loss: 19.4647616147995 mse loss: 19.334136724472046 kl div 1.306251347064972 val loss: 13.858545064926147 mse loss: 13.64286756515503 kl div 2.156777858734131\n",
      "Epoch: 97: train loss: 19.010101795196533 mse loss: 18.8721661567688 kl div 1.3793526589870453 val loss: 13.971367597579956 mse loss: 13.751975297927856 kl div 2.1939266324043274\n",
      "Epoch: 98: train loss: 19.081652879714966 mse loss: 18.93906033039093 kl div 1.4259255230426788 val loss: 13.85351824760437 mse loss: 13.634902000427246 kl div 2.1861597299575806\n",
      "Epoch: 99: train loss: 18.487141385674477 mse loss: 18.354772344231606 kl div 1.3236925899982452 val loss: 13.739683270454407 mse loss: 13.527785539627075 kl div 2.118974506855011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: train loss: 18.487141385674477 val loss: 13.739683270454407\n"
     ]
    }
   ],
   "source": [
    "surrogate.trainmodel(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:03<00:00, 27.05it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = surrogate.predict(df['seq'])\n",
    "y = df['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "omit_mask = df['fitness_raw'] > -1\n",
    "color_mask = df['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_val_pred, y_val = y_pred[val_mask & omit_mask], y[val_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "color_mask = df['n_mut'] > 15\n",
    "# color_mask = n_muts_list > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
