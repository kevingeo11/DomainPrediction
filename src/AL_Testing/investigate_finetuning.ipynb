{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.al import top_model as topmodel\n",
    "from DomainPrediction.al.embeddings import one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../esm')\n",
    "from DomainPrediction.esm.esm3 import ESM3LM\n",
    "from DomainPrediction.esm.esmc import ESMCLM\n",
    "from DomainPrediction.al.confit import ESMCConFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nethome/kgeorge/workspace/DomainPrediction/Data/al_test_experiments/Tdomain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(data_path, 'dataset_2_tdomain.csv')\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_mask(df, omit_zero=False):\n",
    "    if omit_zero:\n",
    "        train_mask = (df['split_id'] == 2) & (df['fitness_raw'] != 0)\n",
    "    else:\n",
    "        train_mask = (df['split_id'] == 2)\n",
    "\n",
    "    val_mask = df['split_id'] == 1\n",
    "    test_mask = df['split_id'].isin([0, 1])\n",
    "    # test_mask = df['split_id'] == 0\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearmanr_bootstrap(a, b, n=1000):\n",
    "    assert type(a) == type(b) == np.ndarray\n",
    "    assert len(a) == len(b)\n",
    "    corr = []\n",
    "    p_value = []\n",
    "    np.random.seed(0)\n",
    "    for _ in range(n):\n",
    "        indices = np.random.choice(len(a), size=len(a), replace=True)\n",
    "        res = stats.spearmanr(a[indices], b[indices])\n",
    "        \n",
    "        if not np.isnan(res.statistic):\n",
    "            corr.append(res.statistic)\n",
    "            p_value.append(res.pvalue)\n",
    "\n",
    "    ci_lower, ci_upper = np.percentile(corr, [5, 95]) \n",
    "    # stats.t.interval(confidence=0.95, df=len(corr)-1, loc=np.mean(corr), scale=np.std(corr))\n",
    "    mean_corr = np.mean(corr)\n",
    "\n",
    "    return round(mean_corr, 2), round(ci_lower, 2), round(ci_upper, 2), corr, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask, test_mask = get_split_mask(df, omit_zero=False)\n",
    "df_train = df[train_mask]\n",
    "df_val = df[val_mask]\n",
    "df_test = df[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmc = ESMCLM(name='esmc_600m', device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wt marginals\n",
    "y_pred = []\n",
    "n_muts_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = esmc.get_wildtype_marginal(mt_sequence, wt_sequence)\n",
    "    # score, n_muts = esmc.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "    # score, n_muts = esmc.get_masked_marginal_var(mt_sequence, wt_sequence, mode='wt')\n",
    "    # score, n_muts = esmc.get_masked_marginal_var(mt_sequence, wt_sequence, mode='mt')\n",
    "    # score = esmc.pseudolikelihood(mt_sequence)\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "    n_muts_list.append(n_muts)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df['fitness_log'].to_numpy().astype(np.float32)\n",
    "\n",
    "n_muts_list = np.array(n_muts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > -1\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_val_pred, y_val = y_pred[val_mask & omit_mask], y[val_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "color_mask = df['n_mut'] > 15\n",
    "# color_mask = n_muts_list > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7,3), layout='constrained')\n",
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "ax[0].hist(df['n_mut'][train_mask & omit_mask])\n",
    "ax[1].hist(df['n_mut'][test_mask & omit_mask])\n",
    "for i in range(2):\n",
    "    ax[i].set_title('hamming dist', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7,3), layout='constrained')\n",
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "ax[0].hist(n_muts_list[train_mask & omit_mask])\n",
    "ax[1].hist(n_muts_list[test_mask & omit_mask])\n",
    "for i in range(2):\n",
    "    ax[i].set_title('hamming dist', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDatasetContrast(Dataset):\n",
    "    def __init__(self, df, wt):\n",
    "        self.seq, self.y = df['seq'].to_numpy(), df['fitness_raw'].to_numpy()\n",
    "        self.wt = np.array([wt]*self.seq.shape[0], dtype='object')\n",
    "        self.n_mut = df['n_mut'].to_numpy()\n",
    "\n",
    "        self.positions = []\n",
    "        for _, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            pos = []\n",
    "            for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt)):\n",
    "                if aa_wt != aa_mt:\n",
    "                    ## mutation pos\n",
    "                    pos.append(i)\n",
    "\n",
    "            assert len(pos) == row['n_mut']\n",
    "\n",
    "            self.positions.append(np.array(pos))\n",
    "\n",
    "        assert len(self.positions) == self.seq.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.y[idx], self.wt[idx], self.positions[idx], self.n_mut[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        seq = np.array([x[0] for x in data], dtype='object')\n",
    "        y = torch.tensor([x[1] for x in data])\n",
    "        wt = np.array([x[2] for x in data], dtype='object')\n",
    "        pos = [x[3] for x in data]\n",
    "        n_mut = np.array([x[4] for x in data])\n",
    "        return seq, y, wt, pos, n_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMCConFit(pl.LightningModule):\n",
    "    def __init__(self, name, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if name == 'esmc_300m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 960\n",
    "        elif name == 'esmc_600m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 1152\n",
    "        else:\n",
    "            raise Exception('Check ESMC name')\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"out_proj\"],\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.basemodel, peft_config)\n",
    "\n",
    "        for name, pm in self.model.named_parameters():\n",
    "            if 'q_ln' in name or 'k_ln' in name:\n",
    "                pm.requires_grad = True\n",
    "\n",
    "        if self.config['use_seq_head']:\n",
    "            for name, pm in self.model.named_parameters():\n",
    "                if 'sequence_head' in name:\n",
    "                    pm.requires_grad = True\n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.accumulate_batch_bt_loss_train = []\n",
    "        self.accumulate_batch_bt_loss_val = []\n",
    "        self.accumulate_batch_kl_div_train = []\n",
    "        self.accumulate_batch_kl_div_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch, batch_tokens_masked, batch_tokens, batch_tokens_wt):\n",
    "        mt_seq, _, wt_seq, pos, n_mut = batch\n",
    "        \n",
    "        output = self.model(batch_tokens_masked)\n",
    "        logits = output.sequence_logits\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        scores = torch.zeros(log_probs.shape[0])\n",
    "        if self.config['device'] == 'gpu':\n",
    "            scores = scores.cuda()\n",
    "\n",
    "        for i in range(log_probs.shape[0]):\n",
    "            scores[i] = torch.sum(log_probs[i, pos[i]+1, batch_tokens[i][pos[i]+1]] - log_probs[i, pos[i]+1, batch_tokens_wt[i][pos[i]+1]])\n",
    "        \n",
    "        return scores, logits\n",
    "    \n",
    "    def BT_loss(self, scores, y):\n",
    "        loss = torch.tensor(0.)\n",
    "        if self.config['device'] == 'gpu':\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(i, len(scores)):\n",
    "                if y[i] > y[j]:\n",
    "                    if torch.abs(scores[j]-scores[i]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[j]-scores[i]))\n",
    "                else:\n",
    "                    if torch.abs(scores[i]-scores[j]) < 80:\n",
    "                        loss += torch.log(1 + torch.exp(scores[i]-scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_train.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_train.append(l_reg.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        # print(y_hat)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        # print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "        self.accumulate_batch_bt_loss_val.append(bt_loss.item())\n",
    "        self.accumulate_batch_kl_div_val.append(l_reg.item())\n",
    "\n",
    "    def trainmodel(self, df, wt, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetContrast(df, wt)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetContrast(val, wt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=True)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"bf16-mixed\",\n",
    "                                accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    def sanity_check(self, df, wt):\n",
    "        dataset = ProteinFunDatasetContrast(df, wt)\n",
    "        loader = DataLoader(dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        y_pred_1 = []\n",
    "        for batch in loader:\n",
    "            mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "            batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "            batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "            \n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat, _ = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "            y_pred_1.append(y_hat.cpu().numpy())\n",
    "\n",
    "        y_pred_1 = np.concatenate(y_pred_1)\n",
    "\n",
    "        y_pred_2 = []\n",
    "        for i, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            score, n_muts = self.get_masked_marginal(mt_sequence, wt)\n",
    "            assert n_muts == row['n_mut']\n",
    "\n",
    "            y_pred_2.append(score)\n",
    "\n",
    "        y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "        np.allclose(y_pred_1, y_pred_2, atol=1e-3)\n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_bt_loss_train.clear()\n",
    "        self.accumulate_batch_bt_loss_val.clear()\n",
    "\n",
    "        self.accumulate_batch_kl_div_train.clear()\n",
    "        self.accumulate_batch_kl_div_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} bt loss: {np.mean(self.accumulate_batch_bt_loss_train)} kl div {np.mean(self.accumulate_batch_kl_div_train)} val loss: {np.mean(self.accumulate_batch_loss_val)} bt loss: {np.mean(self.accumulate_batch_bt_loss_val)} kl div {np.mean(self.accumulate_batch_kl_div_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        esm_protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model.logits(\n",
    "                esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "            )\n",
    "\n",
    "        logits = results.logits.sequence\n",
    "\n",
    "        log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "        return log_prob.to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '_'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def get_masked_marginal_var(self, mt_sequence, wt_sequence, mask_token = '_', mode='wt'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                masked_query_mt = list(mt_sequence)\n",
    "                masked_query_mt[i] = mask_token\n",
    "                masked_sequence_mt = ''.join(masked_query_mt)\n",
    "                masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "                if mode == 'wt':\n",
    "                    masked_query_wt = list(wt_sequence)\n",
    "                elif mode == 'mt':\n",
    "                    masked_query_wt = list(mt_sequence)\n",
    "                else:\n",
    "                    raise Exception('mode takes values mt and wt')\n",
    "\n",
    "                masked_query_wt[i] = mask_token\n",
    "                masked_sequence_wt = ''.join(masked_query_wt)\n",
    "                masked_log_prob_wt = self.get_log_prob(sequence=masked_sequence_wt)\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob_mt[i, idx_mt] - masked_log_prob_wt[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def pseudolikelihood(self, mt_sequence, mask_token = '_'):\n",
    "        score = 0\n",
    "        for i, aa_mt in enumerate(zip(mt_sequence)):\n",
    "\n",
    "            masked_query_mt = list(mt_sequence)\n",
    "            masked_query_mt[i] = mask_token\n",
    "            masked_sequence_mt = ''.join(masked_query_mt)\n",
    "            masked_log_prob_mt = self.get_log_prob(sequence=masked_sequence_mt)\n",
    "\n",
    "            idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "            score += masked_log_prob_mt[i, idx_mt]\n",
    "\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask, test_mask = get_split_mask(df, omit_zero=False)\n",
    "df_train = df[train_mask]\n",
    "df_val = df[val_mask]\n",
    "df_test = df[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 50, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 20,\n",
    "        'early_stopping': False,\n",
    "        'lr': 5e-4,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'use_seq_head': True,\n",
    "        'device': 'gpu'}\n",
    "\n",
    "surrogate = ESMCConFit(name='esmc_600m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pm in surrogate.model.named_parameters():\n",
    "    if pm.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_train, wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['epoch'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, wt_sequence, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## masked marginals\n",
    "y_pred = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    mt_sequence = row['seq']\n",
    "    score, n_muts = surrogate.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "\n",
    "    assert n_muts == row['n_mut']\n",
    "\n",
    "    y_pred.append(score)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y = df['fitness_log'].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > -1\n",
    "color_mask = df['n_mut'] > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_val_pred, y_val = y_pred[val_mask & omit_mask], y[val_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_mask = df['fitness_raw'] > 0.01\n",
    "color_mask = df['n_mut'] > 15\n",
    "# color_mask = n_muts_list > 15\n",
    "\n",
    "y_train_pred, y_train = y_pred[train_mask & omit_mask], y[train_mask & omit_mask]\n",
    "y_test_pred, y_test = y_pred[test_mask & omit_mask], y[test_mask & omit_mask]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.8)\n",
    "ax[0].plot(y[train_mask & omit_mask & color_mask], y_pred[train_mask & omit_mask & color_mask], '.', alpha=0.8, color='r')\n",
    "ax[1].plot(y_test, y_test_pred, '.', alpha=0.8)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_train, y_train_pred)\n",
    "ax[0].set_title(f'Train \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr, ci_lower, ci_upper, *_  = get_spearmanr_bootstrap(y_test, y_test_pred)\n",
    "ax[1].set_title(f'Test \\nspearman correlation = {corr} CI ({ci_lower}, {ci_upper})', size=10)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
