{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.utils.constants import *\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.al import top_model as topmodel\n",
    "from DomainPrediction.al.embeddings import one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nethome/kgeorge/workspace/DomainPrediction/Data/al_test_experiments/Tdomain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(data_path, 'dataset_tdomain.csv')\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_mask(df, omit_zero=False):\n",
    "    if omit_zero:\n",
    "        train_mask = (df['split_id'] == 2) & (df['fitness_raw'] != 0)\n",
    "    else:\n",
    "        train_mask = (df['split_id'] == 2)\n",
    "\n",
    "    val_mask = df['split_id'] == 1\n",
    "    test_mask = df['split_id'] == 0\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDatasetContrast(Dataset):\n",
    "    def __init__(self, df, wt):\n",
    "        self.seq, self.y = df['seq'].to_numpy(), df['fitness_raw'].to_numpy()\n",
    "        self.wt = np.array([wt]*self.seq.shape[0], dtype='object')\n",
    "        self.n_mut = df['n_mut'].to_numpy()\n",
    "\n",
    "        self.positions = []\n",
    "        for _, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            pos = []\n",
    "            for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt)):\n",
    "                if aa_wt != aa_mt:\n",
    "                    ## mutation pos\n",
    "                    pos.append(i)\n",
    "\n",
    "            assert len(pos) == row['n_mut']\n",
    "\n",
    "            self.positions.append(np.array(pos))\n",
    "\n",
    "        assert len(self.positions) == self.seq.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.y[idx], self.wt[idx], self.positions[idx], self.n_mut[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        seq = np.array([x[0] for x in data], dtype='object')\n",
    "        y = torch.tensor([x[1] for x in data])\n",
    "        wt = np.array([x[2] for x in data], dtype='object')\n",
    "        pos = [x[3] for x in data]\n",
    "        n_mut = np.array([x[4] for x in data])\n",
    "        return seq, y, wt, pos, n_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProteinFunDatasetContrast(df=df, wt=wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[20:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2ConFit(pl.LightningModule):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.basemodel, self.alphabet = esm.pretrained.load_model_and_alphabet(config['model_path'])\n",
    "        self.model_reg, _ = esm.pretrained.load_model_and_alphabet(config['model_path'])\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            bias='all'\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.basemodel, peft_config)\n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch, batch_tokens_masked, batch_tokens, batch_tokens_wt):\n",
    "        mt_seq, _, wt_seq, pos, n_mut = batch\n",
    "        \n",
    "        logits = self.model(batch_tokens_masked)['logits']\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        scores = torch.zeros(log_probs.shape[0])\n",
    "        if self.config['device'] == 'gpu':\n",
    "            scores = scores.cuda()\n",
    "\n",
    "        for i in range(log_probs.shape[0]):\n",
    "            scores[i] = torch.sum(log_probs[i, pos[i]+1, batch_tokens[i][pos[i]+1]] - log_probs[i, pos[i]+1, batch_tokens_wt[i][pos[i]+1]])\n",
    "        \n",
    "        return scores, logits\n",
    "    \n",
    "    def BT_loss(self, scores, y):\n",
    "        loss = torch.tensor(0.)\n",
    "        if self.config['device'] == 'gpu':\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(i, len(scores)):\n",
    "                if y[i] > y[j]:\n",
    "                    loss += torch.log(1 + torch.exp(scores[j]-scores[i]))\n",
    "                else:\n",
    "                    loss += torch.log(1 + torch.exp(scores[i]-scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        data = [\n",
    "            (f'P{i}', wt_i) for i, wt_i in enumerate(wt_seq)\n",
    "            ]\n",
    "        _, _, batch_tokens_wt = self.batch_converter(data)\n",
    "\n",
    "        data = [\n",
    "            (f'P{i}', s) for i, s in enumerate(mt_seq)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.alphabet.mask_idx\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        logits_reg = self.model_reg(batch_tokens_wt)['logits']\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        data = [\n",
    "            (f'P{i}', wt_i) for i, wt_i in enumerate(wt_seq)\n",
    "            ]\n",
    "        _, _, batch_tokens_wt = self.batch_converter(data)\n",
    "\n",
    "        data = [\n",
    "            (f'P{i}', s) for i, s in enumerate(mt_seq)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.alphabet.mask_idx\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        logits_reg = self.model_reg(batch_tokens_wt)['logits']\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "\n",
    "    def trainmodel(self, df, wt, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetContrast(df, wt)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetContrast(val, wt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=True)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"16-mixed\",\n",
    "                                # accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    def sanity_check(self, df, wt):\n",
    "        dataset = ProteinFunDatasetContrast(df, wt)\n",
    "        loader = DataLoader(dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        y_pred_1 = []\n",
    "        for batch in loader:\n",
    "            mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "            data = [\n",
    "                (f'P{i}', wt_i) for i, wt_i in enumerate(wt_seq)\n",
    "                ]\n",
    "            _, _, batch_tokens_wt = self.batch_converter(data)\n",
    "\n",
    "            data = [\n",
    "                (f'P{i}', s) for i, s in enumerate(mt_seq)\n",
    "                ]\n",
    "            batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
    "            batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.alphabet.mask_idx\n",
    "            \n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat, _ = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "            y_pred_1.append(y_hat.cpu().numpy())\n",
    "\n",
    "        y_pred_1 = np.concatenate(y_pred_1)\n",
    "\n",
    "        y_pred_2 = []\n",
    "        for i, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            score, n_muts = self.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "            assert n_muts == row['n_mut']\n",
    "\n",
    "            y_pred_2.append(score)\n",
    "\n",
    "        y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "        np.allclose(y_pred_1, y_pred_2, atol=1e-3)\n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        data = [\n",
    "            (\"protein1\", sequence)\n",
    "        ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch_tokens)['logits']\n",
    "\n",
    "        log_prob = torch.log_softmax(logits, dim=-1)[0,1:-1,:]\n",
    "\n",
    "        return log_prob.cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.alphabet.get_idx(aa_mt)\n",
    "                idx_wt = self.alphabet.get_idx(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '<mask>'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.alphabet.get_idx(aa_mt)\n",
    "                idx_wt = self.alphabet.get_idx(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'model_path': '/data/users/kgeorge/workspace/esm2/checkpoints/esm2_t33_650M_UR50D.pt',\n",
    "        'epoch': 20, \n",
    "        'batch_size': 8,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 20,\n",
    "        'early_stopping': False,\n",
    "        'lr': 1e-3,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'device': 'gpu'}\n",
    "surrogate = ESM2ConFit(config=config)\n",
    "surrogate.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['split_id'] == 2]\n",
    "df_val = df[df['split_id'] == 1]\n",
    "df_test = df[df['split_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_train, wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['epoch'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['lr'] = 5e-4\n",
    "surrogate.trainmodel(pd.concat([df_train, df_val]), wt_sequence, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_pred(df):\n",
    "    ## masked marginals\n",
    "    y_pred = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        mt_sequence = row['seq']\n",
    "        score, n_muts = surrogate.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "\n",
    "        assert n_muts == row['n_mut']\n",
    "\n",
    "        y_pred.append(score)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    y = df['fitness_log'].to_numpy().astype(np.float32)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "\n",
    "    ax[0].plot(y, y_pred, '.', alpha=0.5)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    corr = stats.spearmanr(y, y_pred)\n",
    "    s_corr = round(corr.statistic, 2)\n",
    "    ax[0].set_title(f'Full Dataset \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "    mask = ~(df['fitness_raw'] == 0)\n",
    "    ax[1].plot(y[mask], y_pred[mask], '.', alpha=0.5)\n",
    "    mse = mean_squared_error(y[mask], y_pred[mask])\n",
    "    corr = stats.spearmanr(y[mask], y_pred[mask])\n",
    "    s_corr = round(corr.statistic, 2)\n",
    "    ax[1].set_title(f'Omit fitness = 0 \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].set_xlabel('True')\n",
    "        ax[i].set_ylabel('Pred')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['fitness_log'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[df_val['fitness_log'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['fitness_log'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gxps_wt = helper.read_fasta('/nethome/kgeorge/workspace/DomainPrediction/Data/gxps/GxpS_ATC.fasta', mode='str')[0]\n",
    "A_domain_wt = ''.join([s for i, s in enumerate(gxps_wt) if i in A_gxps_atc])\n",
    "C_domain_wt = ''.join([s for i, s in enumerate(gxps_wt) if i in C_gxps_atc])\n",
    "TplusLinker_wt = ''.join([s for i, s in enumerate(gxps_wt) if i not in A_gxps_atc+C_gxps_atc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TplusLinker_wt == df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TplusLinker'] = df['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gxps_wt ==  A_domain_wt + TplusLinker_wt + C_domain_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq'] = df['seq'].apply(lambda x: A_domain_wt+x+C_domain_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gxps_wt == df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_sequence = df.loc[df['name'] == 'WT', 'seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProteinFunDatasetContrast(df=df, wt=wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../esm')\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMCConFit(pl.LightningModule):\n",
    "    def __init__(self, name, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if name == 'esmc_300m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 960\n",
    "        elif name == 'esmc_600m':\n",
    "            self.basemodel = ESMC.from_pretrained(name)\n",
    "            self.model_reg = ESMC.from_pretrained(name)\n",
    "            self.emb_dim = 1152\n",
    "        else:\n",
    "            raise Exception('Check ESMC name')\n",
    "        \n",
    "        for pm in self.model_reg.parameters():\n",
    "            pm.requires_grad = False\n",
    "        self.model_reg.eval()\n",
    "        \n",
    "        # peft_config = LoraConfig(\n",
    "        #     r=8,\n",
    "        #     lora_alpha=8,\n",
    "        #     lora_dropout=0.1,\n",
    "        #     target_modules=[\"out_proj\", \"layernorm_qkv.1\", \"ffn.3\"],\n",
    "        #     bias='all'\n",
    "        # )\n",
    "        \n",
    "        # self.model = get_peft_model(self.basemodel, peft_config)\n",
    "\n",
    "        # self.model = self.basemodel\n",
    "        # for name, pm in self.model.named_parameters():\n",
    "        #     if 'sequence_head' in name:\n",
    "        #         pm.requires_grad = True\n",
    "        #     else:\n",
    "        #         pm.requires_grad = False\n",
    "        \n",
    "        \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "            self.model_reg.cuda()\n",
    "\n",
    "        self.lambda_reg = config['lambda']\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, batch, batch_tokens_masked, batch_tokens, batch_tokens_wt):\n",
    "        mt_seq, _, wt_seq, pos, n_mut = batch\n",
    "        \n",
    "        output = self.model(batch_tokens_masked)\n",
    "        logits = output.sequence_logits\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        scores = torch.zeros(log_probs.shape[0])\n",
    "        if self.config['device'] == 'gpu':\n",
    "            scores = scores.cuda()\n",
    "\n",
    "        for i in range(log_probs.shape[0]):\n",
    "            scores[i] = torch.sum(log_probs[i, pos[i]+1, batch_tokens[i][pos[i]+1]] - log_probs[i, pos[i]+1, batch_tokens_wt[i][pos[i]+1]])\n",
    "        \n",
    "        return scores, logits\n",
    "    \n",
    "    def BT_loss(self, scores, y):\n",
    "        loss = torch.tensor(0.)\n",
    "        if self.config['device'] == 'gpu':\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(i, len(scores)):\n",
    "                if y[i] > y[j]:\n",
    "                    loss += torch.log(1 + torch.exp(scores[j]-scores[i]))\n",
    "                else:\n",
    "                    loss += torch.log(1 + torch.exp(scores[i]-scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        print(f'contrast loss: {bt_loss.item()} | reg loss: {l_reg.item()} | loss: {loss.item()}')\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "        batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "        batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            if len(pos[i]) > 0:\n",
    "                batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "        \n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "        y_hat, logits = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "        bt_loss = self.BT_loss(y_hat, y)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens_wt = batch_tokens_wt.cuda()\n",
    "\n",
    "        output = self.model_reg(batch_tokens_wt)\n",
    "        logits_reg = output.sequence_logits\n",
    "\n",
    "        creterion_reg = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "        l_reg = creterion_reg(probs_reg.log().cuda(), probs)\n",
    "\n",
    "        loss = bt_loss + self.lambda_reg*l_reg\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=y.shape[0])\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "\n",
    "    def trainmodel(self, df, wt, val=None, debug=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "        train_dataset = ProteinFunDatasetContrast(df, wt)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            val_dataset = ProteinFunDatasetContrast(val, wt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=True)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"bf16-mixed\",\n",
    "                                # accumulate_grad_batches=self.config['accumulate_batch_size']\n",
    "                                )\n",
    "        \n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    def sanity_check(self, df, wt):\n",
    "        dataset = ProteinFunDatasetContrast(df, wt)\n",
    "        loader = DataLoader(dataset, batch_size=self.config['batch_size'], collate_fn=ProteinFunDatasetContrast.collate_fn, shuffle=False)\n",
    "\n",
    "        y_pred_1 = []\n",
    "        for batch in loader:\n",
    "            mt_seq, y, wt_seq, pos, n_mut = batch\n",
    "            batch_tokens_wt = self.model._tokenize(wt_seq)\n",
    "            batch_tokens = self.model._tokenize(mt_seq)\n",
    "\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            for i in range(batch_tokens.shape[0]):\n",
    "                if len(pos[i]) > 0:\n",
    "                    batch_tokens_masked[i, pos[i]+1] = self.model.tokenizer.mask_token_id\n",
    "            \n",
    "            if self.config['device'] == 'gpu':\n",
    "                batch_tokens_masked = batch_tokens_masked.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat, _ = self(batch, batch_tokens_masked, batch_tokens, batch_tokens_wt)\n",
    "\n",
    "            y_pred_1.append(y_hat.cpu().numpy())\n",
    "\n",
    "        y_pred_1 = np.concatenate(y_pred_1)\n",
    "\n",
    "        y_pred_2 = []\n",
    "        for i, row in df.iterrows():\n",
    "            mt_sequence = row['seq']\n",
    "            score, n_muts = self.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "            assert n_muts == row['n_mut']\n",
    "\n",
    "            y_pred_2.append(score)\n",
    "\n",
    "        y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "        np.allclose(y_pred_1, y_pred_2, atol=1e-3)\n",
    "            \n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train loss: {np.mean(self.accumulate_batch_loss_train)} val loss: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def get_log_prob(self, sequence):\n",
    "        esm_protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        esm_tensor = self.model.encode(esm_protein)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = self.model.logits(\n",
    "                esm_tensor, LogitsConfig(sequence=True, return_embeddings=False)\n",
    "            )\n",
    "\n",
    "        logits = results.logits.sequence\n",
    "\n",
    "        log_prob = torch.log_softmax(logits[0, 1:-1, :33], dim=-1)\n",
    "\n",
    "        return log_prob.to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    def get_wildtype_marginal(self, mt_sequence, wt_sequence, wt_log_prob=None):\n",
    "        if wt_log_prob is None:\n",
    "            assert len(wt_sequence) == len(mt_sequence)\n",
    "            wt_log_prob = self.get_log_prob(sequence=wt_sequence)\n",
    "\n",
    "        assert wt_log_prob.shape[0] == len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        score = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += wt_log_prob[i, idx_mt] - wt_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def get_masked_marginal(self, mt_sequence, wt_sequence, mask_token = '_'):\n",
    "\n",
    "        assert len(wt_sequence) == len(mt_sequence)\n",
    "\n",
    "        n_muts = 0\n",
    "        mask_positions = []\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "                n_muts += 1\n",
    "                mask_positions.append(i)\n",
    "\n",
    "        assert len(mask_positions) == n_muts\n",
    "        masked_query = list(wt_sequence)\n",
    "        for _pos in mask_positions:\n",
    "            masked_query[_pos] = mask_token\n",
    "        masked_sequence = ''.join(masked_query)\n",
    "\n",
    "        masked_log_prob = self.get_log_prob(sequence=masked_sequence)\n",
    "        \n",
    "        score = 0\n",
    "        _idx = 0\n",
    "        for i, (aa_mt, aa_wt) in enumerate(zip(mt_sequence, wt_sequence)):\n",
    "            if aa_wt != aa_mt:\n",
    "                ## mutation pos\n",
    "\n",
    "                assert mask_positions[_idx] == i\n",
    "                _idx += 1\n",
    "\n",
    "                idx_mt = self.model.tokenizer.convert_tokens_to_ids(aa_mt)\n",
    "                idx_wt = self.model.tokenizer.convert_tokens_to_ids(aa_wt)\n",
    "                score += masked_log_prob[i, idx_mt] - masked_log_prob[i, idx_wt]\n",
    "\n",
    "\n",
    "        return score, n_muts\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'epoch': 10, \n",
    "        'batch_size': 4,\n",
    "        'lambda': 0.1,\n",
    "        'accumulate_batch_size': 32,\n",
    "        'patience': 20,\n",
    "        'early_stopping': False,\n",
    "        'lr': 1e-3,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'device': 'gpu'}\n",
    "surrogate = ESMCConFit(name='esmc_300m', config=config)\n",
    "surrogate.print_trainable_parameters(surrogate.model)\n",
    "surrogate.print_trainable_parameters(surrogate.basemodel)\n",
    "surrogate.print_trainable_parameters(surrogate.model_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['split_id'] == 2]\n",
    "df_val = df[df['split_id'] == 1]\n",
    "df_test = df[df['split_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.sanity_check(df_train, wt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.config['epoch'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.trainmodel(df_train, wt_sequence, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_pred(df):\n",
    "    ## masked marginals\n",
    "    y_pred = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        mt_sequence = row['seq']\n",
    "        score, n_muts = surrogate.get_masked_marginal(mt_sequence, wt_sequence)\n",
    "\n",
    "        assert n_muts == row['n_mut']\n",
    "\n",
    "        y_pred.append(score)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    y = df['fitness_log'].to_numpy().astype(np.float32)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(7,3), layout='constrained')\n",
    "\n",
    "    ax[0].plot(y, y_pred, '.', alpha=0.5)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    corr = stats.spearmanr(y, y_pred)\n",
    "    s_corr = round(corr.statistic, 2)\n",
    "    ax[0].set_title(f'Full Dataset \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "    mask = ~(df['fitness_raw'] == 0)\n",
    "    ax[1].plot(y[mask], y_pred[mask], '.', alpha=0.5)\n",
    "    mse = mean_squared_error(y[mask], y_pred[mask])\n",
    "    corr = stats.spearmanr(y[mask], y_pred[mask])\n",
    "    s_corr = round(corr.statistic, 2)\n",
    "    ax[1].set_title(f'Omit fitness = 0 \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].set_xlabel('True')\n",
    "        ax[i].set_ylabel('Pred')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_pred(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
