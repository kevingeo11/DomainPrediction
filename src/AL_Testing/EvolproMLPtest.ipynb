{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.esm.esm2 import ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Set a lower value for log_every_n_steps*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2 = ESM2(model_path='/data/users/kgeorge/workspace/esm2/checkpoints/esm2_t33_650M_UR50D.pt', device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_mean(sequences):\n",
    "    embeddings = []\n",
    "    for seq in tqdm(sequences):\n",
    "        rep = esm2.get_res(sequence=seq)\n",
    "        embeddings.append(rep['representations'][33][:,1:-1,:].mean(1).cpu().numpy())\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_flatten(sequences):\n",
    "    embeddings = []\n",
    "    for seq in tqdm(sequences):\n",
    "        rep = esm2.get_res(sequence=seq)\n",
    "        embeddings.append(rep['representations'][33][:,1:-1,:].cpu().numpy()[0].flatten())\n",
    "\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_mean_batch(sequences, batch_size=3):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        seqs = sequences[i:i+batch_size]\n",
    "        rep, batch_lens = esm2.get_res_batch(sequences=seqs)\n",
    "        assert (batch_lens == batch_lens[0]).all() == True\n",
    "        embeddings.append(rep['representations'][33][:,1:-1,:].mean(1).cpu().numpy())\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_full(sequences):\n",
    "    embeddings = []\n",
    "    for seq in tqdm(sequences):\n",
    "        rep = esm2.get_res(sequence=seq)\n",
    "        embeddings.append(rep['representations'][33][:,1:-1,:].cpu().numpy()[0])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def one_hot_encode(sequences: list[str]) -> np.ndarray:\n",
    "    \"\"\"Encode a protein sequence as a one-hot array.\"\"\"\n",
    "    embeddings = []\n",
    "    for seq in tqdm(sequences):\n",
    "        amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "        one_hot = np.zeros((len(seq), len(amino_acids)))\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in amino_acids:\n",
    "                one_hot[i, aa_to_index[aa]] = 1\n",
    "    \n",
    "        embeddings.append(one_hot.flatten())  \n",
    "\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFSurrogate():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.model = RandomForestRegressor(n_estimators=100, criterion='friedman_mse', max_depth=None, min_samples_split=2,\n",
    "                                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0,\n",
    "                                            max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False,\n",
    "                                            n_jobs=None, random_state=1, verbose=0, warm_start=False, ccp_alpha=0.0,\n",
    "                                            max_samples=None)\n",
    "    \n",
    "    def trainmodel(self, X, y, val=None, debug=True):\n",
    "        '''\n",
    "            X - embeddings from esm2\n",
    "            X - shape (n, features)\n",
    "            y - shape (n, )\n",
    "        '''\n",
    "        _ = self.model.fit(X, y)\n",
    "        if debug:\n",
    "            self.print_eval(X, y, label='train')\n",
    "            if val is not None:\n",
    "                X_val, y_val = val\n",
    "                self.print_eval(X_val, y_val, label='val')\n",
    "\n",
    "    \n",
    "    def print_eval(self, X, y, label='set'):\n",
    "        ypred = self.model.predict(X)\n",
    "        mse = mean_squared_error(ypred, y)\n",
    "        corr = stats.spearmanr(ypred, y)\n",
    "\n",
    "        print(f'{label}: mse = {mse}, spearman correlation = {corr.statistic}')\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeSurrogate():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.model = Ridge(alpha=1.0, fit_intercept=True, random_state=1)\n",
    "    \n",
    "    def trainmodel(self, X, y, val=None, debug=True):\n",
    "        '''\n",
    "            X - embeddings from esm2\n",
    "            X - shape (n, features)\n",
    "            y - shape (n, )\n",
    "        '''\n",
    "        _ = self.model.fit(X, y)\n",
    "        if debug:\n",
    "            self.print_eval(X, y, label='train')\n",
    "            if val is not None:\n",
    "                X_val, y_val = val\n",
    "                self.print_eval(X_val, y_val, label='val')\n",
    "    \n",
    "    def print_eval(self, X, y, label='set'):\n",
    "        ypred = self.model.predict(X)\n",
    "        mse = mean_squared_error(ypred, y)\n",
    "        corr = stats.spearmanr(ypred, y)\n",
    "\n",
    "        print(f'{label}: mse = {mse}, spearman correlation = {corr.statistic}')\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class MLPSurrogate(pl.LightningModule):\n",
    "    def __init__(self, config={'layers': [1280, 2048, 1280, 1], \n",
    "                               'epoch': 10, \n",
    "                               'batch_size': 16,\n",
    "                               'patience': 10,\n",
    "                               'lr': 1e-3,\n",
    "                               'early_stopping': True}\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        layers = []\n",
    "        for i in range(1, len(config['layers'])-1):\n",
    "            layers.append(nn.Linear(config['layers'][i-1], config['layers'][i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(config['layers'][-2], config['layers'][-1]))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "    \n",
    "\n",
    "    def trainmodel(self, X, y, val=None, debug=True):\n",
    "        '''\n",
    "            X - embeddings from esm2\n",
    "            X - shape (n, features)\n",
    "            y - shape (n, )\n",
    "        '''\n",
    "        self.debug = debug\n",
    "        \n",
    "        train_dataset = ProteinFunDataset(X, y)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            X_val, y_val = val\n",
    "            val_dataset = ProteinFunDataset(X_val, y_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], shuffle=False)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
    "        \n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True\n",
    "                                )\n",
    "\n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        ## Needs to change - we need to load the least val loss model\n",
    "        if val is not None:\n",
    "            y_pred = self.predict(X_val)\n",
    "            val_mse = mean_squared_error(y_pred, y_val)\n",
    "            print(f'Train end val mse: {val_mse}')\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "            X is numpy array\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            y = self(torch.tensor(X))\n",
    "        return y.numpy().flatten()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinFunDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class ESM2Regression(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.esm2, self.alphabet = esm.pretrained.load_model_and_alphabet(config['model_path'])\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        self.tok_to_idx = self.alphabet.tok_to_idx\n",
    "        self.idx_to_tok = {v:k for k,v in self.tok_to_idx.items()}\n",
    "\n",
    "        layers = []\n",
    "        for i in range(1, len(config['layers'])-1):\n",
    "            layers.append(nn.Linear(config['layers'][i-1], config['layers'][i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(config['layers'][-2], config['layers'][-1]))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, batch_tokens):\n",
    "        rep = self.esm2(batch_tokens, repr_layers=[30], return_contacts=True)\n",
    "        embedding = rep['representations'][30].mean(1)\n",
    "\n",
    "        pred = self.mlp(embedding)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class ESM2loraMLPSurrogate(pl.LightningModule):\n",
    "    def __init__(self, config={'layers': [1280, 2048, 1280, 1], \n",
    "                               'epoch': 10, \n",
    "                               'batch_size': 16,\n",
    "                               'patience': 10,\n",
    "                               'lr': 1e-3,\n",
    "                               'early_stopping': True}\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        regressor = ESM2Regression(config)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=4, \n",
    "            lora_alpha=1,\n",
    "            target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"all\",\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(regressor, lora_config)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'mlp' in name:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        if config['device'] == 'gpu':\n",
    "            self.model.cuda()\n",
    "\n",
    "        self.accumulate_batch_loss_train = []\n",
    "        self.accumulate_batch_loss_val = []\n",
    "        self.debug=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        data = [\n",
    "            (fun, seq) for (seq, fun) in zip(x, y)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.model.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat = self(batch_tokens)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.accumulate_batch_loss_train.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        data = [\n",
    "            (fun, seq) for (seq, fun) in zip(x, y)\n",
    "            ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.model.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        if self.config['device'] == 'gpu':\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "\n",
    "        y_hat = self(batch_tokens)\n",
    "        loss = nn.functional.mse_loss(y_hat.flatten(), y)\n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.accumulate_batch_loss_val.append(loss.item())\n",
    "    \n",
    "\n",
    "    def trainmodel(self, X, y, val=None, debug=True):\n",
    "        '''\n",
    "            X - embeddings from esm2\n",
    "            X - shape (n, features)\n",
    "            y - shape (n, )\n",
    "        '''\n",
    "        self.debug = debug\n",
    "        \n",
    "        train_dataset = ProteinFunDataset(X, y)\n",
    "\n",
    "        val_loader = None\n",
    "        if val is not None:\n",
    "            X_val, y_val = val\n",
    "            val_dataset = ProteinFunDataset(X_val, y_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "        \n",
    "        callbacks = None\n",
    "        if self.config['early_stopping']:\n",
    "            callbacks = []\n",
    "            earlystopping_callback = EarlyStopping(monitor=\"val/loss\", patience=self.config['patience'], verbose=False, mode=\"min\")\n",
    "            callbacks.append(earlystopping_callback)\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=self.config['epoch'], callbacks=callbacks,\n",
    "                                accelerator=\"auto\",\n",
    "                                enable_progress_bar=False,\n",
    "                                enable_model_summary=True,\n",
    "                                precision=\"16-mixed\",\n",
    "                                accumulate_grad_batches=self.config['batch_size']\n",
    "                                )\n",
    "\n",
    "        trainer.fit(model=self, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        ## Needs to change - we need to load the least val loss model\n",
    "        if val is not None:\n",
    "            y_pred = self.predict(X_val)\n",
    "            val_mse = mean_squared_error(y_pred, y_val)\n",
    "            print(f'Train end val mse: {val_mse}')\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.accumulate_batch_loss_train.clear()\n",
    "        self.accumulate_batch_loss_val.clear()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch % self.config['print_every_n_epoch'] == 0 and self.debug:\n",
    "            print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f'Epoch: {self.current_epoch}: train mse: {np.mean(self.accumulate_batch_loss_train)} val mse: {np.mean(self.accumulate_batch_loss_val)}')\n",
    "\n",
    "    def predict(self, X, setting='cpu'):\n",
    "        '''\n",
    "            X is list seq\n",
    "        '''\n",
    "        data = [\n",
    "                    (f'P{i}', seq) for i, seq in enumerate(X)\n",
    "                ]\n",
    "        batch_labels, batch_strs, batch_tokens = self.model.batch_converter(data)\n",
    "        batch_lens = (batch_tokens != self.model.alphabet.padding_idx).sum(1)\n",
    "\n",
    "        if setting=='cpu':\n",
    "            assert next(self.parameters()).is_cuda == False\n",
    "            pred = []\n",
    "            for i in range(0, batch_tokens.shape[0], 10):\n",
    "                with torch.no_grad():\n",
    "                    y_pred = self(batch_tokens[i:i+10])\n",
    "                    pred.append(y_pred.cpu().numpy().flatten())\n",
    "\n",
    "            pred = np.concatenate(pred)\n",
    "        else:\n",
    "            raise Exception('Not working')\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../..'\n",
    "data_path = os.path.join(root, 'Data/al_test_experiments/Evolvepro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path, 'brenan.csv')\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['seq'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESM2 mean-pooled embeddings\n",
    "\n",
    "# embeddings = get_embeddings_mean(df['seq'])\n",
    "# file_name = os.path.join(data_path, 'brenan_embeddings.npy')\n",
    "# np.save(file_name, embeddings)\n",
    "\n",
    "file_name = os.path.join(data_path, 'brenan_embeddings.npy')\n",
    "embeddings = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESM2 flattened/concateneted embeddings\n",
    "\n",
    "# embeddings = get_embeddings_flatten(df['seq'])\n",
    "# file_name = os.path.join(data_path, 'brenan_embeddings_concat.npy')\n",
    "# np.save(file_name, embeddings)\n",
    "\n",
    "# file_name = os.path.join(data_path, 'brenan_embeddings_concat.npy')\n",
    "# embeddings = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = one_hot_encode(df['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "num_seq_in_pos = 19\n",
    "block_size = 10\n",
    "num_blocks = int(df.shape[0]*test_split // (num_seq_in_pos * block_size) + 1)\n",
    "positions = df['pos'].unique()\n",
    "step_size = len(positions) // num_blocks\n",
    "block_indices = [i for i in range(0, len(positions) - block_size + 1, step_size)][:num_blocks]\n",
    "blocks = [positions[i:i + block_size] for i in block_indices]\n",
    "\n",
    "for _block in blocks:\n",
    "    assert len(_block) == 10\n",
    "    for i in _block:\n",
    "        assert i in positions\n",
    "\n",
    "test_positions = np.concatenate(blocks)\n",
    "test_indices = np.array(df[df['pos'].isin(test_positions)].index)\n",
    "\n",
    "val_split = 0.1\n",
    "n_pos_val = int((~df['pos'].isin(test_positions)).sum()*val_split // num_seq_in_pos + 1)\n",
    "val_positions = np.random.choice(df.loc[~df['pos'].isin(test_positions), 'pos'].unique(), n_pos_val, replace=False)\n",
    "for i in val_positions:\n",
    "    assert i not in test_positions\n",
    "val_indices = np.array(df[df['pos'].isin(val_positions)].index)\n",
    "train_indices = np.array(df[~df['pos'].isin(np.concatenate([val_positions, test_positions]))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = np.array(df.index)\n",
    "# np.random.shuffle(index)\n",
    "# train_indices = index[:-int(index.shape[0]*0.2)-int(index.shape[0]*0.1)]\n",
    "# val_indices = index[-int(index.shape[0]*0.2)-int(index.shape[0]*0.1):-int(index.shape[0]*0.2)]\n",
    "# test_indices = index[-int(index.shape[0]*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total size: {df.shape[0]}')\n",
    "print(f'train size: {train_indices.shape[0]} ({round(train_indices.shape[0]*100/df.shape[0], 2)}%)')\n",
    "print(f'val size  : {val_indices.shape[0]} ({round(val_indices.shape[0]*100/df.shape[0], 2)}%)')\n",
    "print(f'test size : {test_indices.shape[0]} ({round(test_indices.shape[0]*100/df.shape[0], 2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = False\n",
    "if scaled:\n",
    "    property_label = 'function_scaled'\n",
    "else:\n",
    "    property_label = 'function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings\n",
    "y = df[property_label].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = RidgeSurrogate()\n",
    "surrogate.train(X=X_train, y=y_train, val=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = surrogate.predict(X_train)\n",
    "y_val_pred = surrogate.predict(X_val)\n",
    "y_test_pred = surrogate.predict(X_test)\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.5)\n",
    "ax[1].plot(y_val, y_val_pred, '.', alpha=0.5)\n",
    "ax[2].plot(y_test, y_test_pred, '.', alpha=0.5)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr = stats.spearmanr(y_train, y_train_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[0].set_title(f'Train \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "corr = stats.spearmanr(y_val, y_val_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[1].set_title(f'Val \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr = stats.spearmanr(y_test, y_test_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[2].set_title(f'Test \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = RFSurrogate()\n",
    "surrogate.train(X=X_train, y=y_train, val=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = surrogate.predict(X_train)\n",
    "y_val_pred = surrogate.predict(X_val)\n",
    "y_test_pred = surrogate.predict(X_test)\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.5)\n",
    "ax[1].plot(y_val, y_val_pred, '.', alpha=0.5)\n",
    "ax[2].plot(y_test, y_test_pred, '.', alpha=0.5)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr = stats.spearmanr(y_train, y_train_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[0].set_title(f'Train \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "corr = stats.spearmanr(y_val, y_val_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[1].set_title(f'Val \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr = stats.spearmanr(y_test, y_test_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[2].set_title(f'Test \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'layers': [460800, 1], \n",
    "        'epoch': 100, \n",
    "        'batch_size': 16,\n",
    "        'patience': 10,\n",
    "        'early_stopping': False,\n",
    "        'lr': 1e-3,\n",
    "        'print_every_n_epoch': 10}\n",
    "surrogate = MLPSurrogate(config=config)\n",
    "surrogate.trainmodel(X=X_train, y=y_train, val=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, setting='cpu'):\n",
    "    '''\n",
    "        X is list seq\n",
    "    '''\n",
    "\n",
    "    if setting=='cpu':\n",
    "        assert next(model.parameters()).is_cuda == False\n",
    "        pred = []\n",
    "        for i in range(0, X.shape[0], 10):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(torch.tensor(X[i:i+10]))\n",
    "                pred.append(y_pred.cpu().numpy().flatten())\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "    else:\n",
    "        raise Exception('Not working')\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = surrogate.predict(X_train.astype(np.float32))\n",
    "y_val_pred = surrogate.predict(X_val.astype(np.float32))\n",
    "y_test_pred = surrogate.predict(X_test.astype(np.float32))\n",
    "# y_train_pred = predict(surrogate, X_train)\n",
    "# y_val_pred = predict(surrogate, X_val)\n",
    "# y_test_pred = predict(surrogate, X_test)\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,3), layout='constrained')\n",
    "ax[0].plot(y_train, y_train_pred, '.', alpha=0.3)\n",
    "ax[1].plot(y_val, y_val_pred, '.', alpha=0.5)\n",
    "ax[2].plot(y_test, y_test_pred, '.', alpha=0.5)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "corr = stats.spearmanr(y_train, y_train_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[0].set_title(f'Train \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "corr = stats.spearmanr(y_val, y_val_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[1].set_title(f'Val \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "corr = stats.spearmanr(y_test, y_test_pred)\n",
    "s_corr = round(corr.statistic, 2)\n",
    "ax[2].set_title(f'Test \\nmse : {str(round(mse, 2))} \\nspearman correlation = {s_corr}')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].set_ylabel('Pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'model_path': '/data/users/kgeorge/workspace/esm2/checkpoints/esm2_t30_150M_UR50D.pt',\n",
    "        'layers': [640, 1280, 640, 1], \n",
    "        'epoch': 50, \n",
    "        'batch_size': 16,\n",
    "        'patience': 10,\n",
    "        'early_stopping': False,\n",
    "        'lr': 1e-3,\n",
    "        'print_every_n_epoch': 1,\n",
    "        'device': 'gpu'}\n",
    "surrogate = ESM2loraMLPSurrogate(config=config)\n",
    "surrogate.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['seq'].to_numpy()[train_indices]\n",
    "X_val = df['seq'].to_numpy()[val_indices]\n",
    "X_test = df['seq'].to_numpy()[test_indices]\n",
    "surrogate.trainmodel(X=X_train, y=y_train, val=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
