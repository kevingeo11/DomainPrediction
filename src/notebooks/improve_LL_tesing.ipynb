{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "sys.path.append('../../esm') ## ignore if intsalling esm3\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:02:00.566887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 11:02:00.583236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 11:02:00.583260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 11:02:00.594028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 11:02:02.573353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig, SamplingConfig, LogitsConfig, SamplingTrackConfig\n",
    "from esm.utils.structure.protein_chain import ProteinChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainPrediction.protein.base import BaseProtein\n",
    "from DomainPrediction.eval import metrics\n",
    "from DomainPrediction.utils import helper\n",
    "from DomainPrediction.utils.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../Data/round_2_exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = BaseProtein('../../Data/gxps/gxps_ATC_hm_6mfy.pdb')\n",
    "Tdomain = protein.get_residues(T_gxps_atc)\n",
    "\n",
    "sequence_prompt = Tdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(sequence):\n",
    "    log_prob = 0\n",
    "    for pos in range(len(sequence)):\n",
    "        aa = sequence[pos]\n",
    "        tokens = model.tokenizers.sequence.encode(aa)\n",
    "        assert len(tokens) == 3\n",
    "        token = tokens[1]\n",
    "        assert model.tokenizers.sequence.decode(token) == aa\n",
    "\n",
    "        sequence_prompt = sequence[:pos] + '_' + sequence[pos+1:]\n",
    "        esm_protein = ESMProtein(sequence=sequence_prompt)\n",
    "        protein_tensor = model.encode(esm_protein)\n",
    "        res = model.logits(protein_tensor, LogitsConfig(\n",
    "            sequence = True\n",
    "        ))\n",
    "\n",
    "        logits = res.logits.sequence[0, 1:-1, :][pos, :].cpu()\n",
    "        prob = torch.nn.functional.softmax(logits, dim=0)\n",
    "        log_prob += np.log(prob[token].numpy())\n",
    "\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT T domain LL: -97.99259356735274\n"
     ]
    }
   ],
   "source": [
    "likelihood_db[Tdomain] = calculate_likelihood(Tdomain)\n",
    "print(f'WT T domain LL: {likelihood_db[Tdomain]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT T domain LL: -97.99259356735274\n",
      "0 G S tensor(0.0756) tensor(0.3232) -97.48324622167274\n",
      "mutation at pos 0 from G to S improved ll from -97.99259356735274 to -97.48324622167274\n",
      "1 E P tensor(0.2288) tensor(0.3277) -97.85116550745443\n",
      "2 I R tensor(0.0601) tensor(0.2433) -96.41527913091704\n",
      "mutation at pos 2 from I to R improved ll from -97.48324622167274 to -96.41527913091704\n",
      "3 E E tensor(0.9781) tensor(0.9781) -97.99259356735274\n",
      "4 I E tensor(0.0192) tensor(0.1820) -96.87033583736047\n",
      "5 A R tensor(0.1811) tensor(0.2254) -98.73617736296728\n",
      "6 L L tensor(0.5861) tensor(0.5861) -97.99259356735274\n",
      "7 A A tensor(0.6852) tensor(0.6852) -97.99259356735274\n",
      "8 T E tensor(0.0581) tensor(0.1975) -97.12931999890134\n",
      "9 I I tensor(0.4891) tensor(0.4891) -97.99259356735274\n",
      "10 W W tensor(0.9569) tensor(0.9569) -97.99259356735274\n",
      "11 R Q tensor(0.1927) tensor(0.2084) -98.2137359092012\n",
      "12 E E tensor(0.6387) tensor(0.6387) -97.99259356735274\n",
      "13 L L tensor(0.7949) tensor(0.7949) -97.99259356735274\n",
      "14 L L tensor(0.9756) tensor(0.9756) -97.99259356735274\n",
      "15 N G tensor(0.0464) tensor(0.4254) -95.6404971042648\n",
      "mutation at pos 15 from N to G improved ll from -96.41527913091704 to -95.6404971042648\n",
      "16 V V tensor(0.3550) tensor(0.3550) -97.99259356735274\n",
      "17 E E tensor(0.2951) tensor(0.2951) -97.99259356735274\n",
      "18 Q R tensor(0.1277) tensor(0.4562) -96.44373937696218\n",
      "19 V V tensor(0.6865) tensor(0.6865) -97.99259356735274\n",
      "20 G G tensor(0.8767) tensor(0.8767) -97.99259356735274\n",
      "21 R I tensor(0.2184) tensor(0.2635) -97.6432581320405\n",
      "22 H D tensor(0.1686) tensor(0.3682) -97.3432875345461\n",
      "23 D D tensor(0.9335) tensor(0.9335) -97.99259356735274\n",
      "24 S N tensor(0.0982) tensor(0.5145) -96.96106261899695\n",
      "25 F F tensor(0.9937) tensor(0.9937) -97.99259356735274\n",
      "26 F F tensor(0.9586) tensor(0.9586) -97.99259356735274\n",
      "27 A E tensor(0.1279) tensor(0.3478) -97.71699231211096\n",
      "28 L L tensor(0.9215) tensor(0.9215) -97.99259356735274\n",
      "29 G G tensor(0.9953) tensor(0.9953) -97.99259356735274\n",
      "30 G G tensor(0.9941) tensor(0.9941) -97.99259356735274\n",
      "31 H H tensor(0.6820) tensor(0.6820) -97.99259356735274\n",
      "32 S S tensor(0.9950) tensor(0.9950) -97.99259356735274\n",
      "33 L L tensor(0.6559) tensor(0.6559) -97.99259356735274\n",
      "34 L L tensor(0.7800) tensor(0.7800) -97.99259356735274\n",
      "35 A A tensor(0.8184) tensor(0.8184) -97.99259356735274\n",
      "36 V I tensor(0.2049) tensor(0.2251) -98.25919993687421\n",
      "37 R Q tensor(0.2137) tensor(0.3870) -97.8673052159138\n",
      "38 M L tensor(0.1193) tensor(0.4870) -97.77040594955906\n",
      "39 I V tensor(0.2336) tensor(0.4100) -97.5092250215821\n",
      "40 E A tensor(0.0510) tensor(0.3429) -95.80992931989022\n",
      "41 R R tensor(0.6793) tensor(0.6793) -97.99259356735274\n",
      "42 L A tensor(0.0742) tensor(0.6513) -94.23718828707933\n",
      "mutation at pos 42 from L to A improved ll from -95.6404971042648 to -94.23718828707933\n",
      "43 R R tensor(0.3396) tensor(0.3396) -97.99259356735274\n",
      "44 R E tensor(0.0801) tensor(0.1805) -97.32317752530798\n",
      "45 I Q tensor(0.0290) tensor(0.1645) -95.66111529502086\n",
      "46 G G tensor(0.8550) tensor(0.8550) -97.99259356735274\n",
      "47 L Y tensor(0.1402) tensor(0.1731) -96.42591193597764\n",
      "48 G S tensor(0.0344) tensor(0.1492) -97.07947013294324\n",
      "49 L T tensor(0.1256) tensor(0.3387) -94.47579491138458\n",
      "50 S S tensor(0.3002) tensor(0.3002) -97.99259356735274\n",
      "51 V L tensor(0.2445) tensor(0.2815) -98.01236218446866\n",
      "52 Q R tensor(0.0644) tensor(0.2736) -97.0147271733731\n",
      "53 T Q tensor(0.0796) tensor(0.3053) -95.86973652476445\n",
      "54 L L tensor(0.6143) tensor(0.6143) -97.99259356735274\n",
      "55 F F tensor(0.8340) tensor(0.8340) -97.99259356735274\n",
      "56 Q E tensor(0.1123) tensor(0.3005) -97.70089510036632\n",
      "57 H A tensor(0.1474) tensor(0.2487) -97.91327217267826\n",
      "58 P P tensor(0.6494) tensor(0.6494) -97.99259356735274\n",
      "59 T V tensor(0.2844) tensor(0.3378) -95.48911119857803\n",
      "60 L L tensor(0.4265) tensor(0.4265) -97.99259356735274\n",
      "61 S A tensor(0.0992) tensor(0.3925) -96.31125975819305\n",
      "62 V A tensor(0.0075) tensor(0.2691) -95.074581598863\n",
      "63 L L tensor(0.8962) tensor(0.8962) -97.99259356735274\n",
      "64 A A tensor(0.8359) tensor(0.8359) -97.99259356735274\n",
      "65 Q A tensor(0.0961) tensor(0.2847) -97.35282676573843\n",
      "66 S A tensor(0.0755) tensor(0.2516) -97.09470473369583\n",
      "67 L L tensor(0.4681) tensor(0.4681) -97.99259356735274\n",
      "68 V R tensor(0.0524) tensor(0.1794) -96.66155503317714\n",
      "69 P X tensor(0.0592) tensor(0.1418) -97.6456731078215\n"
     ]
    }
   ],
   "source": [
    "max_LL = likelihood_db[Tdomain]\n",
    "print(f'WT T domain LL: {likelihood_db[Tdomain]}')\n",
    "for pos in range(len(Tdomain)):\n",
    "    aa_wt = Tdomain[pos]\n",
    "    token_wt = model.tokenizers.sequence.encode(aa_wt)[1]\n",
    "    assert model.tokenizers.sequence.decode(token_wt) == aa_wt\n",
    "    \n",
    "    sequence_prompt = Tdomain[:pos] + '_' + Tdomain[pos+1:]\n",
    "    esm_protein = ESMProtein(sequence=sequence_prompt)\n",
    "    protein_tensor = model.encode(esm_protein)\n",
    "    res = model.logits(protein_tensor, LogitsConfig(\n",
    "        sequence = True\n",
    "    ))\n",
    "\n",
    "    assert res.logits.sequence.shape[1] == len(sequence_prompt) + 2\n",
    "\n",
    "    logits = res.logits.sequence[0, 1:-1, :][pos, :].cpu()\n",
    "    softmax_prob = torch.nn.functional.softmax(logits, dim=0)\n",
    "    \n",
    "    token_mt = torch.argmax(logits)\n",
    "    token_mt_prob = torch.argmax(softmax_prob)\n",
    "    assert token_mt == token_mt_prob\n",
    "    \n",
    "    aa_mt = model.tokenizers.sequence.decode(token_mt)\n",
    "    aa_mt_prob = model.tokenizers.sequence.decode(token_mt_prob)\n",
    "    assert aa_mt == aa_mt_prob\n",
    "    \n",
    "    gen_seq = Tdomain[:pos] + aa_mt + Tdomain[pos+1:]\n",
    "\n",
    "    if gen_seq not in likelihood_db:\n",
    "        likelihood_db[gen_seq] = calculate_likelihood(gen_seq)\n",
    "    \n",
    "    ll = likelihood_db[gen_seq]\n",
    "\n",
    "    print(pos, aa_wt, aa_mt, softmax_prob[token_wt], softmax_prob[token_mt], ll)\n",
    "\n",
    "    if ll > max_LL:\n",
    "        print(f'mutation at pos {pos} from {aa_wt} to {aa_mt} improved ll from {max_LL} to {ll}')\n",
    "        max_LL = ll\n",
    "        best_seq = gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEIEIALATIWRELLNVEQVGRHDSFFALGGHSLLAVRMIERLRRIGLGLSVQTLFQHPTLSVLAQSLVP'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEIEIALATIWRELLNVEQVGRHDSFFALGGHSLLAVRMIERARRIGLGLSVQTLFQHPTLSVLAQSLVP'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev best T domain LL: -94.23718828707933\n",
      "0 G S tensor(0.0759) tensor(0.3298) -93.02658353932202\n",
      "mutation at pos 0 from G to S improved ll from -94.23718828707933 to -93.02658353932202\n",
      "1 E P tensor(0.2320) tensor(0.3323) -93.83913527755067\n",
      "2 I R tensor(0.0376) tensor(0.2424) -91.05985924089327\n",
      "mutation at pos 2 from I to R improved ll from -93.02658353932202 to -91.05985924089327\n",
      "3 E E tensor(0.9822) tensor(0.9822) -94.23718828707933\n",
      "4 I E tensor(0.0184) tensor(0.1843) -92.84958053473383\n",
      "5 A A tensor(0.2912) tensor(0.2912) -94.23718828707933\n",
      "6 L L tensor(0.9472) tensor(0.9472) -94.23718828707933\n",
      "7 A A tensor(0.7608) tensor(0.7608) -94.23718828707933\n",
      "8 T E tensor(0.0527) tensor(0.2003) -93.39326152484864\n",
      "9 I I tensor(0.7037) tensor(0.7037) -94.23718828707933\n",
      "10 W W tensor(0.9525) tensor(0.9525) -94.23718828707933\n",
      "11 R Q tensor(0.1959) tensor(0.2022) -94.38500431925058\n",
      "12 E E tensor(0.6471) tensor(0.6471) -94.23718828707933\n",
      "13 L L tensor(0.7582) tensor(0.7582) -94.23718828707933\n",
      "14 L L tensor(0.9726) tensor(0.9726) -94.23718828707933\n",
      "15 N G tensor(0.0453) tensor(0.4301) -91.70578906079754\n",
      "16 V V tensor(0.3671) tensor(0.3671) -94.23718828707933\n",
      "17 E E tensor(0.2932) tensor(0.2932) -94.23718828707933\n",
      "18 Q R tensor(0.1234) tensor(0.4767) -92.62415886251256\n",
      "19 V V tensor(0.6863) tensor(0.6863) -94.23718828707933\n",
      "20 G G tensor(0.8841) tensor(0.8841) -94.23718828707933\n",
      "21 R I tensor(0.2171) tensor(0.2702) -94.10256680333987\n",
      "22 H D tensor(0.1709) tensor(0.3617) -92.97810895042494\n",
      "23 D D tensor(0.9406) tensor(0.9406) -94.23718828707933\n",
      "24 S N tensor(0.0982) tensor(0.5227) -93.38862632215023\n",
      "25 F F tensor(0.9946) tensor(0.9946) -94.23718828707933\n",
      "26 F F tensor(0.9551) tensor(0.9551) -94.23718828707933\n",
      "27 A E tensor(0.1311) tensor(0.3564) -93.73664101585746\n",
      "28 L L tensor(0.9209) tensor(0.9209) -94.23718828707933\n",
      "29 G G tensor(0.9959) tensor(0.9959) -94.23718828707933\n",
      "30 G G tensor(0.9942) tensor(0.9942) -94.23718828707933\n",
      "31 H H tensor(0.6571) tensor(0.6571) -94.23718828707933\n",
      "32 S S tensor(0.9955) tensor(0.9955) -94.23718828707933\n",
      "33 L L tensor(0.6746) tensor(0.6746) -94.23718828707933\n",
      "34 L L tensor(0.8039) tensor(0.8039) -94.23718828707933\n",
      "35 A A tensor(0.8229) tensor(0.8229) -94.23718828707933\n",
      "36 V I tensor(0.2036) tensor(0.2344) -94.47534276824445\n",
      "37 R Q tensor(0.1895) tensor(0.4547) -93.40686082094908\n",
      "38 M L tensor(0.0693) tensor(0.5285) -93.18308418663219\n",
      "39 I V tensor(0.1869) tensor(0.5668) -92.76421440811828\n",
      "40 E A tensor(0.0680) tensor(0.3822) -92.67728571151383\n",
      "41 R R tensor(0.6451) tensor(0.6451) -94.23718828707933\n",
      "42 A A tensor(0.6513) tensor(0.6513) -94.23718828707933\n",
      "43 R R tensor(0.4490) tensor(0.4490) -94.23718828707933\n",
      "44 R A tensor(0.1014) tensor(0.2064) -92.9978211489506\n",
      "45 I A tensor(0.0311) tensor(0.3226) -91.71119484840892\n",
      "46 G G tensor(0.9202) tensor(0.9202) -94.23718828707933\n",
      "47 L L tensor(0.4307) tensor(0.4307) -94.23718828707933\n",
      "48 G S tensor(0.0332) tensor(0.1203) -93.57535040099174\n",
      "49 L F tensor(0.2806) tensor(0.5242) -91.52389757195488\n",
      "50 S S tensor(0.3074) tensor(0.3074) -94.23718828707933\n",
      "51 V V tensor(0.2406) tensor(0.2406) -94.23718828707933\n",
      "52 Q R tensor(0.0606) tensor(0.2960) -93.33621981763281\n",
      "53 T Q tensor(0.0538) tensor(0.2906) -92.08550457749516\n",
      "54 L L tensor(0.5974) tensor(0.5974) -94.23718828707933\n",
      "55 F F tensor(0.8383) tensor(0.8383) -94.23718828707933\n",
      "56 Q E tensor(0.1137) tensor(0.2996) -93.63132808404043\n",
      "57 H A tensor(0.1630) tensor(0.2544) -93.9025022056885\n",
      "58 P P tensor(0.6132) tensor(0.6132) -94.23718828707933\n",
      "59 T V tensor(0.2866) tensor(0.3403) -91.84822157537565\n",
      "60 L L tensor(0.4121) tensor(0.4121) -94.23718828707933\n",
      "61 S A tensor(0.0973) tensor(0.4128) -92.50441026315093\n",
      "62 V A tensor(0.0077) tensor(0.2619) -90.87206053035334\n",
      "mutation at pos 62 from V to A improved ll from -91.05985924089327 to -90.87206053035334\n",
      "63 L L tensor(0.9211) tensor(0.9211) -94.23718828707933\n",
      "64 A A tensor(0.8444) tensor(0.8444) -94.23718828707933\n",
      "65 Q A tensor(0.0914) tensor(0.3021) -93.29300650721416\n",
      "66 S A tensor(0.0920) tensor(0.2580) -93.49401248479262\n",
      "67 L A tensor(0.1706) tensor(0.3726) -92.64447778137401\n",
      "68 V R tensor(0.0549) tensor(0.1537) -93.13585223536938\n",
      "69 P X tensor(0.0618) tensor(0.1542) -92.5630118320696\n"
     ]
    }
   ],
   "source": [
    "Tdomain = best_seq\n",
    "max_LL = likelihood_db[Tdomain]\n",
    "print(f'prev best T domain LL: {likelihood_db[Tdomain]}')\n",
    "for pos in range(len(Tdomain)):\n",
    "    aa_wt = Tdomain[pos]\n",
    "    token_wt = model.tokenizers.sequence.encode(aa_wt)[1]\n",
    "    assert model.tokenizers.sequence.decode(token_wt) == aa_wt\n",
    "    \n",
    "    sequence_prompt = Tdomain[:pos] + '_' + Tdomain[pos+1:]\n",
    "    esm_protein = ESMProtein(sequence=sequence_prompt)\n",
    "    protein_tensor = model.encode(esm_protein)\n",
    "    res = model.logits(protein_tensor, LogitsConfig(\n",
    "        sequence = True\n",
    "    ))\n",
    "\n",
    "    assert res.logits.sequence.shape[1] == len(sequence_prompt) + 2\n",
    "\n",
    "    logits = res.logits.sequence[0, 1:-1, :][pos, :].cpu()\n",
    "    softmax_prob = torch.nn.functional.softmax(logits, dim=0)\n",
    "    \n",
    "    token_mt = torch.argmax(logits)\n",
    "    token_mt_prob = torch.argmax(softmax_prob)\n",
    "    assert token_mt == token_mt_prob\n",
    "    \n",
    "    aa_mt = model.tokenizers.sequence.decode(token_mt)\n",
    "    aa_mt_prob = model.tokenizers.sequence.decode(token_mt_prob)\n",
    "    assert aa_mt == aa_mt_prob\n",
    "    \n",
    "    gen_seq = Tdomain[:pos] + aa_mt + Tdomain[pos+1:]\n",
    "\n",
    "    if gen_seq not in likelihood_db:\n",
    "        likelihood_db[gen_seq] = calculate_likelihood(gen_seq)\n",
    "    \n",
    "    ll = likelihood_db[gen_seq]\n",
    "\n",
    "    print(pos, aa_wt, aa_mt, softmax_prob[token_wt], softmax_prob[token_mt], ll)\n",
    "\n",
    "    if ll > max_LL:\n",
    "        print(f'mutation at pos {pos} from {aa_wt} to {aa_mt} improved ll from {max_LL} to {ll}')\n",
    "        max_LL = ll\n",
    "        best_seq = gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEIEIALATIWRELLNVEQVGRHDSFFALGGHSLLAVRMIERARRIGLGLSVQTLFQHPTLSALAQSLVP'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
