{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.base import BaseProtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = BaseProtein(file='../../Data/GxpS_ATC.pdb')\n",
    "T = [i for i in range(538,608)] ## 539-608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEIEIALATIWRELLNVEQVGRHDSFFALGGHSLLAVRMIERLRRIGLGLSVQTLFQHPTLSVLAQSLVP'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein.get_residues(T) ## T domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_query = ''.join(['<mask>' if i in T else protein.sequence[i] for i in range(len(protein.sequence))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PQQPVTAIDILSSSERELLLENWNATEEPYPTQVCVHQLFEQQIEKTPDAIAVIYENQTLSYAELNARANRLAHQLIALGVAPDQRVAICVTRSLARIIGLLAVLKAGGAYVPLDPAYPGERLAYMLTDATPVILMADNVGRAALSEDILATLTVLDPNTLLEQPDHNPQVSGLTPQHLAYVIYTSGSTGRPKGVMIEHRSVVNLTLTQITQFDVCATSRMLQFASFGFDASVWEIMMALSCGAMLVIPTETVRQDPQRLWRYLEEQAITHACLTPAMFHDGTDLPAIAIKPTLIFAGEAPSPALFQALCSRADLFNAYGPTEITVCATTWDCPADYTGGVIPIGSPVANKRLYLLDEHRQPVPLGTVGELYIGGVGVARGYLNRPELTAERFLNDPFSDETNARMYRAGDLARYLPDGNLVFVGRNDQQVKIRGFRIEPGEIEARLVEHSEVSEALVLALGDGQDKRLVAYVVALADDGLATKLREHLSDILPDYMIPAAFVRLDAFPLTPNGKLDRRSLPAPGEDAFARQAYQAPQ<mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask>HREISVPDNGITADTTVLTPAMLPLIDLTQAEIDRIVEQVPGGIANIQDIYALSPLQDGILFHHLLANEGDPYLLITQQAFADRPLLNRYLAAVQQVVDRHDILRTAFIWEGLSVPAQVICRQAPLSVTELTLNPADGAISNQLAQRFDPRRHRIDLNQAPLLRFVVAQESDGRWILLQLLHHLIGDHTTLEVMNSEVQACLLGQMDSLPAPVPFRHLVAQARQGVSQAEHTRFFTDMLAEVDEPTLLFGLAEAHHDGSQVTESHRMLTAGLNERLRGQARRLGVSVAALCHLAWAQVLSRTSGQTQVVFGTVLFGRMQAGEGSDSGMGLFINTLPLRLDIDNTPVRDSVRAAHSRLAGLLEHEHASLALAQRCSGVESGTPLFNALLNYRHNTQPVTPDEIVSGIEFLGAQERTNYPFVLSVEDSGSDLGLTAQVVQPFDPERICGYMQQALASLVQALEQASDMPVQQLDILPATERTLLLKTWNATETAYPE'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"protein1\", masked_query)\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_tokens = batch_tokens.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1105])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Random Mask Approach\n",
    "# with torch.no_grad():\n",
    "#     fix = 1\n",
    "#     inv = {v:k for k,v in alphabet.tok_to_idx.items()}\n",
    "#     for i in range(len(T)):\n",
    "#         print(f'run {i}')\n",
    "#         results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "#         prob = torch.nn.functional.softmax(results['logits'], dim=-1)\n",
    "#         masked_positions = torch.nonzero(batch_tokens == 32, as_tuple=False)\n",
    "#         print(f'num maxed pos {masked_positions.size(0)}')\n",
    "#         if masked_positions.size(0) > fix:\n",
    "#             selected_positions = masked_positions[torch.randperm(masked_positions.size(0))[:fix]]\n",
    "#             batch_tokens[0, selected_positions[:, 1]] = torch.multinomial(prob[:, selected_positions[:, 1], :][0], 1).flatten()\n",
    "#         else:\n",
    "#             selected_positions = masked_positions\n",
    "#             batch_tokens[0, selected_positions[:, 1]] = torch.multinomial(prob[:, selected_positions[:, 1], :][0], 1).flatten()\n",
    "\n",
    "#         print(f\" T domain : {''.join([inv[i] for i in batch_tokens[0, 1:-1][T].numpy()])}\")\n",
    "\n",
    "#         if torch.nonzero(batch_tokens == 32, as_tuple=False).size(0) == 0:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0\n",
      "num maxed pos 70\n",
      " T domain : S__E__________________________________________________________________\n",
      "run 1\n",
      "num maxed pos 68\n",
      " T domain : S__E__LL______________________________________________________________\n",
      "run 2\n",
      "num maxed pos 66\n",
      " T domain : S__E__LL__I___L_______________________________________________________\n",
      "run 3\n",
      "num maxed pos 64\n",
      " T domain : S__E__LL__I___L___________________________________________________I__P\n",
      "run 4\n",
      "num maxed pos 62\n",
      " T domain : SE_E__LL__I__LL___________________________________________________I__P\n",
      "run 5\n",
      "num maxed pos 60\n",
      " T domain : SE_EH_LL__IQ_LL___________________________________________________I__P\n",
      "run 6\n",
      "num maxed pos 58\n",
      " T domain : SE_EH_LL_LIQ_LLR__________________________________________________I__P\n",
      "run 7\n",
      "num maxed pos 56\n",
      " T domain : SETEH_LL_LIQ_LLRR_________________________________________________I__P\n",
      "run 8\n",
      "num maxed pos 54\n",
      " T domain : SETEH_LLQLIQGLLRR_________________________________________________I__P\n",
      "run 9\n",
      "num maxed pos 52\n",
      " T domain : SETEHCLLQLIQGLLRRS________________________________________________I__P\n",
      "run 10\n",
      "num maxed pos 50\n",
      " T domain : SETEHCLLQLIQGLLRRSG_____________G_________________________________I__P\n",
      "run 11\n",
      "num maxed pos 48\n",
      " T domain : SETEHCLLQLIQGLLRRSG____________LG__S______________________________I__P\n",
      "run 12\n",
      "num maxed pos 46\n",
      " T domain : SETEHCLLQLIQGLLRRSG____G____F__LG__S______________________________I__P\n",
      "run 13\n",
      "num maxed pos 44\n",
      " T domain : SETEHCLLQLIQGLLRRSG____G__Q_F__LGI_S______________________________I__P\n",
      "run 14\n",
      "num maxed pos 42\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LG__Q_F__LGI_SI_____________________________I__P\n",
      "run 15\n",
      "num maxed pos 40\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LG__Q_F__LGIDSI____L________________________I__P\n",
      "run 16\n",
      "num maxed pos 38\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGV_Q_F__LGIDSI__I_L________________________I__P\n",
      "run 17\n",
      "num maxed pos 36\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGV_QSF__LGIDSI__I_L___L____________________I__P\n",
      "run 18\n",
      "num maxed pos 34\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGV_QSF_ELGIDSI__I_L___L___L________________I__P\n",
      "run 19\n",
      "num maxed pos 32\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSF_ELGIDSI__IRL___L___L________________I__P\n",
      "run 20\n",
      "num maxed pos 30\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSF_ELGIDSI_RIRL___L___LK_______________I__P\n",
      "run 21\n",
      "num maxed pos 28\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSF_ELGIDSI_RIRL__VL___LK__P____________I__P\n",
      "run 22\n",
      "num maxed pos 26\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSF_ELGIDSI_RIRL_SVL_E_LK__P____________I__P\n",
      "run 23\n",
      "num maxed pos 24\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSF_ELGIDSINRIRL_SVL_ETLK__P____________I__P\n",
      "run 24\n",
      "num maxed pos 22\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSFAELGIDSINRIRL_SVLAETLK__P____________I__P\n",
      "run 25\n",
      "num maxed pos 20\n",
      " T domain : SETEHCLLQLIQGLLRRSG___LGVNQSFAELGIDSINRIRL_SVLAETLKP_P________K___I__P\n",
      "run 26\n",
      "num maxed pos 18\n",
      " T domain : SETEHCLLQLIQGLLRRSG_ELLGVNQSFAELGIDSINRIRL_SVLAETLKP_P________K___I__P\n",
      "run 27\n",
      "num maxed pos 16\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRL_SVLAETLKP_P________KD__I__P\n",
      "run 28\n",
      "num maxed pos 14\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRL_SVLAETLKP_P________KD_IIA_P\n",
      "run 29\n",
      "num maxed pos 12\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRL_SVLAETLKPPP_______LKD_IIA_P\n",
      "run 30\n",
      "num maxed pos 10\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRL_SVLAETLKPPP______SLKDLIIA_P\n",
      "run 31\n",
      "num maxed pos 8\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRLASVLAETLKPPP______SLKDLIIAIP\n",
      "run 32\n",
      "num maxed pos 6\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRLASVLAETLKPPPA____HSLKDLIIAIP\n",
      "run 33\n",
      "num maxed pos 4\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRLASVLAETLKPPPAR_D_HSLKDLIIAIP\n",
      "run 34\n",
      "num maxed pos 2\n",
      " T domain : SETEHCLLQLIQGLLRRSGGELLGVNQSFAELGIDSINRIRLASVLAETLKPPPARGDNHSLKDLIIAIP\n"
     ]
    }
   ],
   "source": [
    "## Min Entropy position\n",
    "with torch.no_grad():\n",
    "    fix = 2\n",
    "    inv = {v:k for k,v in alphabet.tok_to_idx.items()}\n",
    "    for i in range(len(T)):\n",
    "        print(f'run {i}')\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        prob = torch.nn.functional.softmax(results['logits'], dim=-1)\n",
    "        masked_positions = torch.nonzero(batch_tokens == 32, as_tuple=False)\n",
    "        print(f'num maxed pos {masked_positions.size(0)}')\n",
    "        if masked_positions.size(0) > fix:\n",
    "            masked_probs = prob[0 ,masked_positions[:, 1], :]\n",
    "            entropies = -torch.sum(masked_probs * torch.log(masked_probs + 1e-10), dim=-1)\n",
    "            indices = torch.argsort(entropies)[:fix]\n",
    "            selected_positions = masked_positions[:, 1][indices]\n",
    "            batch_tokens[0, selected_positions] = torch.multinomial(prob[:, selected_positions, :][0], 1).flatten()\n",
    "        else:\n",
    "            selected_positions = masked_positions[:, 1]\n",
    "            batch_tokens[0, selected_positions] = torch.multinomial(prob[:, selected_positions, :][0], 1).flatten()\n",
    "\n",
    "        print(f\" T domain : {''.join([inv[i] if inv[i] != '<mask>' else '_' for i in batch_tokens[0, 1:-1][T].numpy()])}\")\n",
    "\n",
    "        if torch.nonzero(batch_tokens == 32, as_tuple=False).size(0) == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein.get_residues(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logits'].shape, token_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.nn.functional.softmax(results['logits'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[:, selected_positions[:, 1], :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 596\n",
    "plt.plot(prob[0, i, :])\n",
    "plt.yscale('log')\n",
    "torch.argmax(prob[0, i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = {v:k for k,v in alphabet.tok_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(prob[0, T, :], dim=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([inv[i] for i in torch.argmax(prob[:,1:-1,:][0, T, :], dim=-1).numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein.get_residues(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = [608,609,700,701,702,703,704,705]\n",
    "print(protein.get_residues(select))\n",
    "print(''.join([inv[i] for i in torch.argmax(prob[:,1:-1,:][0, select, :], dim=-1).numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[:,1:-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(protein.sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
